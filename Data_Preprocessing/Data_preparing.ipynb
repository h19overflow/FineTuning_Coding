{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation Notebook\n",
    "\n",
    "This notebook contains utilities for data preprocessing and preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1345840862.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mimport os|\u001b[39m\n             ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os|\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "# Semantic chunking imports\n",
    "try:\n",
    "    from langchain_experimental.text_splitter import SemanticChunker as LangChainSemanticChunker\n",
    "    from langchain_core.embeddings import Embeddings\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    LANGCHAIN_SEMANTIC_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LANGCHAIN_SEMANTIC_AVAILABLE = False\n",
    "    print(\"Warning: LangChain semantic chunker not available, using fallback chunking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_text_from_url(url: str) -> str:\n",
    "    \"\"\"Fetch text content from a URL.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "    except requests.RequestException as e:\n",
    "        raise requests.RequestException(f\"Failed to fetch content from {url}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_text_as_markdown(text: str, filename: str, output_dir: str = \".\") -> str:\n",
    "    \"\"\"Save text content as a markdown file.\"\"\"\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    file_path = output_path / f\"{filename}.md\"\n",
    "    \n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "    \n",
    "    return str(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created/verified data directory: data\n",
      "\n",
      "Fetching content from: https://context7.com/langchain-ai/langgraph/llms.txt?tokens=600000\n",
      "Content length: 460281 characters\n",
      "First 100 characters: ========================\n",
      "CODE SNIPPETS\n",
      "========================\n",
      "TITLE: Example LangGraph Project Dir...\n",
      "\n",
      "Fetching content from: https://context7.com/pydantic/pydantic-ai/llms.txt?tokens=100000\n",
      "Content length: 467984 characters\n",
      "First 100 characters: ========================\n",
      "CODE SNIPPETS\n",
      "========================\n",
      "TITLE: Run Pydantic AI Example with ...\n",
      "\n",
      "Fetching content from: https://context7.com/llmstxt/python_langchain_llms_txt/llms.txt?tokens=100000\n",
      "Content length: 476392 characters\n",
      "First 100 characters: ========================\n",
      "CODE SNIPPETS\n",
      "========================\n",
      "TITLE: LangChain Introduction and Tu...\n"
     ]
    }
   ],
   "source": [
    "# Create data directory and fetch text from multiple URLs\n",
    "urls = [\n",
    "    \"https://context7.com/langchain-ai/langgraph/llms.txt?tokens=600000\",\n",
    "    \"https://context7.com/pydantic/pydantic-ai/llms.txt?tokens=100000\",\n",
    "    \"https://context7.com/llmstxt/python_langchain_llms_txt/llms.txt?tokens=100000\"\n",
    "]\n",
    "\n",
    "filenames = [\n",
    "    \"langgraph_llms_data\",\n",
    "    \"pydantic_ai_llms_data\", \n",
    "    \"python_langchain_llms_data\"\n",
    "]\n",
    "\n",
    "data_dir = \"data\"\n",
    "\n",
    "# Create data directory\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "print(f\"Created/verified data directory: {data_dir}\")\n",
    "\n",
    "# Fetch content from all URLs\n",
    "text_contents = []\n",
    "for i, url in enumerate(urls):\n",
    "    print(f\"\\nFetching content from: {url}\")\n",
    "    text_content = fetch_text_from_url(url)\n",
    "    text_contents.append(text_content)\n",
    "    \n",
    "    print(f\"Content length: {len(text_content)} characters\")\n",
    "    print(f\"First 100 characters: {text_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content 1 saved to: data\\langgraph_llms_data.md\n",
      "File size: 474337 bytes\n",
      "\n",
      "Content 2 saved to: data\\pydantic_ai_llms_data.md\n",
      "File size: 483402 bytes\n",
      "\n",
      "Content 3 saved to: data\\python_langchain_llms_data.md\n",
      "File size: 489407 bytes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Save all content as markdown files in the data directory\n",
    "for i, (text_content, filename) in enumerate(zip(text_contents, filenames)):\n",
    "    saved_file = save_text_as_markdown(text_content, filename, data_dir)\n",
    "    print(f\"Content {i+1} saved to: {saved_file}\")\n",
    "    print(f\"File size: {os.path.getsize(saved_file)} bytes\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "class SentenceTransformerEmbeddings(Embeddings):\n",
    "    \"\"\"Wrapper for SentenceTransformer models to work with LangChain.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = 'nomic-ai/nomic-embed-text-v1.5'):\n",
    "        print(f\"Initializing SentenceTransformer with model: {model_name}\")\n",
    "        try:\n",
    "            self.model = SentenceTransformer(model_name, trust_remote_code=True)\n",
    "            print(f\"Successfully loaded model: {model_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {model_name}, falling back to BAAI/bge-large-en-v1.5: {e}\")\n",
    "            self.model = SentenceTransformer('BAAI/bge-large-en-v1.5')\n",
    "    \n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embed search docs.\"\"\"\n",
    "        print(f\"Embedding {len(texts)} documents\")\n",
    "        embeddings = self.model.encode(texts)\n",
    "        return embeddings.tolist()\n",
    "    \n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"Embed query text.\"\"\"\n",
    "        print(f\"Embedding query of length: {len(text)}\")\n",
    "        embedding = self.model.encode([text])\n",
    "        return embedding[0].tolist()\n",
    "\n",
    "class SemanticChunker:\n",
    "    \"\"\"Semantic chunker using LangChain's SemanticChunker and Nomic model.\"\"\"\n",
    "\n",
    "    def __init__(self, chunk_size: int = 8192, threshold: float = 0.75):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.threshold = threshold\n",
    "\n",
    "        # Setup logging\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        if not self.logger.handlers:\n",
    "            handler = logging.StreamHandler()\n",
    "            handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "            self.logger.addHandler(handler)\n",
    "            self.logger.setLevel(logging.INFO)\n",
    "        \n",
    "        print(f\"Initializing SemanticChunker with chunk_size={chunk_size}, threshold={threshold}\")\n",
    "        \n",
    "        # Check if semantic chunking is available\n",
    "        try:\n",
    "            from langchain_experimental.text_splitter import SemanticChunker as LangChainSemanticChunker\n",
    "            semantic_available = True\n",
    "            print(\"LangChain SemanticChunker is available\")\n",
    "        except ImportError as e:\n",
    "            semantic_available = False\n",
    "            print(f\"LangChain SemanticChunker not available: {e}\")\n",
    "        \n",
    "        print(f\"LANGCHAIN_SEMANTIC_AVAILABLE: {semantic_available}\")\n",
    "        \n",
    "        # Initialize semantic chunker\n",
    "        try:\n",
    "            if semantic_available:\n",
    "                print(\"Creating SentenceTransformer embeddings...\")\n",
    "                # Create embeddings wrapper\n",
    "                embeddings = SentenceTransformerEmbeddings('nomic-ai/nomic-embed-text-v1.5')\n",
    "                \n",
    "                # Calculate min_chunk_size based on chunk_size\n",
    "                min_chunk_size = max(500, self.chunk_size // 4)\n",
    "                print(f\"Using min_chunk_size: {min_chunk_size}\")\n",
    "                \n",
    "                # Pass the embeddings wrapper into SemanticChunker\n",
    "                print(\"Creating LangChain SemanticChunker...\")\n",
    "                self.semantic_chunker = LangChainSemanticChunker(\n",
    "                    embeddings=embeddings, \n",
    "                    breakpoint_threshold_amount=self.threshold,\n",
    "                    min_chunk_size=min_chunk_size\n",
    "                )\n",
    "                print(\"LangChain semantic chunker initialized successfully!\")\n",
    "                self.logger.info(f\"LangChain semantic chunker ready with Nomic embeddings (chunk_size={chunk_size}, min_chunk_size={min_chunk_size})\")\n",
    "            else:\n",
    "                print(\"LangChain experimental not available\")\n",
    "                self.semantic_chunker = None\n",
    "                self.logger.warning(\"langchain_experimental not available, using fallback chunking\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR initializing semantic chunker: {e}\")\n",
    "            print(f\"Exception type: {type(e)}\")\n",
    "            import traceback\n",
    "            print(f\"Full traceback:\")\n",
    "            traceback.print_exc()\n",
    "            self.logger.warning(f\"Semantic chunker failed to initialize, using fallback: {e}\")\n",
    "            self.semantic_chunker = None\n",
    "    \n",
    "    def chunk_text(self, text: str, source_file: str) -> List[str]:\n",
    "        \"\"\"Split text into semantic chunks.\"\"\"\n",
    "        print(f\"\\n=== Chunking {source_file} ===\")\n",
    "        print(f\"Text length: {len(text)} characters\")\n",
    "        print(f\"Semantic chunker available: {self.semantic_chunker is not None}\")\n",
    "        \n",
    "        if self.semantic_chunker:\n",
    "            try:\n",
    "                print(\"Attempting semantic chunking...\")\n",
    "                # Create semantic chunks using LangChain's method\n",
    "                docs = self.semantic_chunker.create_documents([text])\n",
    "                chunks = [d.page_content for d in docs]\n",
    "                \n",
    "                print(f\"Semantic chunking completed. Created {len(chunks)} chunks\")\n",
    "                \n",
    "                if not chunks:\n",
    "                    print(\"WARNING: Semantic chunker produced no chunks. Using fallback.\")\n",
    "                    self.logger.warning(f\"Semantic chunker produced no chunks for {source_file}. Using fallback.\")\n",
    "                    return self._fallback_chunker(text)\n",
    "\n",
    "                # Log chunk length statistics\n",
    "                chunk_lengths = [len(chunk) for chunk in chunks]\n",
    "                print(f\"Chunk lengths: min={min(chunk_lengths)}, max={max(chunk_lengths)}, avg={sum(chunk_lengths)/len(chunk_lengths):.0f}\")\n",
    "                \n",
    "                self.logger.info(f\"Created {len(chunks)} semantic chunks for {source_file}\")\n",
    "                return chunks\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR during semantic chunking: {e}\")\n",
    "                print(f\"Exception type: {type(e)}\")\n",
    "                import traceback\n",
    "                print(f\"Full traceback:\")\n",
    "                traceback.print_exc()\n",
    "                self.logger.error(f\"Semantic chunking failed during execution for {source_file}: {e}. Using fallback.\")\n",
    "                return self._fallback_chunker(text)\n",
    "        else:\n",
    "            print(\"Using fallback chunker (semantic chunker not available)\")\n",
    "            self.logger.info(f\"Using fallback chunker for {source_file}.\")\n",
    "            return self._fallback_chunker(text)\n",
    "\n",
    "    def _fallback_chunker(self, text: str) -> List[str]:\n",
    "        \"\"\"Fallback text chunking method.\"\"\"\n",
    "        print(f\"=== FALLBACK CHUNKING ===\")\n",
    "        chunk_size = self.chunk_size\n",
    "        overlap = 500\n",
    "        chunks = []\n",
    "        \n",
    "        for i in range(0, len(text), chunk_size - overlap):\n",
    "            chunk = text[i:i + chunk_size]\n",
    "            if chunk.strip():\n",
    "                chunks.append(chunk)\n",
    "        \n",
    "        print(f\"Fallback chunking created {len(chunks)} chunks\")\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_chunks_as_json(chunks: List[str], source_name: str, output_dir: str = \"data/chunks\") -> str:\n",
    "    \"\"\"\n",
    "    Save text chunks as JSON with source metadata.\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of text chunks\n",
    "        source_name: Name of the source file\n",
    "        output_dir: Directory to save the file in\n",
    "        \n",
    "    Returns:\n",
    "        str: Full path to the saved file\n",
    "    \"\"\"\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Create metadata for each chunk\n",
    "    chunk_data = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_data.append({\n",
    "            \"chunk_id\": i + 1,\n",
    "            \"source\": source_name,\n",
    "            \"content\": chunk,\n",
    "            \"chunk_length\": len(chunk)\n",
    "        })\n",
    "    \n",
    "    # Save as JSON\n",
    "    json_filename = f\"{source_name}_chunks.json\"\n",
    "    file_path = output_path / json_filename\n",
    "    \n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(chunk_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    return str(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing SemanticChunker with chunk_size=8192, threshold=0.75\n",
      "LangChain SemanticChunker is available\n",
      "LANGCHAIN_SEMANTIC_AVAILABLE: True\n",
      "Creating SentenceTransformer embeddings...\n",
      "Initializing SentenceTransformer with model: nomic-ai/nomic-embed-text-v1.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encountered exception while importing einops: No module named 'einops'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load nomic-ai/nomic-embed-text-v1.5, falling back to BAAI/bge-large-en-v1.5: This modeling file requires the following packages that were not found in your environment: einops. Run `pip install einops`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-22 08:42:31,218 - INFO - LangChain semantic chunker ready with Nomic embeddings (chunk_size=8192, min_chunk_size=2048)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using min_chunk_size: 2048\n",
      "Creating LangChain SemanticChunker...\n",
      "LangChain semantic chunker initialized successfully!\n",
      "Starting semantic chunking process...\n",
      "\n",
      "Processing data/langgraph_llms_data.md...\n",
      "File length: 460281 characters\n",
      "\n",
      "=== Chunking langgraph ===\n",
      "Text length: 460281 characters\n",
      "Semantic chunker available: True\n",
      "Attempting semantic chunking...\n",
      "Embedding 1283 documents\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import logging\n",
    "import json\n",
    "# Initialize semantic chunker\n",
    "chunker = SemanticChunker(chunk_size=8192, threshold=0.75)\n",
    "\n",
    "# Define the markdown files to process\n",
    "markdown_files = [\n",
    "    (\"data/langgraph_llms_data.md\", \"langgraph\"),\n",
    "    (\"data/pydantic_ai_llms_data.md\", \"pydantic_ai\"),\n",
    "    (\"data/python_langchain_llms_data.md\", \"python_langchain\")\n",
    "]\n",
    "\n",
    "print(\"Starting semantic chunking process...\")\n",
    "\n",
    "for file_path, source_name in markdown_files:\n",
    "    print(f\"\\nProcessing {file_path}...\")\n",
    "    \n",
    "    try:\n",
    "        # Read the markdown file\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text_content = f.read()\n",
    "        \n",
    "        print(f\"File length: {len(text_content)} characters\")\n",
    "        \n",
    "        # Create semantic chunks\n",
    "        chunks = chunker.chunk_text(text_content, source_name)\n",
    "        \n",
    "        print(f\"Created {len(chunks)} chunks for {source_name}\")\n",
    "        \n",
    "        # Save chunks with metadata\n",
    "        saved_file = save_chunks_as_json(chunks, source_name)\n",
    "        print(f\"Chunks saved to: {saved_file}\")\n",
    "        \n",
    "        # Show chunk statistics\n",
    "        chunk_lengths = [len(chunk) for chunk in chunks]\n",
    "        avg_length = sum(chunk_lengths) / len(chunk_lengths) if chunks else 0\n",
    "        print(f\"Average chunk length: {avg_length:.0f} characters\")\n",
    "        print(f\"Min chunk length: {min(chunk_lengths) if chunks else 0}\")\n",
    "        print(f\"Max chunk length: {max(chunk_lengths) if chunks else 0}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: File {file_path} not found. Skipping...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "print(\"\\nSemantic chunking complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Coding_LLM (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

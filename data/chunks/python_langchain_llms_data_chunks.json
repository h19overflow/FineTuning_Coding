[
  {
    "chunk_id": 1,
    "source": "python_langchain_llms_data",
    "content": "========================\nCODE SNIPPETS\n========================\nTITLE: LangChain Introduction and Tutorials\nDESCRIPTION: Guides for getting started with LangChain and a list of tutorials for building various LLM applications. SOURCE: https://python.langchain.com/docs/how_to/tools_error/\n\nLANGUAGE: APIDOC\nCODE:\n```\nIntroduction: /docs/introduction/\nTutorials: /docs/tutorials/\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides Overview\nDESCRIPTION: This section provides guides on implementing specific features and functionalities within LangChain. Topics include using tools in chains, vectorstores as retrievers, adding memory to chatbots, example selectors, semantic layers over graph databases, parallel runnable invocation, streaming chat model responses, default invocation arguments, retrieval for chatbots, few-shot examples, function calling, package installation, query analysis examples, routing, and structured output from models. SOURCE: https://python.langchain.com/docs/how_to/sql_prompting/\n\nLANGUAGE: python\nCODE:\n```\nHow to use tools in a chain\nHow to use a vectorstore as a retriever\nHow to add memory to chatbots\nHow to use example selectors\nHow to add a semantic layer over graph database\nHow to invoke runnables in parallel\nHow to stream chat model responses\nHow to add default invocation args to a Runnable\nHow to add retrieval to chatbots\nHow to use few shot examples in chat models\nHow to do tool/function calling\nHow to install LangChain packages\nHow to add examples to the prompt for query analysis\nHow to use few shot examples\nHow to run custom functions\nHow to use output parsers to parse an LLM response into structured format\nHow to handle cases where no queries are generated\nHow to route between sub-chains\nHow to return structured data from a model\n```\n\n----------------------------------------\n\nTITLE: How to Use Example Selectors\nDESCRIPTION: This guide covers the use of example selectors in LangChain, which dynamically select relevant examples to include in prompts based on the current input.",
    "chunk_length": 2062
  },
  {
    "chunk_id": 2,
    "source": "python_langchain_llms_data",
    "content": "This is useful for few-shot learning. SOURCE: https://python.langchain.com/docs/how_to/output_parser_structured/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.example_selectors import SemanticSimilarityExampleSelector\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_core.example_selectors import LengthWithoutTokensExampleSelector\nfrom langchain_text_splitters import TextSplitter\n\n# Example using SemanticSimilarityExampleSelector\n# examples = [\n#     {\"input\": \"apple\", \"output\": \"fruit\"},\n#     {\"input\": \"carrot\", \"output\": \"vegetable\"},\n#     {\"input\": \"banana\", \"output\": \"fruit\"},\n# ]\n# \n# example_selector = SemanticSimilarityExampleSelector.from_examples(\n#     examples,\n#     OpenAIEmbeddings(),\n#     Chroma,\n#     k=1,\n# )\n# \n# prompt = PromptTemplate(\n#     input_variables=[\"input\", \"examples\"],\n#     template=\"Input: {input}\\nOutput: {examples}\",\n# )\n# \n# chain = prompt | OpenAI() # Assuming OpenAI is configured\n# \n# print(chain.invoke({\"input\": \"grape\", \"examples\": example_selector}))\n\n```\n\n----------------------------------------\n\nTITLE: How to use example selectors\nDESCRIPTION: This how-to guide covers the usage of example selectors in LangChain. These selectors help in dynamically choosing few-shot examples for prompts. SOURCE: https://python.langchain.com/docs/tutorials/sql_qa/\n\nLANGUAGE: python\nCODE:\n```\nprint(\"How-to: How to use example selectors\")\n# Further implementation details would follow here. ```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides Overview\nDESCRIPTION: Provides a list of how-to guides for implementing specific functionalities in LangChain, such as using tools, vectorstores, memory, example selectors, parallel execution, streaming, function calling, and more. SOURCE: https://python.langchain.com/docs/how_to/example_selectors_mmr/\n\nLANGUAGE: text\nCODE:\n```\nHow to use tools in a chain\nHow to use a vectorstore as a retriever\nHow to add memory to chatbots\nHow to use example selectors\nHow to add a semantic layer over graph database\nHow to invoke runnables in parallel\nHow to stream chat model responses\nHow to add default invocation args to a Runnable\nHow to add retrieval to chatbots\nHow to use few shot examples in chat models\nHow to do tool/function calling\nHow to install LangChain packages\nHow to add examples to the prompt for query analysis\nHow to use few shot examples\nHow to run custom functions\nHow to use output parsers to parse an LLM response into structured format\nHow to handle cases where no queries are generated\nHow to route between sub-chains\nHow to return structured data from a model\n```\n\n----------------------------------------\n\nTITLE: Use Example Selectors\nDESCRIPTION: This guide explains how to use LangChain's example selectors to dynamically select relevant few-shot examples for prompts.",
    "chunk_length": 2919
  },
  {
    "chunk_id": 3,
    "source": "python_langchain_llms_data",
    "content": "This helps in optimizing prompt length and improving model performance by providing the most pertinent examples. SOURCE: https://python.langchain.com/docs/how_to/binding/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\nfrom langchain_core.example_selector import SemanticSimilarityExampleSelector\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_community.embeddings import OpenAIEmbeddings\n\n# Initialize the chat model\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n\n# Define few-shot examples\nexamples = [\n    {\"input\": \"Q: What is the capital of France?\", \"output\": \"A: Paris\"},\n    {\"input\": \"Q: What is the capital of Japan?\", \"output\": \"A: Tokyo\"},\n    {\"input\": \"Q: What is the capital of Canada?\", \"output\": \"A: Ottawa\"},\n    {\"input\": \"Q: What is the capital of Germany?\", \"output\": \"A: Berlin\"},\n    {\"input\": \"Q: What is the capital of Brazil?\", \"output\": \"A: Brasilia\"}\n]\n\n# Convert examples to chat messages format\nexample_messages = []\nfor example in examples:\n    example_messages.append(HumanMessage(content=example['input']))\n    example_messages.append(AIMessage(content=example['output']))\n\n# Create a semantic similarity example selector\n# This requires embedding the example inputs\nexample_selector = SemanticSimilarityExampleSelector.from_examples(\n    example_messages,\n    OpenAIEmbeddings(),\n    FAISS,\n    k=2 # Select top 2 similar examples\n)\n\n# Create a FewShotChatMessagePromptTemplate\n# This template will use the example selector to fetch examples\nexample_prompt = FewShotChatMessagePromptTemplate(\n    example_selector=example_selector,\n    example_messages=example_messages, # Provide all examples here for the selector to use\n    input_variables=[\"input\"],\n    # The prompt structure for each example pair (HumanMessage, AIMessage)\n    # This is implicitly handled by FewShotChatMessagePromptTemplate when using example_messages\n)\n\n# Create the final prompt template including the few-shot examples\nfinal_prompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant that answers questions about capitals.\"),\n    example_prompt,\n    (\"human\", \"{input}\")\n])\n\n# Create a chain\nchain = final_prompt | llm\n\n# Invoke the chain with a new question\n# The example_selector will pick the 2 most relevant examples based on semantic similarity\n# result = chain.invoke({\"input\": \"What is the capital of Italy?\"})\n# print(result.content)\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: Lists how-to guides for LangChain, covering practical implementation details such as using tools, vectorstores, memory, example selectors, parallel execution, streaming, function calling, and more.",
    "chunk_length": 2808
  },
  {
    "chunk_id": 4,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/how_to/callbacks_constructor/\n\nLANGUAGE: APIDOC\nCODE:\n```\nHow-to guides:\n  - How to use tools in a chain: /docs/how_to/tools_chain/\n  - How to use a vectorstore as a retriever: /docs/how_to/vectorstore_retriever/\n  - How to add memory to chatbots: /docs/how_to/chatbots_memory/\n  - How to use example selectors: /docs/how_to/example_selectors/\n  - How to add a semantic layer over graph database: /docs/how_to/graph_semantic/\n  - How to invoke runnables in parallel: /docs/how_to/parallel/\n  - How to stream chat model responses: /docs/how_to/chat_streaming/\n  - How to add default invocation args to a Runnable: /docs/how_to/binding/\n  - How to add retrieval to chatbots: /docs/how_to/chatbots_retrieval/\n  - How to use few shot examples in chat models: /docs/how_to/few_shot_examples_chat/\n  - How to do tool/function calling: /docs/how_to/function_calling/\n  - How to install LangChain packages: /docs/how_to/installation/\n  - How to add examples to the prompt for query analysis: /docs/how_to/query_few_shot/\n  - How to use few shot examples: /docs/how_to/few_shot_examples/\n  - How to run custom functions: /docs/how_to/functions/\n  - How to use output parsers to parse an LLM response into structured format: /docs/how_to/output_parser_structured/\n  - How to handle cases where no queries are generated: /docs/how_to/query_no_queries/\n  - How to route between sub-chains: /docs/how_to/routing/\n  - How to return structured data from a model: /docs/how_to/structured_output/\n```\n\n----------------------------------------\n\nTITLE: LangGraph Quickstarts\nDESCRIPTION: Provides quickstart guides for getting started with LangGraph, including building custom workflows and running local servers. SOURCE: https://langchain-ai.github.io/langgraph/\n\nLANGUAGE: python\nCODE:\n```\nStart with a prebuilt agent\nBuild a custom workflow\nRun a local server\n```\n\n----------------------------------------\n\nTITLE: LangChain Setup and Usage Guide\nDESCRIPTION: Provides a step-by-step guide on how to use LangChain, including environment setup, understanding core concepts like Agents, Chains, and Tools, choosing components based on use cases, integrating with language models (e.g., OpenAI, Anthropic), implementing application logic, and testing.",
    "chunk_length": 2273
  },
  {
    "chunk_id": 5,
    "source": "python_langchain_llms_data",
    "content": "It highlights LangChain's role as a flexible framework for building language-based applications. SOURCE: https://python.langchain.com/docs/how_to/routing/\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\n# Example of setting up environment variables (e.g., for API keys)\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\n# --- Core LangChain Concepts ---\n# Agents: Agents use a language model to decide which actions to take and in what order. # Chains: Chains allow you to combine LLMs with other components or data sources. # Tools: Tools are functions that agents can call to interact with the outside world. # --- Example Usage (Conceptual) ---\n# from langchain.llms import OpenAI\n# from langchain.chains import LLMChain\n# from langchain.prompts import PromptTemplate\n\n# llm = OpenAI(temperature=0.9)\n# prompt = PromptTemplate(\n#     input_variables=[\"product_name\"],\n#     template=\"Tell me a joke about {product_name}.\",\n# )\n# chain = LLMChain(llm=llm, prompt=prompt)\n\n# print(chain.run(\"LangChain\"))\n\n# --- RunnableBranch Example (Conceptual) ---\n# from langchain.schema import StrOutputParser\n# from langchain.schema.runnable import RunnableBranch\n\n# def is_even(n):\n#     return n % 2 == 0\n\n# def is_positive(n):\n#     return n > 0\n\n# branch = RunnableBranch(\n#     (\"even\", RunnableLambda(is_even) | StrOutputParser()),\n#     (\"positive\", RunnableLambda(is_positive) | StrOutputParser()),\n#     name=\"number_branch\"\n# )\n\n# print(branch.invoke(4))\n# print(branch.invoke(-2))\n# print(branch.invoke(3))\n\n```\n\n----------------------------------------\n\nTITLE: How to install LangChain packages\nDESCRIPTION: This how-to guide provides instructions on how to install the necessary LangChain packages. It covers the prerequisites and commands for setting up LangChain. SOURCE: https://python.langchain.com/docs/tutorials/sql_qa/\n\nLANGUAGE: bash\nCODE:\n```\npip install langchain\n```\n\n----------------------------------------\n\nTITLE: How to Use Few Shot Examples\nDESCRIPTION: This guide covers the general practice of using few-shot examples in LangChain prompts.",
    "chunk_length": 2056
  },
  {
    "chunk_id": 6,
    "source": "python_langchain_llms_data",
    "content": "It explains how to format examples to guide language models in tasks like classification, summarization, or translation. SOURCE: https://python.langchain.com/docs/how_to/tool_artifacts/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n\n# example_prompt = PromptTemplate(input_variables=[\"input\", \"output\"], template=\"Input: {input}\\nOutput: {output}\")\n# examples = [\n#     {\"input\": \"apple\", \"output\": \"fruit\"},\n#     {\"input\": \"carrot\", \"output\": \"vegetable\"},\n# ]\n# prompt = FewShotPromptTemplate(\n#     example_prompt=example_prompt,\n#     examples=examples,\n#     suffix=\"Input: {input}\\nOutput:\",\n#     input_variables=[\"input\"],\n# )\n# formatted_prompt = prompt.format(input=\"banana\")\n\n```\n\n----------------------------------------\n\nTITLE: How to use few shot examples\nDESCRIPTION: This how-to guide explains how to leverage few-shot learning by providing examples directly in the prompt to guide the LLM's behavior. SOURCE: https://python.langchain.com/docs/tutorials/rag/\n\nLANGUAGE: python\nCODE:\n```\n# This is a placeholder for the actual code in the how-to guide. # The guide focuses on the concepts and steps involved in using few-shot examples. # For specific code examples, please refer to the official LangChain documentation. # Example conceptual steps:\n# 1. Create a prompt template that includes placeholders for examples. # 2. Manually provide a few input-output pairs as examples within the prompt. # 3. Append the actual user query to the prompt. # 4. Send the complete prompt to the LLM. ```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: Practical guides on implementing specific features and functionalities within LangChain. These cover topics like using tools, vectorstores, memory, example selectors, parallel execution, streaming, and more. SOURCE: https://python.langchain.com/docs/concepts/example_selectors/\n\nLANGUAGE: APIDOC\nCODE:\n```\nHow-to guides:\n- How to use tools in a chain: /docs/how_to/tools_chain/\n- How to use a vectorstore as a retriever: /docs/how_to/vectorstore_retriever/\n- How to add memory to chatbots: /docs/how_to/chatbots_memory/\n- How to use example selectors: /docs/how_to/example_selectors/\n- How to add a semantic layer over graph database: /docs/how_to/graph_semantic/\n- How to invoke runnables in parallel: /docs/how_to/parallel/\n- How to stream chat model responses: /docs/how_to/chat_streaming/\n- How to add default invocation args to a Runnable: /docs/how_to/binding/\n- How to add retrieval to chatbots: /docs/how_to/chatbots_retrieval/\n- How to use few shot examples in chat models: /docs/how_to/few_shot_examples_chat/\n- How to do tool/function calling: /docs/how_to/function_calling/\n- How to install LangChain packages: /docs/how_to/installation/\n- How to add examples to the prompt for query analysis: /docs/how_to/query_few_shot/\n- How to use few shot examples: /docs/how_to/few_shot_examples/\n- How to run custom functions: /docs/how_to/functions/\n- How to use output parsers to parse an LLM response into structured format: /docs/how_to/output_parser_structured/\n- How to handle cases where no queries are generated: /docs/how_to/query_no_queries/\n- How to route between sub-chains: /docs/how_to/routing/\n- How to return structured data from a model: /docs/how_to/structured_output/\n- How to summarize text through parallelization: /docs/how_to/summarize_map_reduce/\n```\n\n----------------------------------------\n\nTITLE: How to use example selectors\nDESCRIPTION: This how-to guide covers the usage of example selectors in LangChain, which help in dynamically selecting few-shot examples for prompts.",
    "chunk_length": 3656
  },
  {
    "chunk_id": 7,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/tutorials/rag/\n\nLANGUAGE: python\nCODE:\n```\n# This is a placeholder for the actual code in the how-to guide. # The guide focuses on the concepts and steps involved in using example selectors. # For specific code examples, please refer to the official LangChain documentation. # Example conceptual steps:\n# 1. Prepare a list of example prompts and outputs\n# 2. Choose an example selector strategy (e.g., LengthBasedExampleSelector, SemanticSimilarityExampleSelector)\n# 3. Instantiate the selector with examples and a prompt template\n# 4. Use the selector to format the prompt with selected examples\n```\n\n----------------------------------------\n\nTITLE: How-to Guides Navigation\nDESCRIPTION: Lists various how-to guides for LangChain, covering essential tasks like installation, using tools, adding memory, and handling structured output. SOURCE: https://python.langchain.com/docs/how_to/filter_messages/\n\nLANGUAGE: HTML\nCODE:\n```\n*   [How-to guides](/docs/how_to/)\n    \n    *   [How-to guides](/docs/how_to/)\n    *   [How to use tools in a chain](/docs/how_to/tools_chain/)\n    *   [How to use a vectorstore as a retriever](/docs/how_to/vectorstore_retriever/)\n    *   [How to add memory to chatbots](/docs/how_to/chatbots_memory/)\n    *   [How to use example selectors](/docs/how_to/example_selectors/)\n    *   [How to add a semantic layer over graph database](/docs/how_to/graph_semantic/)\n    *   [How to invoke runnables in parallel](/docs/how_to/parallel/)\n    *   [How to stream chat model responses](/docs/how_to/chat_streaming/)\n    *   [How to add default invocation args to a Runnable](/docs/how_to/binding/)\n    *   [How to add retrieval to chatbots](/docs/how_to/chatbots_retrieval/)\n    *   [How to use few shot examples in chat models](/docs/how_to/few_shot_examples_chat/)\n    *   [How to do tool/function calling](/docs/how_to/function_calling/)\n    *   [How to install LangChain packages](/docs/how_to/installation/)\n    *   [How to add examples to the prompt for query analysis](/docs/how_to/query_few_shot/)\n    *   [How to use few shot examples](/docs/how_to/few_shot_examples/)\n    *   [How to run custom functions](/docs/how_to/functions/)\n    *   [How to use output parsers to parse an LLM response into structured format](/docs/how_to/output_parser_structured/)\n    *   [How to handle cases where no queries are generated](/docs/how_to/query_no_queries/)\n    *   [How to route between sub-chains](/docs/how_to/routing/)\n    *   [How to return structured data from a model](/docs/how_to/structured_output/)\n    *   [How to summarize text through parallelization](/docs/how_to/summarize_map_reduce/)\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides Overview\nDESCRIPTION: This section provides guides on how to perform specific tasks with LangChain.",
    "chunk_length": 2839
  },
  {
    "chunk_id": 8,
    "source": "python_langchain_llms_data",
    "content": "It includes instructions on using tools, vectorstores, memory, example selectors, parallel execution, streaming, function calling, and more. SOURCE: https://python.langchain.com/docs/how_to/output_parser_xml/\n\nLANGUAGE: markdown\nCODE:\n```\n*   [How-to guides](/docs/how_to/)\n*   [How to use tools in a chain](/docs/how_to/tools_chain/)\n*   [How to use a vectorstore as a retriever](/docs/how_to/vectorstore_retriever/)\n*   [How to add memory to chatbots](/docs/how_to/chatbots_memory/)\n*   [How to use example selectors](/docs/how_to/example_selectors/)\n*   [How to add a semantic layer over graph database](/docs/how_to/graph_semantic/)\n*   [How to invoke runnables in parallel](/docs/how_to/parallel/)\n*   [How to stream chat model responses](/docs/how_to/chat_streaming/)\n*   [How to add default invocation args to a Runnable](/docs/how_to/binding/)\n*   [How to add retrieval to chatbots](/docs/how_to/chatbots_retrieval/)\n*   [How to use few shot examples in chat models](/docs/how_to/few_shot_examples_chat/)\n*   [How to do tool/function calling](/docs/how_to/function_calling/)\n*   [How to install LangChain packages](/docs/how_to/installation/)\n*   [How to add examples to the prompt for query analysis](/docs/how_to/query_few_shot/)\n*   [How to use few shot examples](/docs/how_to/few_shot_examples/)\n*   [How to run custom functions](/docs/how_to/functions/)\n*   [How to use output parsers to parse an LLM response into structured format](/docs/how_to/output_parser_structured/)\n*   [How to handle cases where no queries are generated](/docs/how_to/query_no_queries/)\n*   [How to route between sub-chains](/docs/how_to/routing/)\n*   [How to return structured data from a model](/docs/how_to/structured_output/)\n*   [How to summarize text through parallelization](/docs/how_to/summarize_map_reduce/)\n```\n\n----------------------------------------\n\nTITLE: How to Use Few Shot Examples\nDESCRIPTION: This guide covers the general approach to using few-shot examples in LangChain, applicable to various model types and tasks. It emphasizes the importance of prompt engineering with examples.",
    "chunk_length": 2093
  },
  {
    "chunk_id": 9,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/how_to/output_parser_structured/\n\nLANGUAGE: python\nCODE:\n```\n# This is a conceptual guide. Actual implementation depends on the specific model and prompt structure. # Example structure for few-shot prompting:\n# prompt = \"\"\"\n# Given the following examples:\n# \n# Input: apple\n# Output: fruit\n# \n# Input: carrot\n# Output: vegetable\n# \n# Input: {user_input}\n# Output:\n# \"\"\"\n# \n# chain = LLMChain(llm=llm, prompt=prompt)\n# print(chain.run(\"banana\"))\n```\n\n----------------------------------------\n\nTITLE: Install Dependencies\nDESCRIPTION: Installs necessary libraries including langchain-core, langchain-openai, and langgraph for the examples. SOURCE: https://python.langchain.com/docs/how_to/convert_runnable_to_tool/\n\nLANGUAGE: bash\nCODE:\n```\n%%capture --no-stderr%pip install -U langchain-core langchain-openai langgraph\n```\n\n----------------------------------------\n\nTITLE: LangGraph Examples\nDESCRIPTION: Guided examples for getting started with LangGraph, demonstrating various use cases and functionalities. SOURCE: https://langchain-ai.github.io/langgraph/\n\nLANGUAGE: python\nCODE:\n```\n# Example: Simple graph execution\n# from langgraph.graph import StateGraph\n# def add(state):\n#     return state + 1\n# workflow = StateGraph(int)\n# workflow.add_node(\"add\", add)\n# workflow.set_entry_point(\"add\")\n# app = workflow.compile()\n# result = app.invoke(5)\n# print(result)\n```\n\n----------------------------------------\n\nTITLE: Install LangChain Dependencies\nDESCRIPTION: Installs the necessary LangChain core and OpenAI packages for the project. This is a prerequisite for running the example code. SOURCE: https://python.langchain.com/docs/how_to/query_few_shot/\n\nLANGUAGE: python\nCODE:\n```\n# %pip install -qU langchain-core langchain-openai\n```\n\n----------------------------------------\n\nTITLE: How to Use Few Shot Examples in Chat Models\nDESCRIPTION: This guide explains how to provide few-shot examples to chat models in LangChain to improve their performance on specific tasks. It demonstrates how to format prompts with examples.",
    "chunk_length": 2070
  },
  {
    "chunk_id": 10,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/how_to/output_parser_structured/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.messages import HumanMessage, SystemMessage, AIMessage\nfrom langchain_openai import ChatOpenAI\n\nchat = ChatOpenAI()\n\nmessages = [\n    SystemMessage(\n        content=\"You are a helpful assistant that translates English to French. \"\n        \"Only return the translation and nothing else.\"\n    ),\n    HumanMessage(content=\"Translate this English text to French: 'I am a helpful assistant.'\"),\n    AIMessage(content=\"Je suis un assistant utile.\"),\n    HumanMessage(content=\"Translate\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: This section provides practical guides on implementing specific features within LangChain. It covers topics such as using tools in chains, vectorstore retrievers, adding memory to chatbots, example selectors, semantic layers over graph databases, parallel runnable invocation, streaming chat responses, default invocation arguments, retrieval for chatbots, few-shot examples, function calling, package installation, query analysis, routing, and structured output from models. SOURCE: https://python.langchain.com/docs/how_to/document_loader_directory/\n\nLANGUAGE: python\nCODE:\n```\nHow to use tools in a chain\nHow to use a vectorstore as a retriever\nHow to add memory to chatbots\nHow to use example selectors\nHow to add a semantic layer over graph database\nHow to invoke runnables in parallel\nHow to stream chat model responses\nHow to add default invocation args to a Runnable\nHow to add retrieval to chatbots\nHow to use few shot examples in chat models\nHow to do tool/function calling\nHow to install LangChain packages\nHow to add examples to the prompt for query analysis\nHow to use few shot examples\nHow to run custom functions\nHow to use output parsers to parse an LLM response into structured format\nHow to handle cases where no queries are generated\nHow to route between sub-chains\nHow to return structured data from a model\n```\n\n----------------------------------------\n\nTITLE: How to Add Examples to the Prompt for Query Analysis\nDESCRIPTION: This guide explains how to include few-shot examples within prompts for query analysis tasks.",
    "chunk_length": 2235
  },
  {
    "chunk_id": 11,
    "source": "python_langchain_llms_data",
    "content": "It demonstrates how to structure prompts with examples to improve the accuracy and relevance of query analysis. SOURCE: https://python.langchain.com/docs/how_to/tool_artifacts/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.prompts import PromptTemplate\n\n# template = \"\"\"\n# Analyze the following queries and categorize them. # Query: \"What is the capital of France?\"\n# Category: Geography\n\n# Query: \"{user_query}\"\n# Category:\n# \"\"\"\n# prompt = PromptTemplate.from_template(template)\n# formatted_prompt = prompt.format(user_query=\"Who won the world series in 2020?\")\n\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides Overview\nDESCRIPTION: This section provides practical guides on using various features of LangChain, including tools in chains, vectorstore retrievers, memory for chatbots, example selectors, parallel runnable invocation, streaming chat responses, default invocation arguments, retrieval for chatbots, few-shot examples, function calling, installation, query analysis, routing, and structured output. SOURCE: https://python.langchain.com/docs/how_to/self_query/\n\nLANGUAGE: python\nCODE:\n```\n# Example: How to use tools in a chain\nfrom langchain.agents import AgentType, initialize_agent, load_tools\nfrom langchain.llms import OpenAI\n\nllm = OpenAI(temperature=0)\ntools = load_tools(['serpapi', 'llm-math'], llm=llm)\nagent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\nagent.run(\"What is the weather in San Francisco?\")\n\n# Example: How to use a vectorstore as a retriever\n# This involves creating embeddings for documents and storing them in a vectorstore. # Example: How to add memory to chatbots\n# This allows the chatbot to remember previous interactions. # Example: How to stream chat model responses\n# This enables real-time responses from the chat model. # Example: How to do tool/function calling\n# This allows LLMs to call external tools or functions. # Example: How to return structured data from a model\n# This uses output parsers to get structured output from LLM responses.",
    "chunk_length": 2071
  },
  {
    "chunk_id": 12,
    "source": "python_langchain_llms_data",
    "content": "```\n\n----------------------------------------\n\nTITLE: How to Use Few Shot Examples in Chat Models\nDESCRIPTION: This guide explains how to provide few-shot examples to chat models in LangChain to improve their performance on specific tasks. It demonstrates how to format prompts with examples. SOURCE: https://python.langchain.com/docs/how_to/output_parser_structured/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.messages import HumanMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\n\nchat = ChatOpenAI()\n\nmessages = [\n    SystemMessage(\n        content=\"You are a helpful assistant that translates English to French. \"\n        \"Only return the translation and nothing else.\"\n    ),\n    HumanMessage(content=\"Translate this English text to French: 'I am a helpful assistant.'\"),\n    # Example of a few-shot example:\n    # HumanMessage(content=\"Translate this English text to French: 'Hello world.'\"),\n    # AIMessage(content=\"Bonjour le monde.\")\n]\n\n# print(chat.invoke(messages))\n```\n\n----------------------------------------\n\nTITLE: How to install LangChain packages\nDESCRIPTION: This guide provides instructions on how to install the necessary LangChain packages for Python. It covers using pip to install the core library and common integrations, ensuring you have the right tools to start building LLM applications. SOURCE: https://python.langchain.com/docs/how_to/few_shot_examples_chat/\n\nLANGUAGE: bash\nCODE:\n```\n# Install the core LangChain library\npip install langchain\n\n# Install common integrations (e.g., OpenAI, Hugging Face)\npip install langchain-openai langchain-huggingface\n\n# Install specific components like LangChain Community\npip install langchain-community\n\n# Install LangChain partners (e.g., LangSmith)\npip install langsmith\n\n# Install LangServe for deploying chains as APIs\npip install langchain-serve\n\n# Install LangGraph for stateful, multi-agent applications\npip install langchain-graph\n\n# To install all common integrations:\npip install \"langchain[all]\"\n```\n\n----------------------------------------\n\nTITLE: Use Few Shot Examples\nDESCRIPTION: This guide explains how to incorporate few-shot examples into LangChain prompts to enhance model performance on specific tasks.",
    "chunk_length": 2213
  },
  {
    "chunk_id": 13,
    "source": "python_langchain_llms_data",
    "content": "It covers structuring prompts with examples to guide the LLM's output. SOURCE: https://python.langchain.com/docs/how_to/binding/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_openai import OpenAI\nfrom langchain_core.prompts import PromptTemplate\n\n# Initialize the LLM\nllm = OpenAI(temperature=0.7)\n\n# Define few-shot examples\nexamples = [\n    {\"input\": \"Q: What is the capital of France?\", \"output\": \"A: Paris\"},\n    {\"input\": \"Q: What is the capital of Japan?\", \"output\": \"A: Tokyo\"},\n    {\"input\": \"Q: What is the capital of Canada?\", \"output\": \"A: Ottawa\"}\n]\n\n# Format the examples into a string suitable for the prompt\nexample_str = \"\"\nfor example in examples:\n    example_str += f\"{example['input']}\\n{example['output']}\\n\\n\"\n\n# Create a prompt template that includes the formatted examples\nprompt_template = PromptTemplate(\n    input_variables=[\"input\"],\n    template=f\"{{example_str}}Q: {{input}}\\nA:\",\n    partial_variables={\"example_str\": example_str}\n)\n\n# Create a chain\nchain = prompt_template | llm\n\n# Invoke the chain with a new question\n# The model will use the provided examples to answer the question\n# result = chain.invoke(\"What is the capital of Germany?\")\n# print(result)\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: Practical guides on implementing specific features and patterns in LangChain, such as using tools, memory, vectorstores, and handling streaming responses. SOURCE: https://python.langchain.com/docs/how_to/installation/\n\nLANGUAGE: python\nCODE:\n```\nHow to use tools in a chain\nHow to use a vectorstore as a retriever\nHow to add memory to chatbots\nHow to use example selectors\nHow to add a semantic layer over graph database\nHow to invoke runnables in parallel\nHow to stream chat model responses\nHow to add default invocation args to a Runnable\nHow to add retrieval to chatbots\nHow to use few shot examples in chat models\nHow to do tool/function calling\nHow to add examples to the prompt for query analysis\nHow to use few shot examples\nHow to run custom functions\nHow to use output parsers to parse an LLM response into structured format\nHow to handle cases where no queries are generated\nHow to route between sub-chains\nHow to return structured data from a model\n```\n\n----------------------------------------\n\nTITLE: Semantic Similarity Example Selector Setup\nDESCRIPTION: Illustrates the setup for SemanticSimilarityExampleSelector, which selects examples based solely on their semantic similarity to the input.",
    "chunk_length": 2489
  },
  {
    "chunk_id": 14,
    "source": "python_langchain_llms_data",
    "content": "This is contrasted with the MMR approach to highlight the diversity aspect. It also requires examples, an embedding model, and a vector store. SOURCE: https://python.langchain.com/docs/how_to/example_selectors_mmr/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.example_selectors import (\n    MaxMarginalRelevanceExampleSelector,\n    SemanticSimilarityExampleSelector,\n)\nfrom langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\nfrom langchain_openai import OpenAIEmbeddings\n\nexample_prompt = PromptTemplate(\n    input_variables=[\"input\", \"output\"],\n    template=\"Input: {input}\\nOutput: {output}\",\n)\n\n# Examples of a pretend task of creating antonyms. examples = [\n    {\"input\": \"happy\", \"output\": \"sad\"},\n    {\"input\": \"tall\", \"output\": \"short\"},\n    {\"input\": \"energetic\", \"output\": \"lethargic\"},\n    {\"input\": \"sunny\", \"output\": \"gloomy\"},\n    {\"input\": \"windy\", \"output\": \"calm\"},\n]\n\nexample_selector = SemanticSimilarityExampleSelector.from_examples(\n    # The list of examples available to select from. examples,\n    # The embedding class used to produce embeddings which are used to measure semantic similarity. OpenAIEmbeddings(),\n    # The VectorStore class that is used to store the embeddings and do a similarity search over. FAISS,\n    # The number of examples to produce. k=2,\n)\n\nsimilar_prompt = FewShotPromptTemplate(\n    # We provide an ExampleSelector instead of examples. example_selector=example_selector,\n    example_prompt=example_prompt,\n    prefix=\"Give the antonym of every input\",\n    suffix=\"Input: {adjective}\\nOutput:\",\n    input_variables=[\"adjective\"],\n)\n\nprint(similar_prompt.format(adjective=\"worried\"))\n```\n\n----------------------------------------\n\nTITLE: LangGraph Examples\nDESCRIPTION: Features guided examples to help users get started with LangGraph, demonstrating practical applications and common use cases. SOURCE: https://langchain-ai.github.io/langgraph/\n\nLANGUAGE: python\nCODE:\n```\nhttps://langchain-ai.github.io/langgraph/examples/\n```\n\n----------------------------------------\n\nTITLE: PipelinePromptTemplate Example\nDESCRIPTION: Demonstrates creating a pipeline of prompt templates for sequential composition.",
    "chunk_length": 2223
  },
  {
    "chunk_id": 15,
    "source": "python_langchain_llms_data",
    "content": "It defines an introduction, an example, and a start prompt, then combines them into a final prompt using PipelinePromptTemplate. SOURCE: https://python.langchain.com/docs/how_to/prompts_composition/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.prompts import PipelinePromptTemplate, PromptTemplate\n\nfull_template = \"\"\"{introduction}{example}{start}\"\"\"\nfull_prompt = PromptTemplate.from_template(full_template)\n\nintroduction_template = \"\"\"You are impersonating {person}.\"\"\"\nintroduction_prompt = PromptTemplate.from_template(introduction_template)\n\nexample_template = \"\"\"Here's an example of an interaction:\\nQ: {example_q}\\nA: {example_a}\"\"\"\nexample_prompt = PromptTemplate.from_template(example_template)\n\nstart_template = \"\"\"Now, do this for real!\\nQ: {input}\\nA:\"\"\"\nstart_prompt = PromptTemplate.from_template(start_template)\n\ninput_prompts = [\n    (\"introduction\", introduction_prompt),\n    (\"example\", example_prompt),\n    (\"start\", start_prompt),\n]\n\npipeline_prompt = PipelinePromptTemplate(\n    final_prompt=full_prompt, pipeline_prompts=input_prompts\n)\n\nprint(\n    pipeline_prompt.format(\n        person=\"Elon Musk\",\n        example_q=\"What's your favorite car?\",\n        example_a=\"Tesla\",\n        input=\"What's your favorite social media site?\",\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: This section provides practical guides on how to perform specific tasks within LangChain, such as creating and querying vector stores, and loading web pages. These guides offer step-by-step instructions and code examples. SOURCE: https://python.langchain.com/docs/how_to/document_loader_web/\n\nLANGUAGE: APIDOC\nCODE:\n```\nHow to create and query vector stores:\n  Description: Steps to initialize a vector store and perform similarity searches. Dependencies: Embedding model, vector store implementation (e.g., Chroma). Example:\n    from langchain_community.vectorstores import Chroma\n    from langchain_openai import OpenAIEmbeddings\n\n    embeddings = OpenAIEmbeddings()\n    vectorstore = Chroma.from_texts(\n        [\"hello world\", \"bye world\"],\n        embeddings,\n        metadatas=[{\"source\": \"doc1\"}, {\"source\": \"doc2\"}]\n    )\n    results = vectorstore.similarity_search(\"hello\")\n\nHow to load web pages:\n  Description: Methods for fetching and parsing content from web pages.",
    "chunk_length": 2335
  },
  {
    "chunk_id": 16,
    "source": "python_langchain_llms_data",
    "content": "Dependencies: BeautifulSoup, requests. Example:\n    from langchain_community.document_loaders import WebBaseLoader\n\n    loader = WebBaseLoader(\"https://www.example.com\")\n    docs = loader.load()\n\n```\n\n----------------------------------------\n\nTITLE: How to Use Few Shot Examples in Chat Models\nDESCRIPTION: This guide details how to provide few-shot examples to chat models in LangChain. It covers formatting examples and including them in the prompt to guide the model's responses. SOURCE: https://python.langchain.com/docs/how_to/tool_artifacts/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.messages import HumanMessage, SystemMessage, AIMessage\nfrom langchain_openai import ChatOpenAI\n\n# chat = ChatOpenAI()\n# messages = [\n#     SystemMessage(content=\"You are a helpful assistant.\"),\n#     HumanMessage(content=\"Translate 'hello' to French.\"),\n#     AIMessage(content=\"Bonjour\"),\n#     HumanMessage(content=\"Translate 'goodbye' to French.\")\n# ]\n# response = chat.invoke(messages)\n\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides Overview\nDESCRIPTION: This section outlines various how-to guides for implementing specific functionalities within LangChain applications. SOURCE: https://python.langchain.com/docs/how_to/convert_runnable_to_tool/\n\nLANGUAGE: python\nCODE:\n```\nHow to use tools in a chain\nHow to use a vectorstore as a retriever\nHow to add memory to chatbots\nHow to use example selectors\nHow to add a semantic layer over graph database\nHow to invoke runnables in parallel\nHow to stream chat model responses\nHow to add default invocation args to a Runnable\nHow to add retrieval to chatbots\nHow to use few shot examples in chat models\nHow to do tool/function calling\nHow to install LangChain packages\nHow to add examples to the prompt for query analysis\nHow to use few shot examples\nHow to run custom functions\nHow to use output parsers to parse an LLM response into structured format\nHow to handle cases where no queries are generated\nHow to route between sub-chains\nHow to return structured data from a model\n```\n\n----------------------------------------\n\nTITLE: How to use example selectors\nDESCRIPTION: This guide covers the usage of example selectors in LangChain, which help in dynamically selecting few-shot examples for prompts.",
    "chunk_length": 2280
  },
  {
    "chunk_id": 17,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/how_to/chatbots_tools/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.prompts.example_selector import LengthBasedExampleSelector\nfrom langchain.prompts import FewShotPromptTemplate, PromptTemplate\n\nexamples = [\n    {\"input\": \"apple\", \"output\": \"fruit\"},\n    {\"input\": \"banana\", \"output\": \"fruit\"},\n    {\"input\": \"carrot\", \"output\": \"vegetable\"}\n]\n\nexample_prompt = PromptTemplate(input_variables=[\"input\", \"output\"], template=\"Input: {input}\\nOutput: {output}\")\n\nexample_selector = LengthBasedExampleSelector(\n    examples=examples,\n    example_prompt=example_prompt,\n    maxLength=5\n)\n\nselector_prompt = FewShotPromptTemplate(\n    example_selector=example_selector,\n    example_prompt=example_prompt,\n    prefix=\"Give the category of the input.\",\n    suffix=\"Input: {input}\\nOutput:\",\n    input_variables=[\"input\"]\n)\n```\n\n----------------------------------------\n\nTITLE: How to use example selectors\nDESCRIPTION: This guide explains how to use example selectors in LangChain to dynamically select few-shot examples for prompts. SOURCE: https://python.langchain.com/docs/tutorials/extraction/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.example_selectors import LengthBasedExampleSelector\nfrom langchain_core.prompts import PromptTemplate, FewShotPromptTemplate\nfrom langchain_openai import OpenAI\n\n# Example data\n# examples = [\n#     {\"input\": \"I love this product!\", \"output\": \"positive\"},\n#     {\"input\": \"This is terrible.\", \"output\": \"negative\"}\n# ]\n\n# Example selector\n# example_selector = LengthBasedExampleSelector(\n#     examples=examples,\n#     example_prompt=PromptTemplate(input_variables=[\"input\", \"output\"], template=\"Input: {input}\\nOutput: {output}\"),\n#     maxLength=1\n# )\n\n# Create the few-shot prompt\n# prefix = \"Classify the sentiment of the following input.\"\n# suffix = \"Input: {input}\\nOutput:\"\n# few_shot_prompt = FewShotPromptTemplate(\n#     example_selector=example_selector,\n#     example_prompt=PromptTemplate(input_variables=[\"input\", \"output\"], template=\"Input: {input}\\nOutput: {output}\"),\n#     prefix=prefix,\n#     suffix=suffix,\n#     input_variables=[\"input\"]\n# )\n\n# Example usage:\n# formatted_prompt = few_shot_prompt.format(input=\"This is okay.\")\n# print(formatted_prompt)\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides Overview\nDESCRIPTION: This section outlines various how-to guides for LangChain, demonstrating practical implementation of features like tools, memory, output parsers, and parallel execution.",
    "chunk_length": 2520
  },
  {
    "chunk_id": 18,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/how_to/multimodal_prompts/\n\nLANGUAGE: python\nCODE:\n```\nHow to use tools in a chain\nHow to use a vectorstore as a retriever\nHow to add memory to chatbots\nHow to use example selectors\nHow to add a semantic layer over graph database\nHow to invoke runnables in parallel\nHow to stream chat model responses\nHow to add default invocation args to a Runnable\nHow to add retrieval to chatbots\nHow to use few shot examples in chat models\nHow to do tool/function calling\nHow to install LangChain packages\nHow to add examples to the prompt for query analysis\nHow to use few shot examples\nHow to run custom functions\nHow to use output parsers to parse an LLM response into structured format\nHow to handle cases where no queries are generated\nHow to route between sub-chains\nHow to return structured data from a model\nHow to summarize text through parallelization\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides Overview\nDESCRIPTION: This section provides practical guides on implementing specific features and functionalities within LangChain, such as using tools, memory, and output parsers. SOURCE: https://python.langchain.com/docs/how_to/contextual_compression/\n\nLANGUAGE: markdown\nCODE:\n```\n*   [How to use tools in a chain](/docs/how_to/tools_chain/)\n*   [How to use a vectorstore as a retriever](/docs/how_to/vectorstore_retriever/)\n*   [How to add memory to chatbots](/docs/how_to/chatbots_memory/)\n*   [How to use example selectors](/docs/how_to/example_selectors/)\n*   [How to add a semantic layer over graph database](/docs/how_to/graph_semantic/)\n*   [How to invoke runnables in parallel](/docs/how_to/parallel/)\n*   [How to stream chat model responses](/docs/how_to/chat_streaming/)\n*   [How to add default invocation args to a Runnable](/docs/how_to/binding/)\n*   [How to add retrieval to chatbots](/docs/how_to/chatbots_retrieval/)\n*   [How to use few shot examples in chat models](/docs/how_to/few_shot_examples_chat/)\n*   [How to do tool/function calling](/docs/how_to/function_calling/)\n*   [How to install LangChain packages](/docs/how_to/installation/)\n*   [How to add examples to the prompt for query analysis](/docs/how_to/query_few_shot/)\n*   [How to use few shot examples](/docs/how_to/few_shot_examples/)\n*   [How to run custom functions](/docs/how_to/functions/)\n*   [How to use output parsers to parse an LLM response into structured format](/docs/how_to/output_parser_structured/)\n*   [How to handle cases where no queries are generated](/docs/how_to/query_no_queries/)\n*   [How to route between sub-chains](/docs/how_to/routing/)\n*   [How to return structured data from a model](/docs/how_to/structured_output/)\n```\n\n----------------------------------------\n\nTITLE: SambaStudio Chat Integration\nDESCRIPTION: Guides users on getting started with SambaStudio chat models.",
    "chunk_length": 2856
  },
  {
    "chunk_id": 19,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/integrations/chat/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.chat_models import ChatSambaStudio\n\n# Example usage (requires SambaStudio API key)\n# llm = ChatSambaStudio(api_key=\"YOUR_SAMBASTUDIO_API_KEY\")\n# response = llm.invoke(\"Write a Python script to read a CSV file.\")\n# print(response)\n```\n\n----------------------------------------\n\nTITLE: Pipeshift Chat Integration\nDESCRIPTION: Guides users on getting started with Pipeshift chat models. SOURCE: https://python.langchain.com/docs/integrations/chat/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.chat_models import ChatPipeshift\n\n# Example usage (requires Pipeshift API key)\n# llm = ChatPipeshift(api_key=\"YOUR_PIPESHIFT_API_KEY\")\n# response = llm.invoke(\"Write a product description for a new gadget.\")\n# print(response)\n```\n\n----------------------------------------\n\nTITLE: Use Few Shot Examples in Chat Models\nDESCRIPTION: This guide explains how to provide few-shot examples to LangChain chat models to improve their performance on specific tasks. It covers formatting examples within the prompt to guide the model's responses. SOURCE: https://python.langchain.com/docs/how_to/binding/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import HumanMessage, AIMessage\nfrom langchain_core.prompts import ChatPromptTemplate\n\n# Initialize the chat model\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n\n# Define few-shot examples\nexamples = [\n    HumanMessage(content=\"Translate English to French: see => voir\"),\n    AIMessage(content=\"Translate English to French: run => courir\"),\n    HumanMessage(content=\"Translate English to French: eat => manger\"),\n    AIMessage(content=\"Translate English to French: sleep => dormir\"),\n]\n\n# Create a prompt template that includes the examples\n# The 'messages' format is used for chat models\nchat_prompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant that translates English to French.\"),\n    *examples, # Unpack the examples into the prompt\n    (\"human\", \"Translate English to French: {input}\")\n])\n\n# Create a chain\nchain = chat_prompt | llm\n\n# Invoke the chain with a new input\n# The model will use the provided examples to guide its translation\n# result = chain.invoke({\"input\": \"walk\"})\n# print(result.content)\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides Overview\nDESCRIPTION: This section offers practical guides on implementing specific features and functionalities within LangChain, such as using tools, memory, vectorstores, and handling different output formats.",
    "chunk_length": 2649
  },
  {
    "chunk_id": 20,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/how_to/vectorstores/\n\nLANGUAGE: python\nCODE:\n```\nHow to use tools in a chain\nHow to use a vectorstore as a retriever\nHow to add memory to chatbots\nHow to use example selectors\nHow to add a semantic layer over graph database\nHow to invoke runnables in parallel\nHow to stream chat model responses\nHow to add default invocation args to a Runnable\nHow to add retrieval to chatbots\nHow to use few shot examples in chat models\nHow to do tool/function calling\nHow to install LangChain packages\nHow to add examples to the prompt for query analysis\nHow to use few shot examples\nHow to run custom functions\nHow to use output parsers to parse an LLM response into structured format\nHow to handle cases where no queries are generated\nHow to route between sub-chains\nHow to return structured data from a model\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: This section provides practical guides on implementing various features within LangChain, such as using tools, memory, output parsers, and managing installations. SOURCE: https://python.langchain.com/docs/how_to/passthrough/\n\nLANGUAGE: markdown\nCODE:\n```\n*   [How-to guides](/docs/how_to/)\n    \n    *   [How-to guides](/docs/how_to/)\n    *   [How to use tools in a chain](/docs/how_to/tools_chain/)\n    *   [How to use a vectorstore as a retriever](/docs/how_to/vectorstore_retriever/)\n    *   [How to add memory to chatbots](/docs/how_to/chatbots_memory/)\n    *   [How to use example selectors](/docs/how_to/example_selectors/)\n    *   [How to add a semantic layer over graph database](/docs/how_to/graph_semantic/)\n    *   [How to invoke runnables in parallel](/docs/how_to/parallel/)\n    *   [How to stream chat model responses](/docs/how_to/chat_streaming/)\n    *   [How to add default invocation args to a Runnable](/docs/how_to/binding/)\n    *   [How to add retrieval to chatbots](/docs/how_to/chatbots_retrieval/)\n    *   [How to use few shot examples in chat models](/docs/how_to/few_shot_examples_chat/)\n    *   [How to do tool/function calling](/docs/how_to/function_calling/)\n    *   [How to install LangChain packages](/docs/how_to/installation/)\n    *   [How to add examples to the prompt for query analysis](/docs/how_to/query_few_shot/)\n    *   [How to use few shot examples](/docs/how_to/few_shot_examples/)\n    *   [How to run custom functions](/docs/how_to/functions/)\n    *   [How to use output parsers to parse an LLM response into structured format](/docs/how_to/output_parser_structured/)\n    *   [How to handle cases where no queries are generated](/docs/how_to/query_no_queries/)\n    *   [How to route between sub-chains](/docs/how_to/routing/)\n    *   [How to return structured data from a model](/docs/how_to/structured_output/)\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: Guides on specific functionalities and implementation details within LangChain, such as using tools, vectorstores, memory, and output parsers.",
    "chunk_length": 3018
  },
  {
    "chunk_id": 21,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/how_to/example_selectors_similarity/\n\nLANGUAGE: APIDOC\nCODE:\n```\nHow-to guides:\n  - How to use tools in a chain: /docs/how_to/tools_chain/\n  - How to use a vectorstore as a retriever: /docs/how_to/vectorstore_retriever/\n  - How to add memory to chatbots: /docs/how_to/chatbots_memory/\n  - How to use example selectors: /docs/how_to/example_selectors/\n  - How to add a semantic layer over graph database: /docs/how_to/graph_semantic/\n  - How to invoke runnables in parallel: /docs/how_to/parallel/\n  - How to stream chat model responses: /docs/how_to/chat_streaming/\n  - How to add default invocation args to a Runnable: /docs/how_to/binding/\n  - How to add retrieval to chatbots: /docs/how_to/chatbots_retrieval/\n  - How to use few shot examples in chat models: /docs/how_to/few_shot_examples_chat/\n  - How to do tool/function calling: /docs/how_to/function_calling/\n  - How to install LangChain packages: /docs/how_to/installation/\n  - How to add examples to the prompt for query analysis: /docs/how_to/query_few_shot/\n  - How to use few shot examples: /docs/how_to/few_shot_examples/\n  - How to run custom functions: /docs/how_to/functions/\n  - How to use output parsers to parse an LLM response into structured format: /docs/how_to/output_parser_structured/\n  - How to handle cases where no queries are generated: /docs/how_to/query_no_queries/\n  - How to route between sub-chains: /docs/how_to/routing/\n  - How to return structured data from a model: /docs/how_to/structured_output/\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides Overview\nDESCRIPTION: Lists practical guides for implementing specific functionalities within LangChain applications. SOURCE: https://python.langchain.com/docs/how_to/summarize_refine/\n\nLANGUAGE: python\nCODE:\n```\n# How-to guides available:\n# - How to use tools in a chain\n# - How to use a vectorstore as a retriever\n# - How to add memory to chatbots\n# - How to use example selectors\n# - How to add a semantic layer over graph database\n# - How to invoke runnables in parallel\n# - How to stream chat model responses\n# - How to add default invocation args to a Runnable\n# - How to add retrieval to chatbots\n# - How to use few shot examples in chat models\n# - How to do tool/function calling\n# - How to install LangChain packages\n# - How to add examples to the prompt for query analysis\n# - How to use few shot examples\n# - How to run custom functions\n# - How to use output parsers to parse an LLM response into structured format\n# - How to handle cases where no queries are generated\n# - How to route between sub-chains\n# - How to return structured data from a model\n```\n\n----------------------------------------\n\nTITLE: Install LangChain Packages\nDESCRIPTION: This guide provides instructions on how to install LangChain and its related packages.",
    "chunk_length": 2851
  },
  {
    "chunk_id": 22,
    "source": "python_langchain_llms_data",
    "content": "It covers using pip for installation and managing dependencies for different LangChain components. SOURCE: https://python.langchain.com/docs/how_to/binding/\n\nLANGUAGE: python\nCODE:\n```\n# Install the core LangChain library\npip install langchain\n\n# Install specific integrations, e.g., for OpenAI models\npip install langchain-openai\n\n# Install for vectorstores, e.g., FAISS\npip install langchain-community\n\n# Install for specific tools or frameworks\n# pip install langchain-aws\n# pip install langchain-google-genai\n# pip install langchain-anthropic\n\n# To install all community integrations (use with caution):\n# pip install \"langchain[all]\"\n\n# To install specific optional dependencies:\n# pip install langchain-core\n# pip install langchain-text-splitters\n\n# Example: Installing LangChain with OpenAI and FAISS support\n# pip install langchain langchain-openai langchain-community\n\n# Verify installation\n# python -c \"import langchain; print(langchain.__version__)\"\n```\n\n----------------------------------------\n\nTITLE: Netmind Chat Integration\nDESCRIPTION: Guides users on getting started with Netmind chat models. SOURCE: https://python.langchain.com/docs/integrations/chat/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.chat_models import ChatNetmind\n\n# Example usage (requires Netmind API key)\n# llm = ChatNetmind(api_key=\"YOUR_NETMIND_API_KEY\")\n# response = llm.invoke(\"What are the benefits of mindfulness?\")\n# print(response)\n```\n\n----------------------------------------\n\nTITLE: How to Install LangChain Packages\nDESCRIPTION: This guide provides instructions on installing LangChain and its related packages. It covers using pip to install the core library and optional dependencies for specific integrations. SOURCE: https://python.langchain.com/docs/how_to/tool_artifacts/\n\nLANGUAGE: bash\nCODE:\n```\npip install langchain\n# For specific integrations, e.g., OpenAI:\n# pip install langchain-openai\n\n```\n\n----------------------------------------\n\nTITLE: How to use few shot examples\nDESCRIPTION: This guide explains the general concept and implementation of few-shot learning in LangChain.",
    "chunk_length": 2097
  },
  {
    "chunk_id": 23,
    "source": "python_langchain_llms_data",
    "content": "It covers how to structure prompts with examples to guide the language model's responses, improving accuracy and relevance for specific tasks. SOURCE: https://python.langchain.com/docs/how_to/few_shot_examples_chat/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.prompts import PromptTemplate\n\n# Define a prompt template with few-shot examples\nprompt = PromptTemplate(\n    input_variables=[\"query\"],\n    template=[\n        \"Q: What is the capital of France?\",\n        \"A: Paris\",\n        \"Q: What is the capital of Germany?\",\n        \"A: Berlin\",\n        \"Q: {query}\",\n        \"A:\"\n    ].join(\"\\n\")\n)\n\n# Example usage\n# formatted_prompt = prompt.format(query=\"What is the capital of Spain?\")\n# print(formatted_prompt)\n```\n\n----------------------------------------\n\nTITLE: How to use few shot examples\nDESCRIPTION: This how-to guide covers the general usage of few-shot examples in LangChain. Providing examples can significantly improve the accuracy and relevance of model outputs. SOURCE: https://python.langchain.com/docs/tutorials/sql_qa/\n\nLANGUAGE: python\nCODE:\n```\nprint(\"How-to: How to use few shot examples\")\n# Further implementation details would follow here. ```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides Overview\nDESCRIPTION: This section lists various how-to guides for LangChain, covering practical implementation details for common tasks and features. SOURCE: https://python.langchain.com/docs/how_to/qa_sources/\n\nLANGUAGE: markdown\nCODE:\n```\n*   [How-to guides](/docs/how_to/)\n*   [How to use tools in a chain](/docs/how_to/tools_chain/)\n*   [How to use a vectorstore as a retriever](/docs/how_to/vectorstore_retriever/)\n*   [How to add memory to chatbots](/docs/how_to/chatbots_memory/)\n*   [How to use example selectors](/docs/how_to/example_selectors/)\n*   [How to add a semantic layer over graph database](/docs/how_to/graph_semantic/)\n*   [How to invoke runnables in parallel](/docs/how_to/parallel/)\n*   [How to stream chat model responses](/docs/how_to/chat_streaming/)\n*   [How to add default invocation args to a Runnable](/docs/how_to/binding/)\n*   [How to add retrieval to chatbots](/docs/how_to/chatbots_retrieval/)\n*   [How to use few shot examples in chat models](/docs/how_to/few_shot_examples_chat/)\n*   [How to do tool/function calling](/docs/how_to/function_calling/)\n*   [How to install LangChain packages](/docs/how_to/installation/)\n*   [How to add examples to the prompt for query analysis](/docs/how_to/query_few_shot/)\n*   [How to use few shot examples](/docs/how_to/few_shot_examples/)\n*   [How to run custom functions](/docs/how_to/functions/)\n*   [How to use output parsers to parse an LLM response into structured format](/docs/how_to/output_parser_structured/)\n*   [How to handle cases where no queries are generated](/docs/how_to/query_no_queries/)\n*   [How to route between sub-chains](/docs/how_to/routing/)\n*   [How to return structured data from a model](/docs/how_to/structured_output/)\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: This section provides practical guides on how to perform specific tasks within LangChain.",
    "chunk_length": 3149
  },
  {
    "chunk_id": 24,
    "source": "python_langchain_llms_data",
    "content": "It covers using tools in chains, vectorstores as retrievers, adding memory to chatbots, using example selectors, adding semantic layers, invoking runnables in parallel, streaming chat responses, adding default invocation args, adding retrieval to chatbots, using few-shot examples, function calling, package installation, query analysis, routing, structured output, and summarization through parallelization. SOURCE: https://python.langchain.com/docs/concepts/chat_models/\n\nLANGUAGE: python\nCODE:\n```\nHow to use tools in a chain\nHow to use a vectorstore as a retriever\nHow to add memory to chatbots\nHow to use example selectors\nHow to add a semantic layer over graph database\nHow to invoke runnables in parallel\nHow to stream chat model responses\nHow to add default invocation args to a Runnable\nHow to add retrieval to chatbots\nHow to use few shot examples in chat models\nHow to do tool/function calling\nHow to install LangChain packages\nHow to add examples to the prompt for query analysis\nHow to use few shot examples\nHow to run custom functions\nHow to use output parsers to parse an LLM response into structured format\nHow to handle cases where no queries are generated\nHow to route between sub-chains\nHow to return structured data from a model\nHow to summarize text through parallelization\n```\n\n----------------------------------------\n\nTITLE: Volc Engine Maas Chat Integration\nDESCRIPTION: Provides a guide on getting started with Volc Engine Maas chat models. SOURCE: https://python.langchain.com/docs/integrations/chat/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.chat_models import ChatVolcEngineMaas\n\n# Example usage (requires Volc Engine credentials)\n# llm = ChatVolcEngineMaas(secret_id=\"YOUR_SECRET_ID\", secret_key=\"YOUR_SECRET_KEY\")\n# response = llm.invoke(\"Write a short story about a robot.\")\n# print(response)\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: Practical guides on implementing specific features and functionalities within LangChain. Includes topics like using tools, vectorstores, memory, example selectors, parallel execution, streaming, and function calling.",
    "chunk_length": 2143
  },
  {
    "chunk_id": 25,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/concepts/structured_outputs/\n\nLANGUAGE: APIDOC\nCODE:\n```\nHow-to guides:\n  - How to use tools in a chain: /docs/how_to/tools_chain/\n  - How to use a vectorstore as a retriever: /docs/how_to/vectorstore_retriever/\n  - How to add memory to chatbots: /docs/how_to/chatbots_memory/\n  - How to use example selectors: /docs/how_to/example_selectors/\n  - How to add a semantic layer over graph database: /docs/how_to/graph_semantic/\n  - How to invoke runnables in parallel: /docs/how_to/parallel/\n  - How to stream chat model responses: /docs/how_to/chat_streaming/\n  - How to add default invocation args to a Runnable: /docs/how_to/binding/\n  - How to add retrieval to chatbots: /docs/how_to/chatbots_retrieval/\n  - How to use few shot examples in chat models: /docs/how_to/few_shot_examples_chat/\n  - How to do tool/function calling: /docs/how_to/function_calling/\n  - How to install LangChain packages: /docs/how_to/installation/\n  - How to add examples to the prompt for query analysis: /docs/how_to/query_few_shot/\n  - How to use few shot examples: /docs/how_to/few_shot_examples/\n  - How to run custom functions: /docs/how_to/functions/\n  - How to use output parsers to parse an LLM response into structured format: /docs/how_to/output_parser_structured/\n  - How to handle cases where no queries are generated: /docs/how_to/query_no_queries/\n  - How to route between sub-chains: /docs/how_to/routing/\n  - How to return structured data from a model: /docs/how_to/structured_output/\n  - How to summarize text through parallelization: /docs/how_to/summarize_map_reduce/\n```\n\n----------------------------------------\n\nTITLE: Few-Shot Prompting Setup\nDESCRIPTION: Constructs a few-shot prompt using `ChatPromptTemplate` and a list of example messages (`AIMessage`, `HumanMessage`, `ToolMessage`). This prompt guides the language model on how to correctly use tools for mathematical operations, especially concerning the order of operations. SOURCE: https://python.langchain.com/docs/how_to/tools_few_shot/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.messages import AIMessage, HumanMessage, ToolMessage\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnablePassthrough\n\nexamples = [\n    HumanMessage(\n        \"What's the product of 317253 and 128472 plus four\", name=\"example_user\"\n    ),\n    AIMessage(\n        \"\",\n        name=\"example_assistant\",\n        tool_calls=[\n            {\"name\": \"Multiply\", \"args\": {\"x\": 317253, \"y\": 128472}, \"id\": \"1\"}\n        ],\n    ),\n    ToolMessage(\"16505054784\", tool_call_id=\"1\"),\n    AIMessage(\n        \"\",\n        name=\"example_assistant\",\n        tool_calls=[{\"name\": \"Add\", \"args\": {\"x\": 16505054784, \"y\": 4}, \"id\": \"2\"}],\n    ),\n    ToolMessage(\"16505054788\", tool_call_id=\"2\"),\n    AIMessage(\n        \"The product of 317253 and 128472 plus four is 16505054788\",\n        name=\"example_assistant\",\n    ),\n]\nsystem = \"\"\"You are bad at math but are an expert at using a calculator.",
    "chunk_length": 3005
  },
  {
    "chunk_id": 26,
    "source": "python_langchain_llms_data",
    "content": "Use past tool usage as an example of how to correctly use the tools.\"\"\"\n\nfew_shot_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        *examples,\n        (\"human\", \"{query}\"),\n    ]\n)\n\nchain = {\"query\": RunnablePassthrough()} | few_shot_prompt | llm_with_tools\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides Overview\nDESCRIPTION: Offers practical guides on implementing specific features and functionalities within LangChain applications, including tool usage, memory, and streaming. SOURCE: https://python.langchain.com/docs/how_to/tools_few_shot/\n\nLANGUAGE: markdown\nCODE:\n```\n*   [How-to guides](/docs/how_to/)\n*   [How to use tools in a chain](/docs/how_to/tools_chain/)\n*   [How to use a vectorstore as a retriever](/docs/how_to/vectorstore_retriever/)\n*   [How to add memory to chatbots](/docs/how_to/chatbots_memory/)\n*   [How to use example selectors](/docs/how_to/example_selectors/)\n*   [How to add a semantic layer over graph database](/docs/how_to/graph_semantic/)\n*   [How to invoke runnables in parallel](/docs/how_to/parallel/)\n*   [How to stream chat model responses](/docs/how_to/chat_streaming/)\n*   [How to add default invocation args to a Runnable](/docs/how_to/binding/)\n*   [How to add retrieval to chatbots](/docs/how_to/chatbots_retrieval/)\n*   [How to use few shot examples in chat models](/docs/how_to/few_shot_examples_chat/)\n*   [How to do tool/function calling](/docs/how_to/function_calling/)\n*   [How to install LangChain packages](/docs/how_to/installation/)\n*   [How to add examples to the prompt for query analysis](/docs/how_to/query_few_shot/)\n*   [How to use few shot examples](/docs/how_to/few_shot_examples/)\n*   [How to run custom functions](/docs/how_to/functions/)\n*   [How to use output parsers to parse an LLM response into structured format](/docs/how_to/output_parser_structured/)\n*   [How to handle cases where no queries are generated](/docs/how_to/query_no_queries/)\n*   [How to route between sub-chains](/docs/how_to/routing/)\n*   [How to return structured data from a model](/docs/how_to/structured_output/)\n```\n\n----------------------------------------\n\nTITLE: Few-Shot Examples in LangChain\nDESCRIPTION: Explains how to incorporate few-shot examples into prompts to guide language model outputs.",
    "chunk_length": 2311
  },
  {
    "chunk_id": 27,
    "source": "python_langchain_llms_data",
    "content": "This includes creating formatters, constructing example sets, and utilizing example selectors like `SemanticSimilarityExampleSelector` for dynamic example retrieval. SOURCE: https://context7_llms\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.prompts import FewShotPromptTemplate, PromptTemplate\nfrom langchain.prompts.example_selector import SemanticSimilarityExampleSelector\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import OpenAIEmbeddings\n\n# Example usage:\n# examples = [\n#     {\"question\": \"Q: What is the capital of France?\", \"answer\": \"A: Paris\"},\n#     {\"question\": \"Q: What is the capital of Germany?\", \"answer\": \"A: Berlin\"}\n# ]\n# example_prompt = PromptTemplate(input_variables=[\"question\", \"answer\"], template=\"{question}\\n{answer}\")\n# selector = SemanticSimilarityExampleSelector.from_examples(\n#     examples,\n#     OpenAIEmbeddings(),\n#     Chroma,\n#     k=1\n# )\n# few_shot_prompt = FewShotPromptTemplate(\n#     example_selector=selector,\n#     example_prompt=example_prompt,\n#     prefix=\"Here are some examples:\",\n#     suffix=\"Question: {input}\\nAnswer: \"\n# )\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guide - Select Examples by Similarity\nDESCRIPTION: This guide details how to select examples for few-shot prompting based on similarity. It involves embedding a set of examples and then finding the most similar ones to a given input query, which helps in providing relevant context to the LLM. SOURCE: https://python.langchain.com/docs/how_to/example_selectors_similarity/\n\nLANGUAGE: python\nCODE:\n```\n# Conceptual example of selecting examples by similarity:\n# from langchain_openai import OpenAIEmbeddings\n# from langchain_community.vectorstores import FAISS\n# from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n\n# examples = [\n#     {\"input\": \"...', \"output\": \"...\"},\n#     # ... more examples\n# ]\n\n# # Embed examples and create a vector store\n# embeddings = OpenAIEmbeddings()\n# example_selector = ... # Logic to select examples based on similarity\n\n# # Create a FewShotPromptTemplate using the selector\n# example_prompt = PromptTemplate(input_variables=[\"input\", \"output\"], template=\"Input: {input}\\nOutput: {output}\")\n# prompt = FewShotPromptTemplate(\n#     example_selector=example_selector,\n#     example_prompt=example_prompt,\n#     prefix=\"Given the following examples:\",\n#     suffix=\"Input: {input}\\nOutput:\",\n#     input_variables=[\"input\"],\n# )\n\n# # Use the prompt with an LLM\n# # formatted_prompt = prompt.format(input=\"New input query\")\n\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: Practical guides on implementing specific features and functionalities within LangChain, such as adding memory, using tools, and handling streaming responses.",
    "chunk_length": 2790
  },
  {
    "chunk_id": 28,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/how_to/example_selectors_ngram/\n\nLANGUAGE: APIDOC\nCODE:\n```\nLangChain How-to Guides:\n  - Installation: How to install LangChain packages. - Tools: Using tools in a LangChain chain. - Memory: Adding memory to chatbots. - Example Selectors: Utilizing example selectors for LLM prompts. - Parallel Execution: Invoking runnables in parallel. - Streaming: Streaming chat model responses. - Function Calling: Implementing tool/function calling. - Output Parsers: Parsing LLM responses into structured formats. - Routing: Routing between sub-chains. - Structured Output: Returning structured data from models. ```\n\n----------------------------------------\n\nTITLE: Getting Started with Yi Chat Models\nDESCRIPTION: This guide helps users begin with Yi chat models. It offers detailed documentation for integrating and utilizing Yi models effectively within the LangChain ecosystem. SOURCE: https://python.langchain.com/docs/integrations/chat/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.chat_models import ChatYi\n\n# Example usage (assuming Yi API credentials are set)\n# chat = ChatYi()\n# response = chat.invoke(\"What are the capabilities of Yi models?\")\n# print(response.content)\n```\n\n----------------------------------------\n\nTITLE: How to use example selectors\nDESCRIPTION: This guide explains how to use example selectors in LangChain, which help in dynamically selecting few-shot examples for prompts. SOURCE: https://python.langchain.com/docs/concepts/rag/\n\nLANGUAGE: python\nCODE:\n```\nprint('How-to: How to use example selectors')\n```\n\n----------------------------------------\n\nTITLE: Streaming Runnables in LangChain\nDESCRIPTION: This guide explains how to stream runnables in LangChain, enabling real-time output for LLM applications. It includes examples and setup instructions for integrating streaming capabilities. SOURCE: https://python.langchain.com/docs/how_to/streaming/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.runnables import Runnable\n\ndef stream_runnable(runnable: Runnable, input_data: dict):\n    \"\"\"Streams output from a LangChain runnable.\"\"\"\n    for chunk in runnable.stream(input_data):\n        print(chunk)\n\n# Example usage (assuming 'my_runnable' is defined elsewhere)\n# stream_runnable(my_runnable, {\"input\": \"Hello, world!\"})\n```\n\n----------------------------------------\n\nTITLE: How to use few shot examples\nDESCRIPTION: This guide covers the use of few-shot examples in LangChain to enhance model performance.",
    "chunk_length": 2488
  },
  {
    "chunk_id": 29,
    "source": "python_langchain_llms_data",
    "content": "By providing a few input-output pairs, you can guide the model towards generating more accurate and relevant responses. SOURCE: https://python.langchain.com/docs/how_to/tool_choice/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_openai import OpenAI\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMChain\n\n# Define the prompt template with few-shot examples\ntemplate = \"\"\"\nTranslate English to French:\n\nEnglish: Hello world\nFrench: Bonjour le monde\n\nEnglish: How are you? French: Comment allez-vous? English: {english_text}\nFrench: \"\"\"\n\nprompt = PromptTemplate(template=template, input_variables=[\"english_text\"])\n\n# Initialize the LLM\nllm = OpenAI()\n\n# Create an LLMChain\nllm_chain = LLMChain(prompt=prompt, llm=llm)\n\n# Run the chain with a new input\nresponse = llm_chain.run(\"I love programming.\")\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: How to use example selectors\nDESCRIPTION: This guide explains how to use example selectors in LangChain, which help in dynamically selecting few-shot examples for prompts based on input. SOURCE: https://python.langchain.com/docs/how_to/chatbots_memory/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.prompts.example_selector import SemanticSimilarityExampleSelector\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import OpenAIEmbeddings\n\nexample_selector = SemanticSimilarityExampleSelector.from_examples(\n    examples, OpenAIEmbeddings(), Chroma, k=1, \n)\n\n```\n\n----------------------------------------\n\nTITLE: How to use example selectors\nDESCRIPTION: This guide covers the use of example selectors in LangChain, which help in dynamically selecting relevant examples to include in prompts for few-shot learning. SOURCE: https://python.langchain.com/docs/how_to/extraction_examples/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\nfrom langchain_openai import ChatOpenAI\nfrom langchain.chains import LLMChain\n\n# Define few-shot examplesexamples = [\n    {\"input\": \"I love dogs.\", \"output\": \"I love cats.\", \"input_language\": \"English\", \"output_language\": \"English\"},\n    {\"input\": \"I love apples.\", \"output\": \"I love oranges.\", \"input_language\": \"English\", \"output_language\": \"English\"},\n]\n\n# Create a prompt template for the examplesexample_prompt = ChatPromptTemplate.from_messages([\n    (\"human\", \"{input}\"),\n    (\"ai\", \"{output}\"),\n])\n\n# Create the FewShotChatMessagePromptTemplate\few_shot_prompt = FewShotChatMessagePromptTemplate(\n    examples=examples,\n    example_prompt=example_prompt,\n    input_variables=[\"input\", \"input_language\", \"output_language\"],\n    example_separator=\"\\n\\n\",\n)\n\n# Create the final prompt template\final_prompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant that translates English to French.\"),\n    few_shot_prompt,\n    (\"human\", \"{input}\"),\n])\n\n# Initialize LLM and create chainllm = ChatOpenAI(model=\"gpt-3.5-turbo\")\nchain = LLMChain(llm=llm, prompt=final_prompt)\n\n# Example usage\nesult = chain.invoke({\"input\": \"I love bananas.\", \"input_language\": \"English\", \"output_language\": \"French\"})\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: SemanticSimilarityExampleSelector Setup\nDESCRIPTION: Demonstrates setting up SemanticSimilarityExampleSelector to dynamically select relevant few-shot examples based on semantic similarity.",
    "chunk_length": 3393
  },
  {
    "chunk_id": 30,
    "source": "python_langchain_llms_data",
    "content": "It uses OpenAIEmbeddings and Neo4jVector for example selection. SOURCE: https://python.langchain.com/docs/tutorials/graph/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.example_selectors import SemanticSimilarityExampleSelector\nfrom langchain_neo4j import Neo4jVector\nfrom langchain_openai import OpenAIEmbeddings\n\nexamples = [\n    {\n        \"question\": \"How many artists are there?\",\n        \"query\": \"MATCH (a:Person)-[:ACTED_IN]->(:Movie) RETURN count(DISTINCT a)\",\n    },\n    {\n        \"question\": \"Which actors played in the movie Casino?\",\n        \"query\": \"MATCH (m:Movie {title: 'Casino'})<-[:ACTED_IN]-(a) RETURN a.name\",\n    },\n    {\n        \"question\": \"How many movies has Tom Hanks acted in?\",\n        \"query\": \"MATCH (a:Person {name: 'Tom Hanks'})-[:ACTED_IN]->(m:Movie) RETURN count(m)\",\n    },\n    {\n        \"question\": \"List all the genres of the movie Schindler's List\",\n        \"query\": \"MATCH (m:Movie {title: 'Schindler's List'})-[:IN_GENRE]->(g:Genre) RETURN g.name\",\n    },\n    {\n        \"question\": \"Which actors have worked in movies from both the comedy and action genres?\",\n        \"query\": \"MATCH (a:Person)-[:ACTED_IN]->(:Movie)-[:IN_GENRE]->(g1:Genre), (a)-[:ACTED_IN]->(:Movie)-[:IN_GENRE]->(g2:Genre) WHERE g1.name = 'Comedy' AND g2.name = 'Action' RETURN DISTINCT a.name\",\n    },\n    {\n        \"question\": \"Which directors have made movies with at least three different actors named 'John'?\",\n        \"query\": \"MATCH (d:Person)-[:DIRECTED]->(m:Movie)<-[:ACTED_IN]-(a:Person) WHERE a.name STARTS WITH 'John' WITH d, COUNT(DISTINCT a) AS JohnsCount WHERE JohnsCount >= 3 RETURN d.name\",\n    },\n    {\n        \"question\": \"Identify movies where directors also played a role in the film.\",\n        \"query\": \"MATCH (p:Person)-[:DIRECTED]->(m:Movie), (p)-[:ACTED_IN]->(m) RETURN m.title, p.name\",\n    },\n    {\n        \"question\": \"Find the actor with the highest number of movies in the database.\",\n        \"query\": \"MATCH (a:Actor)-[:ACTED_IN]->(m:Movie) RETURN a.name, COUNT(m) AS movieCount ORDER BY movieCount DESC LIMIT 1\",\n    },\n]\n\nexample_selector = SemanticSimilarityExampleSelector.from_examples(\n    examples,\n    OpenAIEmbeddings(),\n    Neo4jVector,\n    k=5,\n    input_keys=[\"question\"]\n)\n```\n\n----------------------------------------\n\nTITLE: How-to Guides Overview\nDESCRIPTION: Lists various how-to guides for implementing specific features and patterns in LangChain, such as using tools, memory, and output parsers.",
    "chunk_length": 2457
  },
  {
    "chunk_id": 31,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/concepts/prompt_templates/\n\nLANGUAGE: MARKDOWN\nCODE:\n```\nHow-to guides\n*   How to use tools in a chain\n*   How to use a vectorstore as a retriever\n*   How to add memory to chatbots\n*   How to use example selectors\n*   How to add a semantic layer over graph database\n*   How to invoke runnables in parallel\n*   How to stream chat model responses\n*   How to add default invocation args to a Runnable\n*   How to add retrieval to chatbots\n*   How to use few shot examples in chat models\n*   How to do tool/function calling\n*   How to install LangChain packages\n*   How to add examples to the prompt for query analysis\n*   How to use few shot examples\n*   How to run custom functions\n*   How to use output parsers to parse an LLM response into structured format\n*   How to handle cases where no queries are generated\n*   How to route between sub-chains\n*   How to return structured data from a model\n*   How to summarize text through parallelization\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides Overview\nDESCRIPTION: This section provides practical guides on implementing specific features and functionalities within LangChain. It details how to integrate tools, manage memory, handle streaming, and utilize various components for building robust LLM applications. SOURCE: https://python.langchain.com/docs/how_to/example_selectors_length_based/\n\nLANGUAGE: python\nCODE:\n```\nHow to use tools in a chain\nHow to use a vectorstore as a retriever\nHow to add memory to chatbots\nHow to use example selectors\nHow to add a semantic layer over graph database\nHow to invoke runnables in parallel\nHow to stream chat model responses\nHow to add default invocation args to a Runnable\nHow to add retrieval to chatbots\nHow to use few shot examples in chat models\nHow to do tool/function calling\nHow to install LangChain packages\nHow to add examples to the prompt for query analysis\nHow to use few shot examples\nHow to run custom functions\nHow to use output parsers to parse an LLM response into structured format\nHow to handle cases where no queries are generated\nHow to route between sub-chains\nHow to return structured data from a model\n```\n\n----------------------------------------\n\nTITLE: How to add examples to the prompt for query analysis\nDESCRIPTION: This guide explains how to include examples in prompts specifically for query analysis tasks.",
    "chunk_length": 2405
  },
  {
    "chunk_id": 32,
    "source": "python_langchain_llms_data",
    "content": "By providing sample queries and their desired analysis, you can guide the LLM to perform more accurate and consistent query interpretation. SOURCE: https://python.langchain.com/docs/how_to/few_shot_examples_chat/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.prompts import PromptTemplate\n\n# Define a prompt template with examples for query analysis\nquery_analysis_prompt = PromptTemplate(\n    input_variables=[\"query\"],\n    template=[\n        \"Analyze the following user queries and categorize them.\",\n        \"Query: 'What is the weather in London?' -> Category: Weather\",\n        \"Query: 'Set a timer for 5 minutes' -> Category: Timer\",\n        \"Query: 'Play some jazz music' -> Category: Music\",\n        \"Query: '{query}' -> Category:\"\n    ].join(\"\\n\")\n)\n\n# Example usage\n# formatted_prompt = query_analysis_prompt.format(query=\"What is the capital of France?\")\n# print(formatted_prompt)\n```\n\n----------------------------------------\n\nTITLE: Install LangChain Packages\nDESCRIPTION: Learn how to install the main LangChain package and various ecosystem packages like langchain-core, langchain-community, langchain-openai, and others. This covers the fundamental setup for using LangChain. SOURCE: https://context7_llms\n\nLANGUAGE: bash\nCODE:\n```\npip install langchain\npip install langchain-core langchain-community langchain-openai langchain-experimental langgraph langserve langchain-cli langsmith\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: Practical guides on implementing specific features and functionalities within LangChain applications, such as adding memory, using vectorstores, and handling streaming responses. SOURCE: https://python.langchain.com/docs/how_to/tool_runtime/\n\nLANGUAGE: python\nCODE:\n```\n# How to use tools in a chain\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_community.tools import DuckDuckGoSearchRun\nfrom langchain_openai import ChatOpenAI\n\n# Initialize LLM and Tool\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\")\nsearch = DuckDuckGoSearchRun()\n\n# Define a prompt template that incorporates tool usage instructions\n# This is a simplified example; actual tool integration often involves agent frameworks\n# or specific chain types designed for tool use.",
    "chunk_length": 2307
  },
  {
    "chunk_id": 33,
    "source": "python_langchain_llms_data",
    "content": "prompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. You can use the search tool.\"),\n    (\"user\", \"{input}\"),\n])\n\n# A simple chain that might use a tool (conceptual example)\n# In practice, tool use is often managed by agents or specific chain types. # This example shows how a tool *could* be integrated if the LLM is instructed to use it. # For direct tool invocation within a chain without an agent, you'd typically use LCEL's\n# ability to pass tool outputs back into the chain. # Conceptual chain structure:\n# chain = prompt | llm_with_tool_calling_capability | output_parser\n\n# Example of invoking a tool directly (not within a chain context here):\n# print(search.run(\"LangChain documentation\"))\nprint(\"Guide on using tools in a chain - conceptual example.\")\n\n```\n\nLANGUAGE: python\nCODE:\n```\n# How to add memory to chatbots\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_openai import ChatOpenAI\nfrom langchain.memory import ChatMessageHistory\nfrom langchain.chains import ConversationChain\n\n# Initialize LLM\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n\n# Initialize memory\n# Using ChatMessageHistory for simple in-memory storage\n# For more complex scenarios, consider ConversationBufferMemory, etc. message_history = ChatMessageHistory()\n\n# Define a prompt template that includes memory\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. You have a memory of the conversation.\"),\n    MessagesPlaceholder(variable_name=\"history\"),\n    (\"user\", \"{input}\"),\n])\n\n# Create a conversation chain with memory\n# Note: The exact way to integrate memory depends on the chain type. # For LCEL, you'd typically pass the history as part of the input dict. # Example using ConversationChain (a higher-level abstraction):\n# conversation = ConversationChain(llm=llm, memory=ChatMessageHistory(), verbose=True)\n# print(conversation.predict(input=\"Hi there!\"))\n# print(conversation.predict(input=\"I'm doing well, thank you!\"))\n\n# Example using LCEL with explicit history passing:\nchain_with_memory = (\n    {\n        \"history\": RunnablePassthrough(),\n        \"input\": lambda x: x[\"input\"],\n    }\n    | prompt\n    | llm\n)\n\n# To use this, you'd manage the history externally:\n# history_input = {\"history\": message_history.messages, \"input\": \"Hello!\"}\n# response = chain_with_memory.invoke(history_input)\n# message_history.add_user_message(history_input[\"input\"])\n# message_history.add_ai_message(response.content)\n\nprint(\"Guide on adding memory to chatbots - conceptual example.\")\n\n```\n\nLANGUAGE: python\nCODE:\n```\n# How to stream chat model responses\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\n\n# Initialize LLM\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n\n# Define a prompt template\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant.",
    "chunk_length": 2972
  },
  {
    "chunk_id": 34,
    "source": "python_langchain_llms_data",
    "content": "Respond concisely.\"),\n    (\"user\", \"Tell me a short story about a brave knight.\")\n])\n\n# Create the chain\nchain = prompt | llm\n\n# Stream the response\nprint(\"Streaming response...\")\nfor chunk in chain.stream({}):\n    print(chunk.content, end=\"\", flush=True)\nprint(\"\\nStreaming complete.\")\n\n```\n\nLANGUAGE: python\nCODE:\n```\n# How to use output parsers to parse an LLM response into structured format\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import JsonOutputParser\nfrom langchain_core.pydantic_v1 import BaseModel, Field\n\n# Define the desired output structure using Pydantic\nclass Person(BaseModel):\n    name: str = Field(description=\"The person's name\")\n    age: int = Field(description=\"The person's age\")\n    city: str = Field(description=\"The city where the person lives\")\n\n# Initialize LLM\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n\n# Define a prompt template\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Extract information about a person.\"),\n    (\"user\", \"Tell me about John Doe, who is 30 years old and lives in New York.\")\n])\n\n# Create the chain with JsonOutputParser\n# Note: For Pydantic models, use PydanticOutputParser\n# json_chain = prompt | llm | JsonOutputParser()\n\n# Using PydanticOutputParser for structured output\nfrom langchain_core.output_parsers import PydanticOutputParser\npydantic_parser = PydanticOutputParser(pydantic_object=Person)\n\npydantic_chain = prompt | llm | pydantic_parser\n\n# Run the chain\n# response = pydantic_chain.invoke({})\n# print(response)\nprint(\"Guide on using output parsers - conceptual example.\")\n\n```\n\nLANGUAGE: python\nCODE:\n```\n# How to return structured data from a model\n# This is often achieved using output parsers, particularly PydanticOutputParser. from langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import PydanticOutputParser\nfrom langchain_core.pydantic_v1 import BaseModel, Field\n\n# Define the desired output structure using Pydantic\nclass ProductInfo(BaseModel):\n    product_name: str = Field(description=\"Name of the product\")\n    price: float = Field(description=\"Price of the product\")\n    in_stock: bool = Field(description=\"Whether the product is in stock\")\n\n# Initialize LLM\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n\n# Define a prompt template\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are an assistant that extracts product information.\"),\n    (\"user\", \"Extract details for the 'SuperWidget' which costs $19.99 and is in stock.\")\n])\n\n# Create the chain with PydanticOutputParser\noutput_parser = PydanticOutputParser(pydantic_object=ProductInfo)\n\nstructured_chain = prompt | llm | output_parser\n\n# Run the chain\n# product_data = structured_chain.invoke({})\n# print(product_data)\nprint(\"Guide on returning structured data - conceptual example.\")\n\n```\n\n----------------------------------------\n\nTITLE: How to use few shot examples\nDESCRIPTION: This guide covers the general usage of few-shot examples in LangChain, explaining how to provide examples to improve model performance on various tasks.",
    "chunk_length": 3183
  },
  {
    "chunk_id": 35,
    "source": "python_langchain_llms_data",
    "content": "It emphasizes the importance of well-crafted examples for better results. SOURCE: https://python.langchain.com/docs/how_to/query_few_shot/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n\n# Define example entries\nexamples = [\n    {\n        \"input\": \"The weather is sunny.\",\n        \"output\": \"positive\"\n    },\n    {\n        \"input\": \"I am feeling sad today.\",\n        \"output\": \"negative\"\n    }\n]\n\n# Define the template for each example\nexample_template = \"Input: {input}\\nOutput: {output}\"\n\n# Create the prompt template for examples\nexample_prompt = PromptTemplate(\n    input_variables=[\"input\", \"output\"],\n    template=example_template\n)\n\n# Define the overall prompt template\nprompt = FewShotPromptTemplate(\n    examples=examples,\n    example_prompt=example_prompt,\n    prefix=\"Classify the sentiment of the following sentences.\",\n    suffix=\"Input: {input}\\nOutput:\",\n    input_variables=[\"input\"],\n    example_separator=\"\\n\"\n)\n\n# Example usage\ninput_text = \"This is a fantastic day!\"\nformatted_prompt = prompt.format(input=input_text)\nprint(formatted_prompt)\n\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: This section provides practical guides on using specific LangChain features and functionalities, such as tools, vectorstores, memory, example selectors, parallel execution, streaming, function calling, and output parsing. SOURCE: https://python.langchain.com/docs/how_to/prompts_composition/\n\nLANGUAGE: python\nCODE:\n```\nHow to use tools in a chain\nHow to use a vectorstore as a retriever\nHow to add memory to chatbots\nHow to use example selectors\nHow to add a semantic layer over graph database\nHow to invoke runnables in parallel\nHow to stream chat model responses\nHow to add default invocation args to a Runnable\nHow to add retrieval to chatbots\nHow to use few shot examples in chat models\nHow to do tool/function calling\nHow to install LangChain packages\nHow to add examples to the prompt for query analysis\nHow to use few shot examples\nHow to run custom functions\nHow to use output parsers to parse an LLM response into structured format\nHow to handle cases where no queries are generated\nHow to route between sub-chains\nHow to return structured data from a model\nHow to summarize text through parallelization\n```\n\n----------------------------------------\n\nTITLE: LangChain Tutorials\nDESCRIPTION: Guides for building various applications using LangChain, from simple LLM interactions to complex retrieval-augmented generation systems.",
    "chunk_length": 2546
  },
  {
    "chunk_id": 36,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/how_to/migrate_agent/\n\nLANGUAGE: python\nCODE:\n```\n# Build a Question Answering application over a Graph Database\n# ... (details omitted for brevity)\n\n# Build a simple LLM application with chat models and prompt templates\n# ... (details omitted for brevity)\n\n# Build a Chatbot\n# ... (details omitted for brevity)\n\n# Build a Retrieval Augmented Generation (RAG) App: Part 2\n# ... (details omitted for brevity)\n\n# Build an Extraction Chain\n# ... (details omitted for brevity)\n\n# Build an Agent\n# ... (details omitted for brevity)\n\n# Tagging\n# ... (details omitted for brevity)\n\n# Build a Retrieval Augmented Generation (RAG) App: Part 1\n# ... (details omitted for brevity)\n\n# Build a semantic search engine\n# ... (details omitted for brevity)\n\n# Build a Question/Answering system over SQL data\n# ... (details omitted for brevity)\n\n# Summarize Text\n# ... (details omitted for brevity)\n```\n\n----------------------------------------\n\nTITLE: How to use example selectors\nDESCRIPTION: This guide explains how to use example selectors in LangChain, which help in dynamically selecting few-shot examples to include in prompts based on the input. SOURCE: https://python.langchain.com/docs/how_to/extraction_parse/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.output_parsers import StrOutputParser\n\n# Define few-shot examples\nexamples = [\n    {\"input\": \"I am happy\", \"output\": \"I am sad\"},\n    {\"input\": \"I am fast\", \"output\": \"I am slow\"},\n]\n\n# Create a prompt template for the examples\nexample_prompt = ChatPromptTemplate.from_messages([\n    (\"human\", \"{input}\"),\n    (\"ai\", \"{output}\")\n])\n\n# Create a FewShotChatMessagePromptTemplate\nfew_shot_prompt = FewShotChatMessagePromptTemplate(\n    example_prompt=example_prompt,\n    examples=examples,\n)\n\n# Create the final prompt template\nfinal_prompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant that provides antonyms.\"),\n    few_shot_prompt,\n    (\"human\", \"{input}\")\n])\n\n# Initialize the language model and create the chain\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\")\nchain = final_prompt | llm | StrOutputParser()\n\n# Example usage\nresponse = chain.invoke({\"input\": \"I am big\"})\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: How to use few shot examples in chat models\nDESCRIPTION: This how-to guide explains how to provide few-shot examples to chat models in LangChain to improve their performance and guide their responses.",
    "chunk_length": 2591
  },
  {
    "chunk_id": 37,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/tutorials/rag/\n\nLANGUAGE: python\nCODE:\n```\n# This is a placeholder for the actual code in the how-to guide. # The guide focuses on the concepts and steps involved in using few-shot examples. # For specific code examples, please refer to the official LangChain documentation. # Example conceptual steps:\n# 1. Prepare a list of example input/output pairs\n# 2. Format these examples into the chat model's message history\n# 3. Send the examples along with the user's query to the chat model\n```\n\n----------------------------------------\n\nTITLE: How to Install LangChain Packages\nDESCRIPTION: This guide provides instructions on how to install the necessary LangChain packages for your Python environment. It covers the use of pip for managing LangChain dependencies. SOURCE: https://python.langchain.com/docs/how_to/output_parser_structured/\n\nLANGUAGE: bash\nCODE:\n```\npip install langchain\n```\n\n----------------------------------------\n\nTITLE: LangChain Tutorials\nDESCRIPTION: A collection of tutorials demonstrating various use cases and functionalities within LangChain, from simple LLM applications to complex agents and RAG systems. SOURCE: https://python.langchain.com/docs/how_to/installation/\n\nLANGUAGE: python\nCODE:\n```\nBuild a Question Answering application over a Graph Database\nBuild a simple LLM application with chat models and prompt templates\nBuild a Chatbot\nBuild a Retrieval Augmented Generation (RAG) App: Part 2\nBuild an Extraction Chain\nBuild an Agent\nTagging\nBuild a Retrieval Augmented Generation (RAG) App: Part 1\nBuild a semantic search engine\nBuild a Question/Answering system over SQL data\nSummarize Text\n```\n\n----------------------------------------\n\nTITLE: How to install LangChain packages\nDESCRIPTION: This guide provides instructions on how to install the necessary LangChain packages for your project. SOURCE: https://python.langchain.com/docs/tutorials/extraction/\n\nLANGUAGE: bash\nCODE:\n```\npip install langchain\n# For specific integrations, e.g., OpenAI:\n# pip install langchain-openai\n# For LangChain community packages:\n# pip install langchain-community\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides Overview\nDESCRIPTION: This section provides guides on how to implement specific features and functionalities within LangChain.",
    "chunk_length": 2332
  },
  {
    "chunk_id": 38,
    "source": "python_langchain_llms_data",
    "content": "It covers topics like using tools, memory, vectorstores, streaming, and more. SOURCE: https://python.langchain.com/docs/how_to/document_loader_office_file/\n\nLANGUAGE: markdown\nCODE:\n```\n*   [How to use tools in a chain](/docs/how_to/tools_chain/)\n*   [How to use a vectorstore as a retriever](/docs/how_to/vectorstore_retriever/)\n*   [How to add memory to chatbots](/docs/how_to/chatbots_memory/)\n*   [How to use example selectors](/docs/how_to/example_selectors/)\n*   [How to add a semantic layer over graph database](/docs/how_to/graph_semantic/)\n*   [How to invoke runnables in parallel](/docs/how_to/parallel/)\n*   [How to stream chat model responses](/docs/how_to/chat_streaming/)\n*   [How to add default invocation args to a Runnable](/docs/how_to/binding/)\n*   [How to add retrieval to chatbots](/docs/how_to/chatbots_retrieval/)\n*   [How to use few shot examples in chat models](/docs/how_to/few_shot_examples_chat/)\n*   [How to do tool/function calling](/docs/how_to/function_calling/)\n*   [How to install LangChain packages](/docs/how_to/installation/)\n*   [How to add examples to the prompt for query analysis](/docs/how_to/query_few_shot/)\n*   [How to use few shot examples](/docs/how_to/few_shot_examples/)\n*   [How to run custom functions](/docs/how_to/functions/)\n*   [How to use output parsers to parse an LLM response into structured format](/docs/how_to/output_parser_structured/)\n*   [How to handle cases where no queries are generated](/docs/how_to/query_no_queries/)\n*   [How to route between sub-chains](/docs/how_to/routing/)\n*   [How to return structured data from a model](/docs/how_to/structured_output/)\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: This section provides practical guides on implementing specific features within LangChain. It details how to use tools, vectorstores, memory, example selectors, parallel execution, streaming, function calling, and more. SOURCE: https://python.langchain.com/docs/concepts/retrievers/\n\nLANGUAGE: python\nCODE:\n```\nHow to use tools in a chain\nHow to use a vectorstore as a retriever\nHow to add memory to chatbots\nHow to use example selectors\nHow to add a semantic layer over graph database\nHow to invoke runnables in parallel\nHow to stream chat model responses\nHow to add default invocation args to a Runnable\nHow to add retrieval to chatbots\nHow to use few shot examples in chat models\nHow to do tool/function calling\nHow to install LangChain packages\nHow to add examples to the prompt for query analysis\nHow to use few shot examples\nHow to run custom functions\nHow to use output parsers to parse an LLM response into structured format\nHow to handle cases where no queries are generated\nHow to route between sub-chains\nHow to return structured data from a model\nHow to summarize text through parallelization\n```\n\n----------------------------------------\n\nTITLE: How to Use Example Selectors\nDESCRIPTION: This guide explains how to use example selectors in LangChain to dynamically select relevant examples for prompts.",
    "chunk_length": 3037
  },
  {
    "chunk_id": 39,
    "source": "python_langchain_llms_data",
    "content": "This is particularly useful in few-shot learning scenarios to provide the most pertinent examples to the language model. SOURCE: https://python.langchain.com/docs/how_to/custom_tools/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.prompts.example_selector import SemanticSimilarityExampleSelector\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.prompts import FewShotPromptTemplate, PromptTemplate\n\n# Example dataexamples = [\n    {\"question\": \"What is the capital of France?\", \"answer\": \"Paris\"},\n    {\"question\": \"What is the capital of Germany?\", \"answer\": \"Berlin\"},\n    {\"question\": \"What is the capital of Spain?\", \"answer\": \"Madrid\"},\n    {\"question\": \"What is the capital of Italy?\", \"answer\": \"Rome\"},\n    {\"question\": \"What is the capital of Japan?\", \"answer\": \"Tokyo\"}\n]\n\n# Create embeddings and a vector store for examplesembeddings = OpenAIEmbeddings()\nvectorstore = Chroma.from_texts([ex['question'] for ex in examples], embeddings, metadatas=[{'answer': ex['answer']} for ex in examples])\n\n# Create an example selectorexample_selector = SemanticSimilarityExampleSelector(\n    vectorstore=vectorstore,\n    k=2,  # Select top 2 similar examples\n    input_keys=[\"question\"], # Key to use for similarity search\n    example_keys=[\"answer\"], # Key to use for example output\n)\n\n# Define a prompt template for the examplesexample_prompt = PromptTemplate(\n    input_variables=[\"question\"],\n    template=\"Question: {question}\\nAnswer: {answer}\",\n)\n\n# Create a FewShotPromptTemplate\few_shot_prompt = FewShotPromptTemplate(\n    example_selector=example_selector,\n    example_prompt=example_prompt,\n    prefix=\"Here are some examples of question answering:\",\n    suffix=\"Question: {question}\\nAnswer: \"\n)\n\n# Use the prompt template\formatted_prompt = few_shot_prompt.format(question=\"What is the capital of Canada?\")\nprint(formatted_prompt)\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides Overview\nDESCRIPTION: Practical guides on implementing specific features and functionalities within LangChain applications.",
    "chunk_length": 2097
  },
  {
    "chunk_id": 40,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/how_to/document_loader_web/\n\nLANGUAGE: APIDOC\nCODE:\n```\nHow-to Guides:\n  - How to use tools in a chain\n  - How to use a vectorstore as a retriever\n  - How to add memory to chatbots\n  - How to use example selectors\n  - How to add a semantic layer over graph database\n  - How to invoke runnables in parallel\n  - How to stream chat model responses\n  - How to add default invocation args to a Runnable\n  - How to add retrieval to chatbots\n  - How to use few shot examples in chat models\n  - How to do tool/function calling\n  - How to install LangChain packages\n  - How to add examples to the prompt for query analysis\n  - How to use few shot examples\n  - How to run custom functions\n  - How to use output parsers to parse an LLM response into structured format\n  - How to handle cases where no queries are generated\n  - How to route between sub-chains\n  - How to return structured data from a model\n  - How to summarize text through parallelization\n```\n\n----------------------------------------\n\nTITLE: How to use few shot examples\nDESCRIPTION: This guide explains how to effectively use few-shot examples in LangChain prompts to guide LLM behavior and improve response quality for specific tasks. SOURCE: https://python.langchain.com/docs/how_to/chatbots_memory/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.prompts import FewShotPromptTemplate\n\nexample_formatter_template = \"Input: {input}\\nOutput: {output}\"\n\nexample_prompt = PromptTemplate(\n    input_variables=[\"input\", \"output\"],\n    template=example_formatter_template,\n)\n\n```\n\n----------------------------------------\n\nTITLE: How to use example selectors\nDESCRIPTION: This guide details how to use example selectors in LangChain, which dynamically select relevant examples to include in the prompt based on the input. This is useful for managing prompt length and improving efficiency when dealing with a large number of examples. SOURCE: https://python.langchain.com/docs/how_to/few_shot_examples_chat/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.prompts import ChatPromptTemplate, FewShotPromptTemplate, PromptExample\nfrom langchain_core.example_selectors import LengthBasedExampleSelector\n\n# Define example messages\nexample_1 = PromptExample(input=\"What is the capital of France?\", output=\"Paris\")\nexample_2 = PromptExample(input=\"What is the capital of Germany?\", output=\"Berlin\")\nexample_3 = PromptExample(input=\"What is the capital of Spain?\", output=\"Madrid\")\n\nexamples = [example_1, example_2, example_3]\n\n# Create an example selector\nexample_selector = LengthBasedExampleSelector(\n    examples=examples,\n    example_prompt=ChatPromptTemplate.from_messages([\n        (\"human\", \"{input}\"),\n        (\"ai\", \"{output}\")\n    ]),\n    maxLength=100  # Maximum length of the prompt\n)\n\n# Create a FewShotPromptTemplate using the selector\nfew_shot_prompt = FewShotPromptTemplate(\n    example_selector=example_selector,\n    example_prompt=ChatPromptTemplate.from_messages([\n        (\"human\", \"{input}\"),\n        (\"ai\", \"{output}\")\n    ]),\n    input_variables=[\"input\"],\n    prefix=\"Here are some examples:\",\n    suffix=\"\\nQuestion: {input}\\nAnswer:\"\n)\n\n# Example usage\n# formatted_prompt = few_shot_prompt.format(input=\"What is the capital of Italy?\")\n# print(formatted_prompt)\n```\n\n----------------------------------------\n\nTITLE: How to use example selectors\nDESCRIPTION: This guide details how to use example selectors in LangChain, which dynamically select relevant examples to include in the prompt based on the input.",
    "chunk_length": 3518
  },
  {
    "chunk_id": 41,
    "source": "python_langchain_llms_data",
    "content": "This is useful for managing prompt length and improving efficiency when dealing with a large number of examples. SOURCE: https://python.langchain.com/docs/how_to/few_shot_examples_chat/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.prompts import ChatPromptTemplate, FewShotPromptTemplate, PromptExample\nfrom langchain_core.example_selectors import LengthBasedExampleSelector\n\n# Define example messages\nexample_1 = PromptExample(input=\"What is the capital of France?\", output=\"Paris\")\nexample_2 = PromptExample(input=\"What is the capital of Germany?\", output=\"Berlin\")\nexample_3 = PromptExample(input=\"What is the capital of Spain?\", output=\"Madrid\")\n\nexamples = [example_1, example_2, example_3]\n\n# Create an example selector\nexample_selector = LengthBasedExampleSelector(\n    examples=examples,\n    example_prompt=ChatPromptTemplate.from_messages([\n        (\"human\", \"{input}\"),\n        (\"ai\", \"{output}\")\n    ]),\n    maxLength=100  # Maximum length of the prompt\n)\n\n# Create a FewShotPromptTemplate using the selector\nfew_shot_prompt = FewShotPromptTemplate(\n    example_selector=example_selector,\n    example_prompt=ChatPromptTemplate.from_messages([\n        (\"human\", \"{input}\"),\n        (\"ai\", \"{output}\")\n    ]),\n    input_variables=[\"input\"],\n    prefix=\"Here are some examples:\",\n    suffix=\"\\nQuestion: {input}\\nAnswer:\"\n)\n\n# Example usage\n# formatted_prompt = few_shot_prompt.format(input=\"What is the capital of Italy?\")\n# print(formatted_prompt)\n```\n\n----------------------------------------\n\nTITLE: Example Selectors\nDESCRIPTION: This guide explains example selectors in LangChain, which are used to dynamically select few-shot examples for prompts. It covers different strategies for choosing the most relevant examples based on the input. SOURCE: https://python.langchain.com/docs/how_to/callbacks_attach/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.prompts.example_selector import SemanticSimilarityExampleSelector\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.prompts import FewShotPromptTemplate, PromptTemplate\n\n# Assume 'examples' is a list of dictionaries, each with 'input' and 'output' keys\n# Assume 'example_selector' is configured to use a vector store and embeddings\n\n# example_selector = SemanticSimilarityExampleSelector.from_examples(\n#     examples, OpenAIEmbeddings(), Chroma, k=2\n# )\n\n# prompt_template = PromptTemplate(input_variables=[\"input\"], template=\"...\")\n# few_shot_prompt = FewShotPromptTemplate(example_selector=example_selector, ...)\n\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides Overview\nDESCRIPTION: This section lists various how-to guides for LangChain, covering practical implementation details for common tasks and features.",
    "chunk_length": 2769
  },
  {
    "chunk_id": 42,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/how_to/sql_csv/\n\nLANGUAGE: markdown\nCODE:\n```\n*   [How-to guides](/docs/how_to/)\n*   [How to use tools in a chain](/docs/how_to/tools_chain/)\n*   [How to use a vectorstore as a retriever](/docs/how_to/vectorstore_retriever/)\n*   [How to add memory to chatbots](/docs/how_to/chatbots_memory/)\n*   [How to use example selectors](/docs/how_to/example_selectors/)\n*   [How to add a semantic layer over graph database](/docs/how_to/graph_semantic/)\n*   [How to invoke runnables in parallel](/docs/how_to/parallel/)\n*   [How to stream chat model responses](/docs/how_to/chat_streaming/)\n*   [How to add default invocation args to a Runnable](/docs/how_to/binding/)\n*   [How to add retrieval to chatbots](/docs/how_to/chatbots_retrieval/)\n*   [How to use few shot examples in chat models](/docs/how_to/few_shot_examples_chat/)\n*   [How to do tool/function calling](/docs/how_to/function_calling/)\n*   [How to install LangChain packages](/docs/how_to/installation/)\n*   [How to add examples to the prompt for query analysis](/docs/how_to/query_few_shot/)\n*   [How to use few shot examples](/docs/how_to/few_shot_examples/)\n*   [How to run custom functions](/docs/how_to/functions/)\n*   [How to use output parsers to parse an LLM response into structured format](/docs/how_to/output_parser_structured/)\n*   [How to handle cases where no queries are generated](/docs/how_to/query_no_queries/)\n*   [How to route between sub-chains](/docs/how_to/routing/)\n*   [How to return structured data from a model](/docs/how_to/structured_output/)\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools. This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/how_to/binding/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant.",
    "chunk_length": 2610
  },
  {
    "chunk_id": 43,
    "source": "python_langchain_llms_data",
    "content": "Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: How to install LangChain packages\nDESCRIPTION: This guide provides instructions on how to install the necessary LangChain packages for your project. It covers different installation methods and common dependencies. SOURCE: https://python.langchain.com/docs/how_to/chatbots_memory/\n\nLANGUAGE: bash\nCODE:\n```\npip install langchain\n\n```\n\n----------------------------------------\n\nTITLE: Getting Started with xAI Chat Models\nDESCRIPTION: This page helps users get started with xAI chat models. It provides detailed documentation on how to integrate and use xAI models within the LangChain framework. SOURCE: https://python.langchain.com/docs/integrations/chat/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.chat_models import ChatXai\n\n# Example usage (assuming you have xAI credentials configured)\n# chat = ChatXai()\n# response = chat.invoke(\"Hello, how are you?\")\n# print(response.content)\n```\n\n----------------------------------------\n\nTITLE: Tutorials Overview\nDESCRIPTION: Lists various tutorials available for building LLM applications with LangChain, covering topics from basic LLM applications to agents and RAG. SOURCE: https://python.langchain.com/docs/concepts/prompt_templates/\n\nLANGUAGE: MARKDOWN\nCODE:\n```\nTutorials\n*   [Build a Question Answering application over a Graph Database](/docs/tutorials/graph/)\n*   [Build a simple LLM application with chat models and prompt templates](/docs/tutorials/llm_chain/)\n*   [Build a Chatbot](/docs/tutorials/chatbot/)\n*   [Build a Retrieval Augmented Generation (RAG) App: Part 2](/docs/tutorials/qa_chat_history/)\n*   [Build an Extraction Chain](/docs/tutorials/extraction/)\n*   [Build an Agent](/docs/tutorials/agents/)\n*   [Tagging](/docs/tutorials/classification/)\n*   [Build a Retrieval Augmented Generation (RAG) App: Part 1](/docs/tutorials/rag/)\n*   [Build a semantic search engine](/docs/tutorials/retrievers/)\n*   [Build a Question/Answering system over SQL data](/docs/tutorials/sql_qa/)\n*   [Summarize Text](/docs/tutorials/summarization/)\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides Overview\nDESCRIPTION: This section provides practical guides on how to implement specific features and functionalities within LangChain.",
    "chunk_length": 2967
  },
  {
    "chunk_id": 44,
    "source": "python_langchain_llms_data",
    "content": "It covers topics such as using tools, vectorstores, memory, parallel execution, streaming, and more. SOURCE: https://python.langchain.com/docs/concepts/vectorstores/\n\nLANGUAGE: python\nCODE:\n```\nHow to use tools in a chain\nHow to use a vectorstore as a retriever\nHow to add memory to chatbots\nHow to use example selectors\nHow to add a semantic layer over graph database\nHow to invoke runnables in parallel\nHow to stream chat model responses\nHow to add default invocation args to a Runnable\nHow to add retrieval to chatbots\nHow to use few shot examples in chat models\nHow to do tool/function calling\nHow to install LangChain packages\nHow to add examples to the prompt for query analysis\nHow to use few shot examples\nHow to run custom functions\nHow to use output parsers to parse an LLM response into structured format\nHow to handle cases where no queries are generated\nHow to route between sub-chains\nHow to return structured data from a model\nHow to summarize text through parallelization\n```\n\n----------------------------------------\n\nTITLE: How to install LangChain packages\nDESCRIPTION: This guide provides instructions on how to install the necessary LangChain packages for your project. SOURCE: https://python.langchain.com/docs/how_to/chatbots_tools/\n\nLANGUAGE: bash\nCODE:\n```\npip install langchain\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install langchain-community\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install langchain-openai\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: Details various how-to guides for implementing specific functionalities within LangChain, including tool usage, vectorstore retrieval, memory, prompt examples, and more. SOURCE: https://python.langchain.com/docs/how_to/graph_semantic/\n\nLANGUAGE: APIDOC\nCODE:\n```\nHow-to guides:\n  - How to use tools in a chain: /docs/how_to/tools_chain/\n  - How to use a vectorstore as a retriever: /docs/how_to/vectorstore_retriever/\n  - How to add memory to chatbots: /docs/how_to/chatbots_memory/\n  - How to use example selectors: /docs/how_to/example_selectors/\n  - How to add a semantic layer over graph database: /docs/how_to/graph_semantic/\n  - How to invoke runnables in parallel: /docs/how_to/parallel/\n  - How to stream chat model responses: /docs/how_to/chat_streaming/\n  - How to add default invocation args to a Runnable: /docs/how_to/binding/\n  - How to add retrieval to chatbots: /docs/how_to/chatbots_retrieval/\n  - How to use few shot examples in chat models: /docs/how_to/few_shot_examples_chat/\n  - How to do tool/function calling: /docs/how_to/function_calling/\n  - How to install LangChain packages: /docs/how_to/installation/\n  - How to add examples to the prompt for query analysis: /docs/how_to/query_few_shot/\n  - How to use few shot examples: /docs/how_to/few_shot_examples/\n  - How to run custom functions: /docs/how_to/functions/\n  - How to use output parsers to parse an LLM response into structured format: /docs/how_to/output_parser_structured/\n  - How to handle cases where no queries are generated: /docs/how_to/query_no_queries/\n  - How to route between sub-chains: /docs/how_to/routing/\n  - How to return structured data from a model: /docs/how_to/structured_output/\n```\n\n----------------------------------------\n\nTITLE: Install Langchain Packages\nDESCRIPTION: Installs the necessary Langchain and Langchain-Community packages for the project.",
    "chunk_length": 3377
  },
  {
    "chunk_id": 45,
    "source": "python_langchain_llms_data",
    "content": "Also includes optional setup for LangSmith tracing. SOURCE: https://python.langchain.com/docs/how_to/tools_prompting/\n\nLANGUAGE: python\nCODE:\n```\n%pip install --upgrade --quiet langchain langchain-community\n\nimport getpass\nimport os\n# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides Overview\nDESCRIPTION: Details various how-to guides for implementing specific functionalities within LangChain applications, such as using tools, vectorstores, memory, and handling streaming responses. SOURCE: https://python.langchain.com/docs/how_to/tool_configure/\n\nLANGUAGE: python\nCODE:\n```\nHow to use tools in a chain\nHow to use a vectorstore as a retriever\nHow to add memory to chatbots\nHow to use example selectors\nHow to add a semantic layer over graph database\nHow to invoke runnables in parallel\nHow to stream chat model responses\nHow to add default invocation args to a Runnable\nHow to add retrieval to chatbots\nHow to use few shot examples in chat models\nHow to do tool/function calling\nHow to install LangChain packages\nHow to add examples to the prompt for query analysis\nHow to use few shot examples\nHow to run custom functions\nHow to use output parsers to parse an LLM response into structured format\nHow to handle cases where no queries are generated\nHow to route between sub-chains\nHow to return structured data from a model\n```\n\n----------------------------------------\n\nTITLE: LangChain Installation\nDESCRIPTION: This section details how to install LangChain packages. It covers the general installation process and provides links to more specific guides for different functionalities and versions. SOURCE: https://python.langchain.com/docs/how_to/installation/\n\nLANGUAGE: python\nCODE:\n```\npip install langchain\n```\n\n----------------------------------------\n\nTITLE: How to add examples to the prompt for query analysis\nDESCRIPTION: This how-to guide explains how to include examples within prompts for query analysis. This can help the language model better understand and process queries.",
    "chunk_length": 2117
  },
  {
    "chunk_id": 46,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/tutorials/sql_qa/\n\nLANGUAGE: python\nCODE:\n```\nprint(\"How-to: How to add examples to the prompt for query analysis\")\n# Further implementation details would follow here. ```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides Overview\nDESCRIPTION: Offers practical guides on implementing specific functionalities within LangChain, such as using tools, managing memory, handling output parsing, and optimizing chains. SOURCE: https://python.langchain.com/docs/concepts/callbacks/\n\nLANGUAGE: python\nCODE:\n```\nHow to use tools in a chain\nHow to use a vectorstore as a retriever\nHow to add memory to chatbots\nHow to use example selectors\nHow to add a semantic layer over graph database\nHow to invoke runnables in parallel\nHow to stream chat model responses\nHow to add default invocation args to a Runnable\nHow to add retrieval to chatbots\nHow to use few shot examples in chat models\nHow to do tool/function calling\nHow to install LangChain packages\nHow to add examples to the prompt for query analysis\nHow to use few shot examples\nHow to run custom functions\nHow to use output parsers to parse an LLM response into structured format\nHow to handle cases where no queries are generated\nHow to route between sub-chains\nHow to return structured data from a model\nHow to summarize text through parallelization\n```\n\n----------------------------------------\n\nTITLE: How to add examples to the prompt for query analysis\nDESCRIPTION: This guide explains how to incorporate few-shot examples into prompts for query analysis, improving the model's understanding and performance. SOURCE: https://python.langchain.com/docs/how_to/sql_query_checking/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_community.llms import OpenAI\n\n# Assume llm is an initialized language model instance\n# Assume db_chain is a pre-configured chain for database interaction\n\n# Example prompt with few-shot examples for query analysis\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant that analyzes SQL queries.\"),\n    (\"human\", \"Analyze the following query: {query}\\nExamples:\\nQuery: SELECT * FROM users WHERE age > 30\\nAnalysis: This query selects all columns from the users table for users older than 30.\\n\\nQuery: SELECT COUNT(*) FROM orders\\nAnalysis: This query counts the total number of records in the orders table.\"),\n    (\"human\", \"Analyze the following query: {query}\")\n])\n\n# Chain to analyze the query with few-shot examples\nquery_analysis_chain = prompt | llm | StrOutputParser()\n\n# Example usage:\nquery_to_analyze = \"SELECT name FROM products WHERE price < 100\"\nanalysis_result = query_analysis_chain.invoke({\"query\": query_to_analyze})\nprint(f\"Analysis: {analysis_result}\")\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: Practical guides on implementing specific features and functionalities within LangChain, such as using tools, memory, output parsers, and parallel execution.",
    "chunk_length": 3162
  },
  {
    "chunk_id": 47,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/how_to/chat_models_universal_init/\n\nLANGUAGE: APIDOC\nCODE:\n```\nHow-to Guides:\n  - How to use tools in a chain\n    URL: /docs/how_to/tools_chain/\n  - How to use a vectorstore as a retriever\n    URL: /docs/how_to/vectorstore_retriever/\n  - How to add memory to chatbots\n    URL: /docs/how_to/chatbots_memory/\n  - How to use example selectors\n    URL: /docs/how_to/example_selectors/\n  - How to add a semantic layer over graph database\n    URL: /docs/how_to/graph_semantic/\n  - How to invoke runnables in parallel\n    URL: /docs/how_to/parallel/\n  - How to stream chat model responses\n    URL: /docs/how_to/chat_streaming/\n  - How to add default invocation args to a Runnable\n    URL: /docs/how_to/binding/\n  - How to add retrieval to chatbots\n    URL: /docs/how_to/chatbots_retrieval/\n  - How to use few shot examples in chat models\n    URL: /docs/how_to/few_shot_examples_chat/\n  - How to do tool/function calling\n    URL: /docs/how_to/function_calling/\n  - How to install LangChain packages\n    URL: /docs/how_to/installation/\n  - How to add examples to the prompt for query analysis\n    URL: /docs/how_to/query_few_shot/\n  - How to use few shot examples\n    URL: /docs/how_to/few_shot_examples/\n  - How to run custom functions\n    URL: /docs/how_to/functions/\n  - How to use output parsers to parse an LLM response into structured format\n    URL: /docs/how_to/output_parser_structured/\n  - How to handle cases where no queries are generated\n    URL: /docs/how_to/query_no_queries/\n  - How to route between sub-chains\n    URL: /docs/how_to/routing/\n  - How to return structured data from a model\n    URL: /docs/how_to/structured_output/\n```\n\n----------------------------------------\n\nTITLE: LangChain Tutorials Overview\nDESCRIPTION: Lists various tutorials available for building different types of applications with LangChain, including QA, LLM chains, chatbots, agents, RAG, and more. SOURCE: https://python.langchain.com/docs/how_to/example_selectors_mmr/\n\nLANGUAGE: text\nCODE:\n```\nBuild a Question Answering application over a Graph Database\nBuild a simple LLM application with chat models and prompt templates\nBuild a Chatbot\nBuild a Retrieval Augmented Generation (RAG) App: Part 2\nBuild an Extraction Chain\nBuild an Agent\nTagging\nBuild a Retrieval Augmented Generation (RAG) App: Part 1\nBuild a semantic search engine\nBuild a Question/Answering system over SQL data\nSummarize Text\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides Overview\nDESCRIPTION: A list of how-to guides for implementing specific features and functionalities in LangChain.",
    "chunk_length": 2622
  },
  {
    "chunk_id": 48,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/tutorials/summarization/\n\nLANGUAGE: APIDOC\nCODE:\n```\nHow to use tools in a chain\nHow to use a vectorstore as a retriever\nHow to add memory to chatbots\nHow to use example selectors\nHow to add a semantic layer over graph database\nHow to invoke runnables in parallel\nHow to stream chat model responses\nHow to add default invocation args to a Runnable\nHow to add retrieval to chatbots\nHow to use few shot examples in chat models\nHow to do tool/function calling\nHow to install LangChain packages\nHow to add examples to the prompt for query analysis\nHow to use few shot examples\nHow to run custom functions\nHow to use output parsers to parse an LLM response into structured format\nHow to handle cases where no queries are generated\nHow to route between sub-chains\nHow to return structured data from a model\nHow to summarize text through parallelization\n```\n\n----------------------------------------\n\nTITLE: How to install LangChain packages\nDESCRIPTION: This how-to guide provides instructions on how to install the necessary LangChain packages and their dependencies. SOURCE: https://python.langchain.com/docs/tutorials/rag/\n\nLANGUAGE: python\nCODE:\n```\n# This is a placeholder for the actual code in the how-to guide. # The guide focuses on the installation process. # For specific commands, please refer to the official LangChain documentation. # Example installation command:\n# pip install langchain\n\n# Installation of specific integrations might require additional packages:\n# pip install langchain-openai\n# pip install langchain-community\n```\n\n----------------------------------------\n\nTITLE: LangChain Tutorials Overview\nDESCRIPTION: This section lists various tutorials available for LangChain, covering a wide range of LLM application development tasks. SOURCE: https://python.langchain.com/docs/how_to/convert_runnable_to_tool/\n\nLANGUAGE: python\nCODE:\n```\nBuild a Question Answering application over a Graph Database\nBuild a simple LLM application with chat models and prompt templates\nBuild a Chatbot\nBuild a Retrieval Augmented Generation (RAG) App: Part 2\nBuild an Extraction Chain\nBuild an Agent\nTagging\nBuild a Retrieval Augmented Generation (RAG) App: Part 1\nBuild a semantic search engine\nBuild a Question/Answering system over SQL data\nSummarize Text\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides Overview\nDESCRIPTION: Provides a collection of how-to guides for common tasks and advanced features in LangChain, such as using tools, memory, vectorstores, and handling specific output formats.",
    "chunk_length": 2576
  },
  {
    "chunk_id": 49,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/concepts/few_shot_prompting/\n\nLANGUAGE: APIDOC\nCODE:\n```\nHow-to Guides:\n  - How to use tools in a chain\n  - How to use a vectorstore as a retriever\n  - How to add memory to chatbots\n  - How to use example selectors\n  - How to add a semantic layer over graph database\n  - How to invoke runnables in parallel\n  - How to stream chat model responses\n  - How to add default invocation args to a Runnable\n  - How to add retrieval to chatbots\n  - How to use few shot examples in chat models\n  - How to do tool/function calling\n  - How to install LangChain packages\n  - How to add examples to the prompt for query analysis\n  - How to use few shot examples\n  - How to run custom functions\n  - How to use output parsers to parse an LLM response into structured format\n  - How to handle cases where no queries are generated\n  - How to route between sub-chains\n  - How to return structured data from a model\n  - How to summarize text through parallelization\n\nThese guides offer practical solutions and best practices for integrating LangChain into your projects. ```\n\n----------------------------------------\n\nTITLE: LangChain Tutorials Overview\nDESCRIPTION: Lists various tutorials available for building different types of LLM applications with LangChain. SOURCE: https://python.langchain.com/docs/how_to/summarize_refine/\n\nLANGUAGE: python\nCODE:\n```\n# Tutorials available:\n# - Build a Question Answering application over a Graph Database\n# - Build a simple LLM application with chat models and prompt templates\n# - Build a Chatbot\n# - Build a Retrieval Augmented Generation (RAG) App: Part 2\n# - Build an Extraction Chain\n# - Build an Agent\n# - Tagging\n# - Build a Retrieval Augmented Generation (RAG) App: Part 1\n# - Build a semantic search engine\n# - Build a Question/Answering system over SQL data\n# - Summarize Text\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: Guides on specific functionalities and techniques within LangChain, such as using tools, vectorstores, memory, example selectors, and handling specific scenarios.",
    "chunk_length": 2106
  },
  {
    "chunk_id": 50,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/how_to/query_high_cardinality/\n\nLANGUAGE: python\nCODE:\n```\nHow to use tools in a chain\nHow to use a vectorstore as a retriever\nHow to add memory to chatbots\nHow to use example selectors\nHow to add a semantic layer over graph database\nHow to invoke runnables in parallel\nHow to stream chat model responses\nHow to add default invocation args to a Runnable\nHow to add retrieval to chatbots\nHow to use few shot examples in chat models\nHow to do tool/function calling\nHow to install LangChain packages\nHow to add examples to the prompt for query analysis\nHow to use few shot examples\nHow to run custom functions\nHow to use output parsers to parse an LLM response into structured format\nHow to handle cases where no queries are generated\nHow to route between sub-chains\nHow to return structured data from a model\n```\n\n----------------------------------------\n\nTITLE: How to add examples to the prompt for query analysis\nDESCRIPTION: This guide explains how to include few\n\nSOURCE: https://python.langchain.com/docs/how_to/extraction_parse/\n\n\n\n----------------------------------------\n\nTITLE: How to add examples to the prompt for query analysis\nDESCRIPTION: This guide explains how to include examples in prompts for query analysis, improving the accuracy of the analysis. SOURCE: https://python.langchain.com/docs/tutorials/extraction/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_openai import OpenAI\n\n# Example prompt with few-shot examples\n# prompt = PromptTemplate(\n#     input_variables=[\"query\"],\n#     template=\"Analyze the following queries and categorize them:\\n\\nQuery: \"What is the weather like?\"\\nCategory: Weather\\n\\nQuery: \"Set a timer for 5 minutes\"\\nCategory: Timer\\n\\nQuery: {query}\\nCategory: \"\n# )\n\n# Initialize LLM\n# llm = OpenAI()\n\n# Create the chain\n# chain = prompt | llm\n\n# Example usage:\n# response = chain.invoke({\"query\": \"What is the capital of France?\"})\n# print(response)\n```\n\n----------------------------------------\n\nTITLE: Pydantic Output Parsing with LCEL\nDESCRIPTION: Illustrates parsing an LLM's response into a Pydantic object using PydanticOutputParser and LCEL.",
    "chunk_length": 2185
  },
  {
    "chunk_id": 51,
    "source": "python_langchain_llms_data",
    "content": "This example shows a chain that takes a query and returns a structured joke object with 'setup' and 'punchline' fields. SOURCE: https://python.langchain.com/docs/how_to/output_parser_structured/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.output_parsers import PydanticOutputParser\nfrom langchain_core.pydantic_v1 import BaseModel, Field\n\n# Define a Pydantic model for the output\nclass Joke(BaseModel):\n    setup: str = Field(description=\"The setup of the joke\")\n    punchline: str = Field(description=\"The punchline of the joke\")\n    \n# Assuming 'chain' is a pre-configured LangChain LCEL chain\n# chain = prompt | model | PydanticOutputParser(pydantic_object=Joke)\n\n# Example usage:\n# print(list(chain.stream({\"query\": \"Tell me a joke.\" })))\n\n```\n\n----------------------------------------\n\nTITLE: Install LangChain and OpenAI\nDESCRIPTION: Installs the necessary LangChain and OpenAI libraries, and prompts for the OpenAI API key if not already set. SOURCE: https://python.langchain.com/docs/how_to/few_shot_examples_chat/\n\nLANGUAGE: python\nCODE:\n```\n%pip install -qU langchain langchain-openai langchain-chromaimport osfrom getpass import getpassif \"OPENAI_API_KEY\" not in os.environ:    os.environ[\"OPENAI_API_KEY\"] = getpass()\n```\n\n----------------------------------------\n\nTITLE: Improving Extraction with Reference Examples\nDESCRIPTION: This guide explains how to structure example inputs and outputs for extraction tasks in LangChain. It details incorporating these examples into prompts to enhance extraction performance. SOURCE: https://context7_llms\n\nLANGUAGE: python\nCODE:\n```\n# Conceptual example of structuring examples for LangChain's tool calling API\n# examples = [\n#     {\"input\": \"Text 1\", \"output\": \"Extracted Data 1\"},\n#     {\"input\": \"Text 2\", \"output\": \"Extracted Data 2\"}\n# ]\n# prompt = f\"Extract data from the following text: {text}\\nExamples:\\n{examples}\"\n```\n\n----------------------------------------\n\nTITLE: How to use few shot examples\nDESCRIPTION: This guide covers the general approach to using few-shot examples in LangChain to enhance model performance.",
    "chunk_length": 2082
  },
  {
    "chunk_id": 52,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/how_to/chatbots_tools/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n\nexamples = [\n    {\"input\": \"apple\", \"output\": \"fruit\"},\n    {\"input\": \"banana\", \"output\": \"fruit\"},\n    {\"input\": \"carrot\", \"output\": \"vegetable\"}\n]\n\nexample_prompt = PromptTemplate(input_variables=[\"input\", \"output\"], template=\"Input: {input}\\nOutput: {output}\")\n\nfinal_prompt = FewShotPromptTemplate(\n    example_prompt=example_prompt,\n    examples=examples,\n    prefix=\"Classify the input.\",\n    suffix=\"Input: {input}\\nOutput:\",\n    input_variables=[\"input\"]\n)\n```\n\n----------------------------------------\n\nTITLE: How to add examples to the prompt for query analysis\nDESCRIPTION: This guide explains how to include examples within prompts for query analysis tasks. Providing examples helps the LLM understand the desired output format and context. SOURCE: https://python.langchain.com/docs/how_to/chatbots_memory/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.prompts import PromptTemplate\n\nexample = \"Input: What is the capital of France? Output: Paris\"\n\nprompt = PromptTemplate(\n    input_variables=[\"input\"],\n    template=f\"{example}\\nInput: {{input}}\\nOutput:\"\n)\n\n```\n\n----------------------------------------\n\nTITLE: How to Use Few Shot Examples in Chat Models\nDESCRIPTION: This guide covers the implementation of few-shot learning with LangChain chat models. It demonstrates how to provide example interactions within the prompt to guide the model's responses and improve accuracy for specific tasks. SOURCE: https://python.langchain.com/docs/how_to/custom_tools/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.schema import HumanMessage, SystemMessage, AIMessage\n\n# Initialize a chat modelllm = ChatOpenAI()\n\n# Define few-shot examplesmessages = [\n    SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n    HumanMessage(content=\"Translate: 'hello'\"),\n    AIMessage(content=\"Bonjour\"),\n    HumanMessage(content=\"Translate: 'goodbye'\"),\n    AIMessage(content=\"Au revoir\"),\n    HumanMessage(content=\"Translate: 'thank you'\")\n]\n\n# Send the messages including examples to the chat model\nesponse = llm(messages)\nprint(response.content)\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides Overview\nDESCRIPTION: This section provides practical guides on implementing specific features and functionalities within LangChain, such as using tools, memory, vectorstores, and handling different output formats.",
    "chunk_length": 2577
  },
  {
    "chunk_id": 53,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/how_to/callbacks_attach/\n\nLANGUAGE: python\nCODE:\n```\nHow to use tools in a chain\nHow to use a vectorstore as a retriever\nHow to add memory to chatbots\nHow to use example selectors\nHow to add a semantic layer over graph database\nHow to invoke runnables in parallel\nHow to stream chat model responses\nHow to add default invocation args to a Runnable\nHow to add retrieval to chatbots\nHow to use few shot examples in chat models\nHow to do tool/function calling\nHow to install LangChain packages\nHow to add examples to the prompt for query analysis\nHow to use few shot examples\nHow to run custom functions\nHow to use output parsers to parse an LLM response into structured format\nHow to handle cases where no queries are generated\nHow to route between sub-chains\nHow to return structured data from a model\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools. This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/how_to/installation/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: Define Prompt Template with Examples\nDESCRIPTION: Creates a chat prompt template that includes system instructions and a placeholder for few-shot examples to guide LLM extraction.",
    "chunk_length": 2815
  },
  {
    "chunk_id": 54,
    "source": "python_langchain_llms_data",
    "content": "It specifies how to handle missing information by returning null. SOURCE: https://python.langchain.com/docs/how_to/extraction_examples/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\n# Define a custom prompt to provide instructions and any additional context. # 1) You can add examples into the prompt template to improve extraction quality\n# 2) Introduce additional parameters to take context into account (e.g., include metadata\n#    about the document from which the text was extracted.)\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"You are an expert extraction algorithm. \"\n            \"Only extract relevant information from the text. \"\n            \"If you do not know the value of an attribute asked \"\n            \"to extract, return null for the attribute's value.\",\n        ),\n        # \n        MessagesPlaceholder(\"examples\"),  # <-- EXAMPLES! # \n        (\"human\", \"{text}\"),\n    ]\n)\n```\n\n----------------------------------------\n\nTITLE: How to install LangChain packages\nDESCRIPTION: This guide provides instructions on how to install the necessary LangChain packages and their dependencies using pip. SOURCE: https://python.langchain.com/docs/how_to/extraction_examples/\n\nLANGUAGE: bash\nCODE:\n```\n# Install the core LangChain library\npip install langchain\n\n# Install specific integrations, e.g., for OpenAI\npip install langchain-openai\n\n# Install for vector stores, e.g., Chroma\npip install langchain-chroma\n\n# Install for specific tools, e.g., DuckDuckGo\npip install duckduckgo-search\n\n# Install for experimental features\npip install langchain-experimental\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: Practical guides on implementing specific features and functionalities within LangChain, such as adding memory, using tools, and handling streaming responses. SOURCE: https://python.langchain.com/docs/how_to/document_loader_json/\n\nLANGUAGE: python\nCODE:\n```\nHow to use tools in a chain: /docs/how_to/tools_chain/\nHow to use a vectorstore as a retriever: /docs/how_to/vectorstore_retriever/\nHow to add memory to chatbots: /docs/how_to/chatbots_memory/\nHow to use example selectors: /docs/how_to/example_selectors/\nHow to add a semantic layer over graph database: /docs/how_to/graph_semantic/\nHow to invoke runnables in parallel: /docs/how_to/parallel/\nHow to stream chat model responses: /docs/how_to/chat_streaming/\nHow to add default invocation args to a Runnable: /docs/how_to/binding/\nHow to add retrieval to chatbots: /docs/how_to/chatbots_retrieval/\nHow to use few shot examples in chat models: /docs/how_to/few_shot_examples_chat/\nHow to do tool/function calling: /docs/how_to/function_calling/\nHow to add examples to the prompt for query analysis: /docs/how_to/query_few_shot/\nHow to use few shot examples: /docs/how_to/few_shot_examples/\nHow to run custom functions: /docs/how_to/functions/\nHow to use output parsers to parse an LLM response into structured format: /docs/how_to/output_parser_structured/\nHow to handle cases where no queries are generated: /docs/how_to/query_no_queries/\nHow to route between sub-chains: /docs/how_to/routing/\nHow to return structured data from a model: /docs/how_to/structured_output/\nHow to summarize text through parallelization: /docs/how_to/summarize_map_reduce/\n```\n\n----------------------------------------\n\nTITLE: Install Main LangChain Package\nDESCRIPTION: Installs the core LangChain package, serving as a starting point for using the framework.",
    "chunk_length": 3620
  },
  {
    "chunk_id": 55,
    "source": "python_langchain_llms_data",
    "content": "This package acts as a foundation, but additional dependencies for specific integrations are not included by default. SOURCE: https://python.langchain.com/docs/how_to/installation/\n\nLANGUAGE: bash\nCODE:\n```\npip install langchain\n```\n\nLANGUAGE: bash\nCODE:\n```\nconda install langchain -c conda-forge\n```\n\n----------------------------------------\n\nTITLE: Test Prompt Formatting\nDESCRIPTION: Invokes the previously defined `example_prompt` with the first example from the `examples` list and prints the formatted output. This demonstrates how the prompt template processes the input data. SOURCE: https://python.langchain.com/docs/how_to/few_shot_examples/\n\nLANGUAGE: python\nCODE:\n```\nprint(example_prompt.invoke(examples[0]).to_string())\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools. This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/how_to/output_parser_structured/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: How to use few shot examples\nDESCRIPTION: This guide explains how to provide few-shot examples to models in LangChain to improve their performance on specific tasks.",
    "chunk_length": 2705
  },
  {
    "chunk_id": 56,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/tutorials/extraction/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_openai import OpenAI\n\n# Example prompt with few-shot examples\n# prompt = PromptTemplate(\n#     input_variables=[\"input\"],\n#     template=\"Translate English to French:\\nEnglish: Hello\\nFrench: Bonjour\\nEnglish: {input}\\nFrench: \"\n# )\n\n# Initialize LLM\n# llm = OpenAI()\n\n# Create the chain\n# chain = prompt | llm\n\n# Example usage:\n# response = chain.invoke({\"input\": \"How are you?\"})\n# print(response)\n```\n\n----------------------------------------\n\nTITLE: End-to-End Example Setup\nDESCRIPTION: Sets up an in-memory vector store with sample documents and initializes the LangChain components for an agentic RAG system. This includes embeddings, the vector store, and the agent creation. SOURCE: https://python.langchain.com/docs/integrations/chat/anthropic/\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Literal\nfrom langchain.chat_models import init_chat_model\nfrom langchain.embeddings import init_embeddings\nfrom langchain_core.documents import Document\nfrom langchain_core.vectorstores import InMemoryVectorStore\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.prebuilt import create_react_agent\n\n# Set up vector store\nembeddings = init_embeddings(\"openai:text-embedding-3-small\")\nvector_store = InMemoryVectorStore(embeddings)\n\ndocument_1 = Document(\n    id=\"1\",\n    page_content=(\n        \"To request vacation days, submit a leave request form through the \"\n        \"HR portal. Approval will be sent by email.\"\n    ),\n    metadata={\n        \"category\": \"HR Policy\",\n        \"doc_title\": \"Leave Policy\",\n        \"provenance\": \"Leave Policy - page 1\",\n    },\n)\ndocument_2 = Document(id=\"2\", page_content=\"Managers will review vacation requests within 3 business days.\", metadata={\n        \"category\": \"HR Policy\",\n        \"doc_title\": \"Leave Policy\",\n        \"provenance\": \"Leave Policy - page 2\",\n    })\ndocument_3 = Document(\n    id=\"3\",\n    page_content=(\n        \"Employees with over 6 months tenure are eligible for 20 paid vacation days \"\n        \"per year.\"\n    ),\n    metadata={\n        \"category\": \"Benefits Policy\",\n        \"doc_title\": \"Benefits Guide 2025\",\n        \"provenance\": \"Benefits Policy - page 1\",\n    },\n)\ndocuments = [document_1, document_2, document_3]\nvector_store.add_documents(documents=documents)\n\n# Define tool\nasync def retrieval_tool(\n    query: str,\n    category: Literal[\"HR Policy\", \"Benefits Policy\"],\n) -> list[dict]:\n    \"\"\"Access my knowledge base.\"\"\"\n    def _filter_function(doc: Document) -> bool:\n        return doc.metadata.get(\"category\") == category\n    results = vector_store.similarity_search(\n        query=query, k=2, filter=_filter_function\n    )\n    return [\n        {\n            \"type\": \"search_result\",\n            \"title\": doc.metadata[\"doc_title\"],\n            \"source\": doc.metadata[\"provenance\"],\n            \"citations\": {\"enabled\": True},\n            \"content\": [{\"type\": \"text\", \"text\": doc.page_content}],\n        }\n        for doc in results\n    ]\n\n# Create agent\nllm = init_chat_model(\n    \"anthropic:claude-3-5-haiku-latest\",\n    betas=[\"search-results-2025-06-09\"],\n)\ncheckpointer = InMemorySaver()\nagent = create_react_agent(llm, [retrieval_tool], checkpointer=checkpointer)\n\n# Invoke on a query\nconfig = {\"configurable\": {\"thread_id\": \"session_1\"}}\ninput_message = {\n    \"role\": \"user\",\n    \"content\": \"How do I request vacation days?\",\n}\n\nasync for step in agent.astream(\n    {\"messages\": [input_message]},\n    config,\n    stream_mode=\"values\",\n):\n    step[\"messages\"][-1].pretty_print()\n```\n\n----------------------------------------\n\nTITLE: LangChain Tutorials\nDESCRIPTION: A collection of tutorials to guide users through building various applications with LangChain, from simple LLM apps to complex agents.",
    "chunk_length": 3859
  },
  {
    "chunk_id": 57,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/how_to/callbacks_async/\n\nLANGUAGE: APIDOC\nCODE:\n```\nLangChain Tutorials:\n  - Build a Question Answering application over a Graph Database\n    URL: /docs/tutorials/graph/\n  - Build a simple LLM application with chat models and prompt templates\n    URL: /docs/tutorials/llm_chain/\n  - Build a Chatbot\n    URL: /docs/tutorials/chatbot/\n  - Build a Retrieval Augmented Generation (RAG) App: Part 2\n    URL: /docs/tutorials/qa_chat_history/\n  - Build an Extraction Chain\n    URL: /docs/tutorials/extraction/\n  - Build an Agent\n    URL: /docs/tutorials/agents/\n  - Tagging\n    URL: /docs/tutorials/classification/\n  - Build a Retrieval Augmented Generation (RAG) App: Part 1\n    URL: /docs/tutorials/rag/\n  - Build a semantic search engine\n    URL: /docs/tutorials/retrievers/\n  - Build a Question/Answering system over SQL data\n    URL: /docs/tutorials/sql_qa/\n  - Summarize Text\n    URL: /docs/tutorials/summarization/\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides Overview\nDESCRIPTION: This section provides guides on how to perform specific tasks and implement features within LangChain. Topics include using tools, vectorstores, memory, example selectors, parallel execution, streaming, function calling, and more. SOURCE: https://python.langchain.com/docs/how_to/custom_chat_model/\n\nLANGUAGE: markdown\nCODE:\n```\n*   [How-to guides](/docs/how_to/)\n*   [How to use tools in a chain](/docs/how_to/tools_chain/)\n*   [How to use a vectorstore as a retriever](/docs/how_to/vectorstore_retriever/)\n*   [How to add memory to chatbots](/docs/how_to/chatbots_memory/)\n*   [How to use example selectors](/docs/how_to/example_selectors/)\n*   [How to add a semantic layer over graph database](/docs/how_to/graph_semantic/)\n*   [How to invoke runnables in parallel](/docs/how_to/parallel/)\n*   [How to stream chat model responses](/docs/how_to/chat_streaming/)\n*   [How to add default invocation args to a Runnable](/docs/how_to/binding/)\n*   [How to add retrieval to chatbots](/docs/how_to/chatbots_retrieval/)\n*   [How to use few shot examples in chat models](/docs/how_to/few_shot_examples_chat/)\n*   [How to do tool/function calling](/docs/how_to/function_calling/)\n*   [How to install LangChain packages](/docs/how_to/installation/)\n*   [How to add examples to the prompt for query analysis](/docs/how_to/query_few_shot/)\n*   [How to use few shot examples](/docs/how_to/few_shot_examples/)\n*   [How to run custom functions](/docs/how_to/functions/)\n*   [How to use output parsers to parse an LLM response into structured format](/docs/how_to/output_parser_structured/)\n*   [How to handle cases where no queries are generated](/docs/how_to/query_no_queries/)\n*   [How to route between sub-chains](/docs/how_to/routing/)\n*   [How to return structured data from a model](/docs/how_to/structured_output/)\n```\n\n----------------------------------------\n\nTITLE: How to add examples to the prompt for query analysis\nDESCRIPTION: This how-to guide explains how to include examples within prompts to improve the accuracy and relevance of query analysis performed by LLMs.",
    "chunk_length": 3126
  },
  {
    "chunk_id": 58,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/tutorials/rag/\n\nLANGUAGE: python\nCODE:\n```\n# This is a placeholder for the actual code in the how-to guide. # The guide focuses on the concepts and steps involved in adding examples to prompts for query analysis. # For specific code examples, please refer to the official LangChain documentation. # Example conceptual steps:\n# 1. Prepare a dataset of queries and their desired analysis or classification\n# 2. Construct a prompt that includes these examples before the actual query\n# 3. Use an LLM to analyze the query based on the provided examples\n```\n\n----------------------------------------\n\nTITLE: How to add examples to the prompt for query analysis\nDESCRIPTION: This guide explains how to include few-shot examples within prompts to help language models better understand and analyze user queries. SOURCE: https://python.langchain.com/docs/how_to/extraction_examples/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\nfrom langchain_openai import ChatOpenAI\nfrom langchain.chains import LLMChain\n\n# Define few-shot examples for query analysisexamples = [\n    {\"query\": \"What is the weather today?\", \"analysis\": \"Weather query\"},\n    {\"query\": \"Tell me a joke.\", \"analysis\": \"Entertainment query\"},\n    {\"query\": \"What is the capital of France?\", \"analysis\": \"Factual query\"},\n]\n\n# Create a prompt template for the examplesexample_prompt = ChatPromptTemplate.from_messages([\n    (\"human\", \"Analyze this query: {query}\"),\n    (\"ai\", \"Analysis: {analysis}\"),\n])\n\n# Create the FewShotChatMessagePromptTemplate\few_shot_prompt = FewShotChatMessagePromptTemplate(\n    examples=examples,\n    example_prompt=example_prompt,\n    input_variables=[\"query\", \"analysis\"],\n    example_separator=\"\\n\\n\",\n)\n\n# Create the final prompt template\final_prompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a query analysis assistant.\"),\n    few_shot_prompt,\n    (\"human\", \"Analyze this query: {query}\"),\n])\n\n# Initialize LLM and create chainllm = ChatOpenAI(model=\"gpt-3.5-turbo\")\nchain = LLMChain(llm=llm, prompt=final_prompt)\n\n# Example usage\nesult = chain.invoke({\"query\": \"What is the latest stock price?\"})\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: How to Install LangChain Packages\nDESCRIPTION: This guide provides instructions on how to install the necessary LangChain packages for your Python environment.",
    "chunk_length": 2447
  },
  {
    "chunk_id": 59,
    "source": "python_langchain_llms_data",
    "content": "It covers the use of pip for installing the core library and optional dependencies. SOURCE: https://python.langchain.com/docs/how_to/custom_tools/\n\nLANGUAGE: bash\nCODE:\n```\n# Install the core LangChain library\npip install langchain\n\n# Install specific integrations (e.g., for OpenAI)\npip install langchain-openai\n\n# Install for specific features like document loaders\npip install langchain-community\n\n# Install all common integrations\npip install \"langchain[all]\"\n```\n\n----------------------------------------\n\nTITLE: How to Use Few Shot Examples in Chat Models\nDESCRIPTION: This guide explains how to effectively use few-shot learning with LangChain chat models. By providing example interactions within the prompt, you can guide the model to produce desired outputs for specific tasks. SOURCE: https://python.langchain.com/docs/how_to/custom_tools/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.schema import HumanMessage, SystemMessage, AIMessage\n\n# Initialize a chat modelllm = ChatOpenAI()\n\n# Define few-shot examples for a classification taskmessages = [\n    SystemMessage(content=\"Classify the following movie review as Positive, Negative, or Neutral.\"),\n    HumanMessage(content=\"This movie was fantastic, I loved every minute!\"),\n    AIMessage(content=\"Positive\"),\n    HumanMessage(content=\"It was okay, nothing special.\"),\n    AIMessage(content=\"Neutral\"),\n    HumanMessage(content=\"A complete waste of time and money.\")\n]\n\n# Send the messages including examples to the chat model\nesponse = llm(messages)\nprint(response.content)\n```\n\n----------------------------------------\n\nTITLE: Setup and Environment Configuration\nDESCRIPTION: Installs necessary LangChain packages and configures the OpenAI API key from environment variables or user input. It also initializes a ChatOpenAI model. SOURCE: https://python.langchain.com/docs/how_to/chatbots_memory/\n\nLANGUAGE: python\nCODE:\n```\n%pip install --upgrade --quiet langchain langchain-openai langgraph\nimport getpass\nimport os\nif not os.environ.get(\"OPENAI_API_KEY\"): os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n\nfrom langchain_openai import ChatOpenAI\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\n----------------------------------------\n\nTITLE: Add Examples to Prompt for Tuning\nDESCRIPTION: Shows how to add example input questions and their corresponding desired structured outputs (Search objects) to the prompt to improve the query generation results.",
    "chunk_length": 2477
  },
  {
    "chunk_id": 60,
    "source": "python_langchain_llms_data",
    "content": "This helps the LLM decompose questions more effectively. SOURCE: https://python.langchain.com/docs/how_to/query_few_shot/\n\nLANGUAGE: python\nCODE:\n```\nexamples = []\n\nquestion = \"What's chat langchain, is it a langchain template?\"\nquery = Search(\n    query=\"What is chat langchain and is it a langchain template?\",\n    sub_queries=[\"What is chat langchain\", \"What is a langchain template\"],\n)\nexamples.append({\"input\": question, \"tool_calls\": [query]})\n\nquestion = \"How to build multi-agent system and stream intermediate steps from it\"\nquery = Search(\n    query=\"How to build multi-agent system and stream intermediate steps from it\",\n    sub_queries=[\n        \"How to build multi-agent system\",\n        \"How to stream intermediate steps from multi-agent system\",\n        \"How to stream intermediate steps\",\n    ],\n)\nexamples.append({\"input\": question, \"tool_calls\": [query]})\n```\n\n----------------------------------------\n\nTITLE: LangChain Tutorials\nDESCRIPTION: A collection of tutorials to guide users through building various LLM applications with LangChain. SOURCE: https://python.langchain.com/docs/how_to/callbacks_runtime/\n\nLANGUAGE: APIDOC\nCODE:\n```\nTutorials:\n  - Build a Question Answering application over a Graph Database\n  - Build a simple LLM application with chat models and prompt templates\n  - Build a Chatbot\n  - Build a Retrieval Augmented Generation (RAG) App: Part 2\n  - Build an Extraction Chain\n  - Build an Agent\n  - Tagging\n  - Build a Retrieval Augmented Generation (RAG) App: Part 1\n  - Build a semantic search engine\n  - Build a Question/Answering system over SQL data\n  - Summarize Text\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides Overview\nDESCRIPTION: This section outlines various how-to guides for implementing specific functionalities within LangChain, including tool usage, vectorstore retrieval, memory, parallel execution, streaming, and more. SOURCE: https://python.langchain.com/docs/how_to/agent_executor/\n\nLANGUAGE: python\nCODE:\n```\nHow-to guides\nHow to use tools in a chain\nHow to use a vectorstore as a retriever\nHow to add memory to chatbots\nHow to use example selectors\nHow to add a semantic layer over graph database\nHow to invoke runnables in parallel\nHow to stream chat model responses\nHow to add default invocation args to a Runnable\nHow to add retrieval to chatbots\nHow to use few shot examples in chat models\nHow to do tool/function calling\nHow to install LangChain packages\nHow to add examples to the prompt for query analysis\nHow to use few shot examples\nHow to run custom functions\nHow to use output parsers to parse an LLM response into structured format\nHow to handle cases where no queries are generated\nHow to route between sub-chains\nHow to return structured data from a model\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides Overview\nDESCRIPTION: This section provides practical guides on implementing specific features and functionalities within LangChain, including tool usage, vectorstores, memory, parallel execution, streaming, and more.",
    "chunk_length": 3064
  },
  {
    "chunk_id": 61,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/how_to/parent_document_retriever/\n\nLANGUAGE: python\nCODE:\n```\nHow to use tools in a chain\nHow to use a vectorstore as a retriever\nHow to add memory to chatbots\nHow to use example selectors\nHow to add a semantic layer over graph database\nHow to invoke runnables in parallel\nHow to stream chat model responses\nHow to add default invocation args to a Runnable\nHow to add retrieval to chatbots\nHow to use few shot examples in chat models\nHow to do tool/function calling\nHow to install LangChain packages\nHow to add examples to the prompt for query analysis\nHow to use few shot examples\nHow to run custom functions\nHow to use output parsers to parse an LLM response into structured format\nHow to handle cases where no queries are generated\nHow to route between sub-chains\nHow to return structured data from a model\n```\n\n----------------------------------------\n\nTITLE: LangChain Tutorials Overview\nDESCRIPTION: This section provides a list of tutorials for building various applications with LangChain, covering topics from basic LLM applications to complex agents and retrieval systems. SOURCE: https://python.langchain.com/docs/how_to/vectorstores/\n\nLANGUAGE: python\nCODE:\n```\nBuild a Question Answering application over a Graph Database\nBuild a simple LLM application with chat models and prompt templates\nBuild a Chatbot\nBuild a Retrieval Augmented Generation (RAG) App: Part 2\nBuild an Extraction Chain\nBuild an Agent\nTagging\nBuild a Retrieval Augmented Generation (RAG) App: Part 1\nBuild a semantic search engine\nBuild a Question/Answering system over SQL data\nSummarize Text\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: Practical guides on implementing specific features and functionalities within LangChain applications. SOURCE: https://python.langchain.com/docs/how_to/document_loader_csv/\n\nLANGUAGE: python\nCODE:\n```\nHow-to guides:\n  How to use tools in a chain: /docs/how_to/tools_chain/\n  How to use a vectorstore as a retriever: /docs/how_to/vectorstore_retriever/\n  How to add memory to chatbots: /docs/how_to/chatbots_memory/\n  How to use example selectors: /docs/how_to/example_selectors/\n  How to add a semantic layer over graph database: /docs/how_to/graph_semantic/\n  How to invoke runnables in parallel: /docs/how_to/parallel/\n  How to stream chat model responses: /docs/how_to/chat_streaming/\n  How to add default invocation args to a Runnable: /docs/how_to/binding/\n  How to add retrieval to chatbots: /docs/how_to/chatbots_retrieval/\n  How to use few shot examples in chat models: /docs/how_to/few_shot_examples_chat/\n  How to do tool/function calling: /docs/how_to/function_calling/\n  How to add examples to the prompt for query analysis: /docs/how_to/query_few_shot/\n  How to use few shot examples: /docs/how_to/few_shot_examples/\n  How to run custom functions: /docs/how_to/functions/\n  How to use output parsers to parse an LLM response into structured format: /docs/how_to/output_parser_structured/\n  How to handle cases where no queries are generated: /docs/how_to/query_no_queries/\n  How to route between sub-chains: /docs/how_to/routing/\n  How to return structured data from a model: /docs/how_to/structured_output/\n  How to summarize text through parallelization: /docs/how_to/summarize_map_reduce/\n```\n\n----------------------------------------\n\nTITLE: How to add examples to the prompt for query analysis\nDESCRIPTION: This guide explains how to incorporate few-shot examples into prompts for analyzing queries.",
    "chunk_length": 3526
  },
  {
    "chunk_id": 62,
    "source": "python_langchain_llms_data",
    "content": "It covers techniques for improving the accuracy and relevance of query analysis by providing the language model with illustrative examples. SOURCE: https://python.langchain.com/docs/how_to/query_few_shot/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n\n# Example data\nexamples = [\n    {\n        \"query\": \"What is the capital of France?\",\n        \"analysis\": \"This is a factual question about geography.\"\n    },\n    {\n        \"query\": \"Write a poem about a cat.\",\n        \"analysis\": \"This is a creative writing request.\"\n    }\n]\n\n# Define the template for each example\nexample_template = \"Query: {query}\\nAnalysis: {analysis}\"\n\n# Create the prompt template for examples\nexample_prompt = PromptTemplate(\n    input_variables=[\"query\", \"analysis\"],\n    template=example_template\n)\n\n# Define the overall prompt template\nprompt = FewShotPromptTemplate(\n    examples=examples,\n    example_prompt=example_prompt,\n    prefix=\"Analyze the following queries and categorize them.\",\n    suffix=\"Query: {query}\\nAnalysis:\",\n    input_variables=[\"query\"],\n    example_separator=\"\\n\\n\"\n)\n\n# Example usage\nquery = \"Tell me a joke.\"\nformatted_prompt = prompt.format(query=query)\nprint(formatted_prompt)\n\n```\n\n----------------------------------------\n\nTITLE: How to install LangChain packages\nDESCRIPTION: This guide provides instructions on how to install the necessary LangChain packages for your Python environment. SOURCE: https://python.langchain.com/docs/concepts/rag/\n\nLANGUAGE: python\nCODE:\n```\nprint('How-to: How to install LangChain packages')\n```\n\n----------------------------------------\n\nTITLE: How to add examples to the prompt for query analysis\nDESCRIPTION: This guide explains how to include examples within prompts for query analysis tasks. Providing examples helps the model better understand the structure and intent of the queries it needs to analyze. SOURCE: https://python.langchain.com/docs/how_to/tool_choice/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\n\n# Define the prompt with examples for query analysis\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a query analysis assistant.",
    "chunk_length": 2242
  },
  {
    "chunk_id": 63,
    "source": "python_langchain_llms_data",
    "content": "Classify the user query.\"),\n    (\"human\", \"Classify the following query: 'What is the weather in London?' -> WEATHER\"),\n    (\"human\", \"Classify the following query: 'What is the stock price of AAPL?' -> STOCK\"),\n    (\"human\", \"Classify the following query: '{query}' -> \"),\n])\n\n# Initialize the chat model\nllm = ChatOpenAI()\n\n# Create a chain\nchain = prompt | llm\n\n# Invoke the chain with a new query\nresponse = chain.invoke({\"query\": \"Tell me about the latest news.\"})\nprint(response.content)\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides Overview\nDESCRIPTION: This section provides guides on specific functionalities and implementation details within LangChain, such as using tools, managing memory, handling streaming, and implementing function calling. SOURCE: https://python.langchain.com/docs/how_to/serialization/\n\nLANGUAGE: python\nCODE:\n```\nHow to use tools in a chain\nHow to use a vectorstore as a retriever\nHow to add memory to chatbots\nHow to use example selectors\nHow to add a semantic layer over graph database\nHow to invoke runnables in parallel\nHow to stream chat model responses\nHow to add default invocation args to a Runnable\nHow to add retrieval to chatbots\nHow to use few shot examples in chat models\nHow to do tool/function calling\nHow to install LangChain packages\nHow to add examples to the prompt for query analysis\nHow to use few shot examples\nHow to run custom functions\nHow to use output parsers to parse an LLM response into structured format\nHow to handle cases where no queries are generated\nHow to route between sub-chains\nHow to return structured data from a model\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides Overview\nDESCRIPTION: This section outlines various how-to guides for implementing specific functionalities within LangChain. It covers essential topics such as using tools, memory, prompt engineering techniques, and deployment considerations. SOURCE: https://python.langchain.com/docs/how_to/sequence/\n\nLANGUAGE: markdown\nCODE:\n```\n*   [How to use tools in a chain](/docs/how_to/tools_chain/)\n*   [How to use a vectorstore as a retriever](/docs/how_to/vectorstore_retriever/)\n*   [How to add memory to chatbots](/docs/how_to/chatbots_memory/)\n*   [How to use example selectors](/docs/how_to/example_selectors/)\n*   [How to add a semantic layer over graph database](/docs/how_to/graph_semantic/)\n*   [How to invoke runnables in parallel](/docs/how_to/parallel/)\n*   [How to stream chat model responses](/docs/how_to/chat_streaming/)\n*   [How to add default invocation args to a Runnable](/docs/how_to/binding/)\n*   [How to add retrieval to chatbots](/docs/how_to/chatbots_retrieval/)\n*   [How to use few shot examples in chat models](/docs/how_to/few_shot_examples_chat/)\n*   [How to do tool/function calling](/docs/how_to/function_calling/)\n*   [How to install LangChain packages](/docs/how_to/installation/)\n*   [How to add examples to the prompt for query analysis](/docs/how_to/query_few_shot/)\n*   [How to use few shot examples](/docs/how_to/few_shot_examples/)\n*   [How to run custom functions](/docs/how_to/functions/)\n*   [How to use output parsers to parse an LLM response into structured format](/docs/how_to/output_parser_structured/)\n*   [How to handle cases where no queries are generated](/docs/how_to/query_no_queries/)\n*   [How to route between sub-chains](/docs/how_to/routing/)\n*   [How to return structured data from a model](/docs/how_to/structured_output/)\n*   [How to summarize text through parallelization](/docs/how_to/summarize_map_reduce/)\n```\n\n----------------------------------------\n\nTITLE: Adding Examples to Prompts for Query Analysis\nDESCRIPTION: This section covers how to guide an LLM to generate queries by incorporating examples into few-shot prompts.",
    "chunk_length": 3794
  },
  {
    "chunk_id": 64,
    "source": "python_langchain_llms_data",
    "content": "It's useful when fine-tuning an LLM for query generation or when needing to structure query output. SOURCE: https://context7_llms\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.prompts import PromptTemplate\nfrom langchain.llms import OpenAI\nfrom langchain.chains import LLMChain\n\n# Example of a prompt with few-shot examples for query generation\n# prompt_template = \"\"\"\n# Given the following context, generate a search query:\n# Context: {context}\n# Query:\n# \"\"\"\n# prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\"])\n# llm = OpenAI(temperature=0)\n# query_generation_chain = LLMChain(llm=llm, prompt=prompt)\n\n# The actual implementation would involve providing examples within the prompt\n# to guide the LLM's output format and content. ```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: This section provides practical guides on implementing specific features and functionalities within LangChain, such as tool usage, memory, streaming, and structured output. SOURCE: https://python.langchain.com/docs/how_to/tools_prompting/\n\nLANGUAGE: python\nCODE:\n```\nHow to use tools in a chain\nHow to use a vectorstore as a retriever\nHow to add memory to chatbots\nHow to use example selectors\nHow to add a semantic layer over graph database\nHow to invoke runnables in parallel\nHow to stream chat model responses\nHow to add default invocation args to a Runnable\nHow to add retrieval to chatbots\nHow to use few shot examples in chat models\nHow to do tool/function calling\nHow to install LangChain packages\nHow to add examples to the prompt for query analysis\nHow to use few shot examples\nHow to run custom functions\nHow to use output parsers to parse an LLM response into structured format\nHow to handle cases where no queries are generated\nHow to route between sub-chains\nHow to return structured data from a model\n```\n\n----------------------------------------\n\nTITLE: Ollama Setup and Model Management\nDESCRIPTION: Instructions for setting up a local Ollama instance, downloading models, and managing them via the command line.",
    "chunk_length": 2076
  },
  {
    "chunk_id": 65,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/integrations/chat/ollama/\n\nLANGUAGE: bash\nCODE:\n```\nbrew install ollama\nbrew services start ollama\nollama pull <name-of-model>\nollama pull gpt-oss:20b\nollama list\nollama run <name-of-model>\nollama help\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: This section provides guides on how to implement specific features and functionalities within LangChain. Topics include using tools, vectorstores, memory, example selectors, parallel execution, streaming, function calling, and more. SOURCE: https://python.langchain.com/docs/how_to/merge_message_runs/\n\nLANGUAGE: markdown\nCODE:\n```\n*   [How-to guides](/docs/how_to/)\n    \n    *   [How-to guides](/docs/how_to/)\n    *   [How to use tools in a chain](/docs/how_to/tools_chain/)\n    *   [How to use a vectorstore as a retriever](/docs/how_to/vectorstore_retriever/)\n    *   [How to add memory to chatbots](/docs/how_to/chatbots_memory/)\n    *   [How to use example selectors](/docs/how_to/example_selectors/)\n    *   [How to add a semantic layer over graph database](/docs/how_to/graph_semantic/)\n    *   [How to invoke runnables in parallel](/docs/how_to/parallel/)\n    *   [How to stream chat model responses](/docs/how_to/chat_streaming/)\n    *   [How to add default invocation args to a Runnable](/docs/how_to/binding/)\n    *   [How to add retrieval to chatbots](/docs/how_to/chatbots_retrieval/)\n    *   [How to use few shot examples in chat models](/docs/how_to/few_shot_examples_chat/)\n    *   [How to do tool/function calling](/docs/how_to/function_calling/)\n    *   [How to install LangChain packages](/docs/how_to/installation/)\n    *   [How to add examples to the prompt for query analysis](/docs/how_to/query_few_shot/)\n    *   [How to use few shot examples](/docs/how_to/few_shot_examples/)\n    *   [How to run custom functions](/docs/how_to/functions/)\n    *   [How to use output parsers to parse an LLM response into structured format](/docs/how_to/output_parser_structured/)\n    *   [How to handle cases where no queries are generated](/docs/how_to/query_no_queries/)\n    *   [How to route between sub-chains](/docs/how_to/routing/)\n    *   [How to return structured data from a model](/docs/how_to/structured_output/)\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools.",
    "chunk_length": 2475
  },
  {
    "chunk_id": 66,
    "source": "python_langchain_llms_data",
    "content": "This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/how_to/tool_artifacts/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: Practical guides on implementing specific features and functionalities within LangChain applications. SOURCE: https://python.langchain.com/docs/how_to/callbacks_runtime/\n\nLANGUAGE: APIDOC\nCODE:\n```\nHow-to Guides:\n  - How to use tools in a chain\n  - How to use a vectorstore as a retriever\n  - How to add memory to chatbots\n  - How to use example selectors\n  - How to add a semantic layer over graph database\n  - How to invoke runnables in parallel\n  - How to stream chat model responses\n  - How to add default invocation args to a Runnable\n  - How to add retrieval to chatbots\n  - How to use few shot examples in chat models\n  - How to do tool/function calling\n  - How to install LangChain packages\n  - How to add examples to the prompt for query analysis\n  - How to use few shot examples\n  - How to run custom functions\n  - How to use output parsers to parse an LLM response into structured format\n  - How to handle cases where no queries are generated\n  - How to route between sub-chains\n  - How to return structured data from a model\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides Overview\nDESCRIPTION: This section outlines various how-to guides for implementing specific features and functionalities within LangChain, including tool usage, vectorstore retrievers, memory, streaming, and function calling.",
    "chunk_length": 2963
  },
  {
    "chunk_id": 67,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/how_to/prompts_partial/\n\nLANGUAGE: python\nCODE:\n```\nHow to use tools in a chain\nHow to use a vectorstore as a retriever\nHow to add memory to chatbots\nHow to use example selectors\nHow to add a semantic layer over graph database\nHow to invoke runnables in parallel\nHow to stream chat model responses\nHow to add default invocation args to a Runnable\nHow to add retrieval to chatbots\nHow to use few shot examples in chat models\nHow to do tool/function calling\nHow to install LangChain packages\nHow to add examples to the prompt for query analysis\nHow to use few shot examples\nHow to run custom functions\nHow to use output parsers to parse an LLM response into structured format\nHow to handle cases where no queries are generated\nHow to route between sub-chains\nHow to return structured data from a model\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides Overview\nDESCRIPTION: This section provides practical guides on implementing specific functionalities within LangChain, such as using tools, memory, vectorstores, and handling streaming responses. SOURCE: https://python.langchain.com/docs/how_to/llm_caching/\n\nLANGUAGE: python\nCODE:\n```\nHow to use tools in a chain\nHow to use a vectorstore as a retriever\nHow to add memory to chatbots\nHow to use example selectors\nHow to add a semantic layer over graph database\nHow to invoke runnables in parallel\nHow to stream chat model responses\nHow to add default invocation args to a Runnable\nHow to add retrieval to chatbots\nHow to use few shot examples in chat models\nHow to do tool/function calling\nHow to install LangChain packages\nHow to add examples to the prompt for query analysis\nHow to use few shot examples\nHow to run custom functions\nHow to use output parsers to parse an LLM response into structured format\nHow to handle cases where no queries are generated\nHow to route between sub-chains\nHow to return structured data from a model\nHow to summarize text through parallelization\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: Guides on implementing specific functionalities within LangChain, such as using tools, memory, vectorstores, and handling structured output.",
    "chunk_length": 2232
  },
  {
    "chunk_id": 68,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/how_to/example_selectors_langsmith/\n\nLANGUAGE: python\nCODE:\n```\nHow to use tools in a chain\nHow to use a vectorstore as a retriever\nHow to add memory to chatbots\nHow to use example selectors\nHow to add a semantic layer over graph database\nHow to invoke runnables in parallel\nHow to stream chat model responses\nHow to add default invocation args to a Runnable\nHow to add retrieval to chatbots\nHow to use few shot examples in chat models\nHow to do tool/function calling\nHow to install LangChain packages\nHow to add examples to the prompt for query analysis\nHow to use few shot examples\nHow to run custom functions\nHow to use output parsers to parse an LLM response into structured format\nHow to handle cases where no queries are generated\nHow to route between sub-chains\nHow to return structured data from a model\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: Practical guides on implementing specific features and functionalities within LangChain, such as adding memory, using tools, and handling streaming responses. SOURCE: https://python.langchain.com/docs/concepts/output_parsers/\n\nLANGUAGE: python\nCODE:\n```\n# How-to guides:\n# - How to use tools in a chain\n# - How to use a vectorstore as a retriever\n# - How to add memory to chatbots\n# - How to use example selectors\n# - How to add a semantic layer over graph database\n# - How to invoke runnables in parallel\n# - How to stream chat model responses\n# - How to add default invocation args to a Runnable\n# - How to add retrieval to chatbots\n# - How to use few shot examples in chat models\n# - How to do tool/function calling\n# - How to install LangChain packages\n# - How to add examples to the prompt for query analysis\n# - How to use few shot examples\n# - How to run custom functions\n# - How to use output parsers to parse an LLM response into structured format\n# - How to handle cases where no queries are generated\n# - How to route between sub-chains\n# - How to return structured data from a model\n# - How to summarize text through parallelization\n```\n\n----------------------------------------\n\nTITLE: Example Selectors\nDESCRIPTION: Demonstrates various strategies for selecting examples, including using LangSmith datasets, selecting by length, maximal marginal relevance (MMR), n-gram overlap, and similarity.",
    "chunk_length": 2353
  },
  {
    "chunk_id": 69,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/how_to/output_parser_structured/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.prompts.example_selector import SemanticSimilarityExampleSelector\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_core.example_selectors import LengthBasedExampleSelector\n\n# Example for selecting examples by length\n# example_selector = LengthBasedExampleSelector(\n#     examples=my_examples, \n#     example_prompt=example_prompt, \n#     max_length=100\n# )\n\n# Example for selecting examples by similarity\n# example_selector = SemanticSimilarityExampleSelector(\n#     vectorstore=Chroma.from_documents(my_examples, OpenAIEmbeddings()),\n#     k=2,\n#     example_prompt=example_prompt,\n#     input_keys=[\"input\"],\n# )\n\n```\n\n----------------------------------------\n\nTITLE: How to install LangChain packages\nDESCRIPTION: This guide provides instructions on how to install the necessary LangChain packages and their dependencies using pip. SOURCE: https://python.langchain.com/docs/how_to/extraction_parse/\n\nLANGUAGE: bash\nCODE:\n```\n# Install the core LangChain library\npip install langchain\n\n# Install specific integrations (e.g., OpenAI)\npip install langchain-openai\n\n# Install other useful packages like LangChain Community\npip install langchain-community\n\n# For specific features like agents or memory, you might need additional installs:\npip install langchain-core\n\n# Example: Install everything needed for OpenAI and basic chains\npip install langchain langchain-openai langchain-core\n```\n\n----------------------------------------\n\nTITLE: Install Dependencies\nDESCRIPTION: Installs the necessary LangChain packages for Chroma vector store and OpenAI embeddings. This is a prerequisite for the subsequent code examples. SOURCE: https://python.langchain.com/docs/how_to/multi_vector/\n\nLANGUAGE: bash\nCODE:\n```\n%pip install --upgrade --quiet  langchain-chroma langchain langchain-openai > /dev/null\n```\n\n----------------------------------------\n\nTITLE: How to use few shot examples\nDESCRIPTION: Explains how to leverage few-shot examples to improve the performance of language models on specific tasks.",
    "chunk_length": 2181
  },
  {
    "chunk_id": 70,
    "source": "python_langchain_llms_data",
    "content": "This involves providing a few input-output pairs to guide the model. SOURCE: https://python.langchain.com/docs/how_to/few_shot_examples/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.prompts import PromptTemplate, FewShotPromptTemplate\n\n# Define the examples\nexamples = [\n    {\n        \"input\": \"The weather is sunny and warm.\",\n        \"output\": \"positive\"\n    },\n    {\n        \"input\": \"I am feeling very sad today.\",\n        \"output\": \"negative\"\n    },\n    {\n        \"input\": \"This is an amazing experience!\",\n        \"output\": \"positive\"\n    },\n    {\n        \"input\": \"I hate this.\",\n        \"output\": \"negative\"\n    }\n]\n\n# Create a prompt template for the examples\nexample_prompt = PromptTemplate(\n    input_variables=[\"input\", \"output\"],\n    template=\"Input: {input}\\nOutput: {output}\",\n)\n\n# Create the FewShotPromptTemplate\nfew_shot_prompt = FewShotPromptTemplate(\n    examples=examples,\n    example_prompt=example_prompt,\n    prefix=\"Classify the sentiment of the following text.\",\n    suffix=\"Input: {input}\\nOutput:\",\n    input_variables=[\"input\"],\n    example_separator=\"\\n\\n\"\n)\n\n# Format the prompt with a new input\nformatted_prompt = few_shot_prompt.format(input=\"I love this product!\")\n\n# print(formatted_prompt)\n# You can then pass this formatted_prompt to an LLM\n```\n\n----------------------------------------\n\nTITLE: How to use few shot examples\nDESCRIPTION: This guide explains the usage of few-shot examples in LangChain to improve the performance of language models on specific tasks. SOURCE: https://python.langchain.com/docs/concepts/rag/\n\nLANGUAGE: python\nCODE:\n```\nprint('How-to: How to use few shot examples')\n```\n\n----------------------------------------\n\nTITLE: Example Selectors\nDESCRIPTION: Demonstrates various strategies for selecting examples, including using LangSmith datasets, selecting by length, maximal marginal relevance (MMR), n-gram overlap, and similarity. SOURCE: https://python.langchain.com/docs/how_to/tool_artifacts/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.prompts.example_selector import SemanticSimilarityExampleSelector\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_core.example_selectors import LengthBasedExampleSelector\n\n# Example for selecting examples by length\n# example_selector = LengthBasedExampleSelector(\n#     examples=my_examples, \n#     example_prompt=example_prompt, \n#     max_length=100\n# )\n\n# Example for selecting examples by similarity\n# example_selector = SemanticSimilarityExampleSelector(\n#     vectorstore=Chroma.from_documents(my_examples, OpenAIEmbeddings()),\n#     k=2,\n#     example_prompt=example_prompt,\n#     input_keys=[\"input\"],\n# )\n\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides Overview\nDESCRIPTION: This section outlines various how-to guides for LangChain, detailing specific functionalities and implementation patterns for common tasks.",
    "chunk_length": 2927
  },
  {
    "chunk_id": 71,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/how_to/qa_citations/\n\nLANGUAGE: markdown\nCODE:\n```\n*   [How to use tools in a chain](/docs/how_to/tools_chain/)\n*   [How to use a vectorstore as a retriever](/docs/how_to/vectorstore_retriever/)\n*   [How to add memory to chatbots](/docs/how_to/chatbots_memory/)\n*   [How to use example selectors](/docs/how_to/example_selectors/)\n*   [How to add a semantic layer over graph database](/docs/how_to/graph_semantic/)\n*   [How to invoke runnables in parallel](/docs/how_to/parallel/)\n*   [How to stream chat model responses](/docs/how_to/chat_streaming/)\n*   [How to add default invocation args to a Runnable](/docs/how_to/binding/)\n*   [How to add retrieval to chatbots](/docs/how_to/chatbots_retrieval/)\n*   [How to use few shot examples in chat models](/docs/how_to/few_shot_examples_chat/)\n*   [How to do tool/function calling](/docs/how_to/function_calling/)\n*   [How to install LangChain packages](/docs/how_to/installation/)\n*   [How to add examples to the prompt for query analysis](/docs/how_to/query_few_shot/)\n*   [How to use few shot examples](/docs/how_to/few_shot_examples/)\n*   [How to run custom functions](/docs/how_to/functions/)\n*   [How to use output parsers to parse an LLM response into structured format](/docs/how_to/output_parser_structured/)\n*   [How to handle cases where no queries are generated](/docs/how_to/query_no_queries/)\n*   [How to route between sub-chains](/docs/how_to/routing/)\n*   [How to return structured data from a model](/docs/how_to/structured_output/)\n```\n\n----------------------------------------\n\nTITLE: LangChain Tutorials Overview\nDESCRIPTION: This section lists various tutorials available for LangChain, covering a wide range of applications from simple LLM setups to complex agents and RAG systems. SOURCE: https://python.langchain.com/docs/how_to/ensemble_retriever/\n\nLANGUAGE: python\nCODE:\n```\nBuild a Question Answering application over a Graph Database\nBuild a simple LLM application with chat models and prompt templates\nBuild a Chatbot\nBuild a Retrieval Augmented Generation (RAG) App: Part 2\nBuild an Extraction Chain\nBuild an Agent\nTagging\nBuild a Retrieval Augmented Generation (RAG) App: Part 1\nBuild a semantic search engine\nBuild a Question/Answering system over SQL data\nSummarize Text\n```\n\n----------------------------------------\n\nTITLE: LangSmith Setup and API Key Configuration\nDESCRIPTION: Sets up the LangSmith environment by configuring the API key and enabling tracing.",
    "chunk_length": 2483
  },
  {
    "chunk_id": 72,
    "source": "python_langchain_llms_data",
    "content": "It also installs necessary libraries like langsmith, langchain-core, langchain, langchain-openai, and langchain-benchmarks. SOURCE: https://python.langchain.com/docs/how_to/example_selectors_langsmith/\n\nLANGUAGE: python\nCODE:\n```\nimport getpass\nimport os\n\nif not os.environ.get(\"LANGSMITH_API_KEY\"):\n    os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"Set LangSmith API key:\\n\\n\")\n\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\n\n# %pip install -qU \"langsmith>=0.1.101\" \"langchain-core>=0.2.34\" langchain langchain-openai langchain-benchmarks\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides Overview\nDESCRIPTION: This section provides practical guides on implementing specific features and functionalities within LangChain, such as using tools, memory, vectorstores, and parallel execution. SOURCE: https://python.langchain.com/docs/how_to/message_history/\n\nLANGUAGE: python\nCODE:\n```\nHow to use tools in a chain\nHow to use a vectorstore as a retriever\nHow to add memory to chatbots\nHow to use example selectors\nHow to add a semantic layer over graph database\nHow to invoke runnables in parallel\nHow to stream chat model responses\nHow to add default invocation args to a Runnable\nHow to add retrieval to chatbots\nHow to use few shot examples in chat models\nHow to do tool/function calling\nHow to install LangChain packages\nHow to add examples to the prompt for query analysis\nHow to use few shot examples\nHow to run custom functions\nHow to use output parsers to parse an LLM response into structured format\nHow to handle cases where no queries are generated\nHow to route between sub-chains\nHow to return structured data from a model\nHow to summarize text through parallelization\n```\n\n----------------------------------------\n\nTITLE: How to Use Few Shot Examples in Chat Models\nDESCRIPTION: This guide demonstrates how to provide few-shot examples to LangChain chat models to improve their performance on specific tasks. By including example input-output pairs in the prompt, the model can better understand the desired behavior.",
    "chunk_length": 2055
  },
  {
    "chunk_id": 73,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/how_to/custom_tools/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.schema import HumanMessage, SystemMessage, AIMessage\n\n# Initialize a chat modelllm = ChatOpenAI()\n\n# Define few-shot examples for sentiment analysismessages = [\n    SystemMessage(content=\"You are a sentiment analysis assistant. Classify the sentiment of the following text.\"),\n    HumanMessage(content=\"I love this new movie!\"),\n    AIMessage(content=\"Positive\"),\n    HumanMessage(content=\"The service was terrible.\"),\n    AIMessage(content=\"Negative\"),\n    HumanMessage(content=\"It was an okay experience.\")\n]\n\n# Send the messages including examples to the chat model\nesponse = llm(messages)\nprint(response.content)\n```\n\n----------------------------------------\n\nTITLE: LangChain Tutorials Overview\nDESCRIPTION: This section lists various tutorials available for LangChain, covering a wide range of applications from question answering to building agents and chatbots. SOURCE: https://python.langchain.com/docs/how_to/contextual_compression/\n\nLANGUAGE: markdown\nCODE:\n```\n*   [Build a Question Answering application over a Graph Database](/docs/tutorials/graph/)\n*   [Build a simple LLM application with chat models and prompt templates](/docs/tutorials/llm_chain/)\n*   [Build a Chatbot](/docs/tutorials/chatbot/)\n*   [Build a Retrieval Augmented Generation (RAG) App: Part 2](/docs/tutorials/qa_chat_history/)\n*   [Build an Extraction Chain](/docs/tutorials/extraction/)\n*   [Build an Agent](/docs/tutorials/agents/)\n*   [Tagging](/docs/tutorials/classification/)\n*   [Build a Retrieval Augmented Generation (RAG) App: Part 1](/docs/tutorials/rag/)\n*   [Build a semantic search engine](/docs/tutorials/retrievers/)\n*   [Build a Question/Answering system over SQL data](/docs/tutorials/sql_qa/)\n*   [Summarize Text](/docs/tutorials/summarization/)\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: Practical guides on implementing specific features and functionalities within LangChain, such as using tools, memory, output parsers, and more.",
    "chunk_length": 2134
  },
  {
    "chunk_id": 74,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/how_to/output_parser_json/\n\nLANGUAGE: python\nCODE:\n```\nHow to use tools in a chain\nHow to use a vectorstore as a retriever\nHow to add memory to chatbots\nHow to use example selectors\nHow to add a semantic layer over graph database\nHow to invoke runnables in parallel\nHow to stream chat model responses\nHow to add default invocation args to a Runnable\nHow to add retrieval to chatbots\nHow to use few shot examples in chat models\nHow to do tool/function calling\nHow to install LangChain packages\nHow to add examples to the prompt for query analysis\nHow to use few shot examples\nHow to run custom functions\nHow to use output parsers to parse an LLM response into structured format\nHow to handle cases where no queries are generated\nHow to route between sub-chains\nHow to return structured data from a model\nHow to summarize text through parallelization\n```\n\n----------------------------------------\n\nTITLE: LangChain Tutorials Overview\nDESCRIPTION: A collection of tutorials demonstrating how to build various applications using LangChain, from simple LLM interactions to complex agents and retrieval systems. SOURCE: https://python.langchain.com/docs/how_to/qa_per_user/\n\nLANGUAGE: APIDOC\nCODE:\n```\nTutorials:\n  - Build a Question Answering application over a Graph Database\n  - Build a simple LLM application with chat models and prompt templates\n  - Build a Chatbot\n  - Build a Retrieval Augmented Generation (RAG) App: Part 2\n  - Build an Extraction Chain\n  - Build an Agent\n  - Tagging\n  - Build a Retrieval Augmented Generation (RAG) App: Part 1\n  - Build a semantic search engine\n  - Build a Question/Answering system over SQL data\n  - Summarize Text\n```\n\n----------------------------------------\n\nTITLE: AI21 Chat Model Integration\nDESCRIPTION: This section covers the integration details, model features, setup (credentials and installation), instantiation, invocation, and chaining of AI21 chat models within LangChain. It serves as a comprehensive guide for developers looking to use AI21's capabilities through the LangChain framework.",
    "chunk_length": 2085
  },
  {
    "chunk_id": 75,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/integrations/chat/ai21/\n\nLANGUAGE: python\nCODE:\n```\n# Conceptual guide for Chat Models\n# import ChatModel from langchain_core.chat_models\n\n# How-to guides for Chat Models\n# from langchain_ai21 import ChatAI21\n\n# Example Setup (Conceptual)\n# Ensure you have the AI21 library installed:\n# pip install langchain-ai21\n\n# Set your AI21 API key as an environment variable:\n# export AI21_API_KEY='YOUR_API_KEY'\n\n# Example Instantiation\n# from langchain_ai21 import ChatAI21\n# chat = ChatAI21()\n\n# Example Invocation\n# from langchain_core.messages import HumanMessage\n# response = chat.invoke([HumanMessage(content=\"Hello!\")])\n# print(response.content)\n\n# Example Chaining (Conceptual)\n# from langchain.chains import LLMChain\n# from langchain.prompts import PromptTemplate\n# \n# prompt = PromptTemplate.from_template(\"What is the capital of {country}?\")\n# chain = LLMChain(llm=chat, prompt=prompt)\n# print(chain.run(\"France\"))\n```\n\n----------------------------------------\n\nTITLE: How to use few shot examples in chat models\nDESCRIPTION: This guide demonstrates how to provide few-shot examples to chat models in LangChain. This technique helps improve the model's performance on specific tasks by showing it examples. SOURCE: https://python.langchain.com/docs/how_to/chatbots_memory/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.prompts import FewShotChatMessagePromptTemplate\n\nexample_prompt = ChatPromptTemplate.from_messages([\n    (\"human\", \"{input}\"),\n    (\"ai\", \"{output}\")\n])\n\nfew_shot_prompt = FewShotChatMessagePromptTemplate(example_prompt=example_prompt, examples=examples)\n\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: Practical guides on implementing specific functionalities within LangChain, such as adding memory to chatbots, using vectorstores as retrievers, and handling tool/function calling. SOURCE: https://python.langchain.com/docs/how_to/qa_per_user/\n\nLANGUAGE: APIDOC\nCODE:\n```\nHow-to guides:\n  - How to use tools in a chain\n  - How to use a vectorstore as a retriever\n  - How to add memory to chatbots\n  - How to use example selectors\n  - How to add a semantic layer over graph database\n  - How to invoke runnables in parallel\n  - How to stream chat model responses\n  - How to add default invocation args to a Runnable\n  - How to add retrieval to chatbots\n  - How to use few shot examples in chat models\n  - How to do tool/function calling\n  - How to install LangChain packages\n  - How to add examples to the prompt for query analysis\n  - How to use few shot examples\n  - How to run custom functions\n  - How to use output parsers to parse an LLM response into structured format\n  - How to handle cases where no queries are generated\n  - How to route between sub-chains\n  - How to return structured data from a model\n  - How to summarize text through parallelization\n```\n\n----------------------------------------\n\nTITLE: How to install LangChain packages\nDESCRIPTION: This guide provides instructions on how to install the necessary LangChain packages for your Python environment.",
    "chunk_length": 3081
  },
  {
    "chunk_id": 76,
    "source": "python_langchain_llms_data",
    "content": "It covers the use of pip for managing dependencies. SOURCE: https://python.langchain.com/docs/how_to/tool_choice/\n\nLANGUAGE: bash\nCODE:\n```\npip install langchain\n# For specific integrations, you might need to install additional packages:\npip install langchain-openai\npip install langchain-community\n# Or install all core packages:\npip install \"langchain[all]\"\n```\n\n----------------------------------------\n\nTITLE: Few-Shot Examples in Chat Models\nDESCRIPTION: Demonstrates how to provide few-shot examples to chat models to guide their responses and improve accuracy for specific tasks. SOURCE: https://python.langchain.com/docs/how_to/output_parser_fixing/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\nfrom langchain_core.messages import SystemMessage, AIMessage\nfrom langchain_openai import ChatOpenAI\n\n# Define few-shot examples\nexample_1 = HumanMessage(\"I love dogs.\")\nexample_1_output = AIMessage(\"I love dogs too! It's great you have a pet.\")\n\nexample_2 = HumanMessage(\"The weather is nice today.\")\nexample_2_output = AIMessage(\"Yes, it is! Perfect for a walk.\")\n\n# Create a prompt template with examples\nprompt = ChatPromptTemplate.from_messages([\n    SystemMessage(\"You are a helpful assistant.\"),\n    example_1,\n    example_1_output,\n    example_2,\n    example_2_output,\n    HumanMessagePromptTemplate.from_template(\"{user_input}\")\n])\n\n# Initialize the chat model\nmodel = ChatOpenAI()\n\n# Create a chain\nchain = prompt | model\n\n# Invoke the chain with user input\nresponse = chain.invoke({\"user_input\": \"What do you think about AI?\"})\nprint(response.content)\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: Practical guides on implementing specific functionalities within LangChain. These cover topics like using tools, vectorstores, memory, prompt selectors, parallel execution, streaming, and function calling. SOURCE: https://python.langchain.com/docs/how_to/indexing/\n\nLANGUAGE: python\nCODE:\n```\nHow to use tools in a chain\nHow to use a vectorstore as a retriever\nHow to add memory to chatbots\nHow to use example selectors\nHow to add a semantic layer over graph database\nHow to invoke runnables in parallel\nHow to stream chat model responses\nHow to add default invocation args to a Runnable\nHow to add retrieval to chatbots\nHow to use few shot examples in chat models\nHow to do tool/function calling\nHow to install LangChain packages\nHow to add examples to the prompt for query analysis\nHow to use few shot examples\nHow to run custom functions\nHow to use output parsers to parse an LLM response into structured format\nHow to handle cases where no queries are generated\nHow to route between sub-chains\nHow to return structured data from a model\n```\n\n----------------------------------------\n\nTITLE: How to use few shot examples\nDESCRIPTION: This guide provides a comprehensive explanation of how to use few-shot examples in LangChain to improve the performance and accuracy of language model responses.",
    "chunk_length": 3041
  },
  {
    "chunk_id": 77,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/how_to/extraction_examples/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\nfrom langchain_openai import ChatOpenAI\nfrom langchain.chains import LLMChain\n\n# Define few-shot examplesexamples = [\n    {\"input\": \"sea otter\", \"output\": \"otter\"},\n    {\"input\": \"peppermint\", \"output\": \"mint\"},\n    {\"input\": \"cheese\", \"output\": \"dairy product\"},\n]\n\n# Create a prompt template for the examplesexample_prompt = ChatPromptTemplate.from_messages([\n    (\"human\", \"{input}\"),\n    (\"ai\", \"{output}\"),\n])\n\n# Create the FewShotChatMessagePromptTemplate\few_shot_prompt = FewShotChatMessagePromptTemplate(\n    examples\n```\n\n----------------------------------------\n\nTITLE: How to use few shot examples in chat models\nDESCRIPTION: This section details how to leverage few-shot examples when working with chat models in LangChain. It demonstrates how to structure prompts with example conversations to guide the chat model's responses. SOURCE: https://python.langchain.com/docs/how_to/query_few_shot/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\nfrom langchain_core.messages import SystemMessage, HumanMessage\n\n# Define example messages\nexample_messages = [\n    HumanMessage(content=\"Hi, I'm Bob!\"),\n    SystemMessage(content=\"Hi Bob! How can I help you today?\"),\n    HumanMessage(content=\"Can you tell me about LangChain?\"),\n    SystemMessage(content=\"LangChain is a framework for developing applications powered by language models.\")\n]\n\n# Create the chat prompt template\nchat_prompt = ChatPromptTemplate.from_messages([\n    SystemMessage(content=\"You are a helpful assistant.\"),\n    MessagesPlaceholder(variable_name=\"examples\"),\n    HumanMessagePromptTemplate.from_template(\"{user_input}\")\n])\n\n# Format the prompt with examples and user input\nformatted_chat_prompt = chat_prompt.format_messages(\n    examples=example_messages,\n    user_input=\"What is RAG?\"\n)\n\n# Print the formatted messages\nfor message in formatted_chat_prompt:\n    print(f\"{message.type}: {message.content}\")\n\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools.",
    "chunk_length": 2366
  },
  {
    "chunk_id": 78,
    "source": "python_langchain_llms_data",
    "content": "This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/how_to/contextual_compression/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: How to use few shot examples in chat models\nDESCRIPTION: This guide explains how to provide few-shot examples to chat models in LangChain to improve their performance on specific tasks. Including examples in the prompt helps the model understand the desired output format and behavior. SOURCE: https://python.langchain.com/docs/how_to/tool_choice/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\nfrom langchain.prompts import SystemMessagePromptTemplate\n\n# Define system message and few-shot examples\nsystem_template = \"You are a helpful assistant.\"\nexample_human = \"Translate English to French: 'I love programming.'\"\nexample_ai = \"J'adore la programmation.\"\n\n# Create the prompt template with few-shot examples\nchat_prompt = ChatPromptTemplate.from_messages([\n    SystemMessagePromptTemplate.from_template(system_template),\n    HumanMessagePromptTemplate.from_template(example_human),\n    AIMessagePromptTemplate.from_template(example_ai),\n    HumanMessagePromptTemplate.from_template(\"Translate English to French: '{input}'\"),\n])\n\n# Initialize the chat model\nllm = ChatOpenAI()\n\n# Create a chain\nchain = chat_prompt | llm\n\n# Invoke the chain with a new input\nresponse = chain.invoke({\"input\": \"Hello, how are you?\"})\nprint(response.content)\n```\n\n----------------------------------------\n\nTITLE: How to use few shot examples in chat models\nDESCRIPTION: Learn how to provide few-shot examples to chat models in LangChain to improve their performance and guide their responses.",
    "chunk_length": 3194
  },
  {
    "chunk_id": 79,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/concepts/rag/\n\nLANGUAGE: python\nCODE:\n```\nprint('How-to: How to use few shot examples in chat models')\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides Overview\nDESCRIPTION: This section provides practical guides on implementing specific features and functionalities within LangChain, such as using tools, vectorstores, memory, and output parsers. SOURCE: https://python.langchain.com/docs/how_to/tool_results_pass_to_model/\n\nLANGUAGE: markdown\nCODE:\n```\n- [How to use tools in a chain](/docs/how_to/tools_chain/)\n- [How to use a vectorstore as a retriever](/docs/how_to/vectorstore_retriever/)\n- [How to add memory to chatbots](/docs/how_to/chatbots_memory/)\n- [How to use example selectors](/docs/how_to/example_selectors/)\n- [How to add a semantic layer over graph database](/docs/how_to/graph_semantic/)\n- [How to invoke runnables in parallel](/docs/how_to/parallel/)\n- [How to stream chat model responses](/docs/how_to/chat_streaming/)\n- [How to add default invocation args to a Runnable](/docs/how_to/binding/)\n- [How to add retrieval to chatbots](/docs/how_to/chatbots_retrieval/)\n- [How to use few shot examples in chat models](/docs/how_to/few_shot_examples_chat/)\n- [How to do tool/function calling](/docs/how_to/function_calling/)\n- [How to install LangChain packages](/docs/how_to/installation/)\n- [How to add examples to the prompt for query analysis](/docs/how_to/query_few_shot/)\n- [How to use few shot examples](/docs/how_to/few_shot_examples/)\n- [How to run custom functions](/docs/how_to/functions/)\n- [How to use output parsers to parse an LLM response into structured format](/docs/how_to/output_parser_structured/)\n- [How to handle cases where no queries are generated](/docs/how_to/query_no_queries/)\n- [How to route between sub-chains](/docs/how_to/routing/)\n- [How to return structured data from a model](/docs/how_to/structured_output/)\n```\n\n----------------------------------------\n\nTITLE: Setup and Environment Variables\nDESCRIPTION: Installs necessary packages and sets up LangSmith environment variables for tracing.",
    "chunk_length": 2116
  },
  {
    "chunk_id": 80,
    "source": "python_langchain_llms_data",
    "content": "It requires the langchain-community and langgraph packages and optionally configures LangSmith API key and tracing. SOURCE: https://python.langchain.com/docs/tutorials/sql_qa/\n\nLANGUAGE: python\nCODE:\n```\n%%capture --no-stderr\n%pip install --upgrade --quiet langchain-community langgraph\n\n# Comment out the below to opt-out of using LangSmith in this notebook. Not required. if not os.environ.get(\"LANGSMITH_API_KEY\"):\n    os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n    os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n```\n\n----------------------------------------\n\nTITLE: LangChain Tutorials\nDESCRIPTION: Lists various tutorials available for LangChain, covering different use cases and functionalities such as building QA applications, chatbots, agents, RAG, and more. SOURCE: https://python.langchain.com/docs/how_to/callbacks_constructor/\n\nLANGUAGE: APIDOC\nCODE:\n```\nTutorials:\n  - Build a Question Answering application over a Graph Database: /docs/tutorials/graph/\n  - Build a simple LLM application with chat models and prompt templates: /docs/tutorials/llm_chain/\n  - Build a Chatbot: /docs/tutorials/chatbot/\n  - Build a Retrieval Augmented Generation (RAG) App: Part 2: /docs/tutorials/qa_chat_history/\n  - Build an Extraction Chain: /docs/tutorials/extraction/\n  - Build an Agent: /docs/tutorials/agents/\n  - Tagging: /docs/tutorials/classification/\n  - Build a Retrieval Augmented Generation (RAG) App: Part 1: /docs/tutorials/rag/\n  - Build a semantic search engine: /docs/tutorials/retrievers/\n  - Build a Question/Answering system over SQL data: /docs/tutorials/sql_qa/\n  - Summarize Text: /docs/tutorials/summarization/\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides Overview\nDESCRIPTION: This section provides practical guides on how to implement specific features and functionalities within LangChain, such as using tools, memory, output parsers, and more. SOURCE: https://python.langchain.com/docs/how_to/ensemble_retriever/\n\nLANGUAGE: python\nCODE:\n```\nHow to use tools in a chain\nHow to use a vectorstore as a retriever\nHow to add memory to chatbots\nHow to use example selectors\nHow to add a semantic layer over graph database\nHow to invoke runnables in parallel\nHow to stream chat model responses\nHow to add default invocation args to a Runnable\nHow to add retrieval to chatbots\nHow to use few shot examples in chat models\nHow to do tool/function calling\nHow to install LangChain packages\nHow to add examples to the prompt for query analysis\nHow to use few shot examples\nHow to run custom functions\nHow to use output parsers to parse an LLM response into structured format\nHow to handle cases where no queries are generated\nHow to route between sub-chains\nHow to return structured data from a model\n```\n\n----------------------------------------\n\nTITLE: Few-Shot Example Prompting\nDESCRIPTION: Illustrates the concept of few-shot example prompting for AI agents, where past input-output examples are used to guide the model's behavior.",
    "chunk_length": 2986
  },
  {
    "chunk_id": 81,
    "source": "python_langchain_llms_data",
    "content": "This is a common technique for episodic memory implementation. SOURCE: https://langchain-ai.github.io/langgraph/concepts/memory/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\nfrom langchain_openai import ChatOpenAI\n\n# Define the examples\nexamples = [\n    {\n        \"input\": \"Translate to French: Hello world\",\n        \"output\": \"Bonjour le monde\"\n    },\n    {\n        \"input\": \"Translate to French: How are you?\",\n        \"output\": \"Comment allez-vous?\"\n    }\n]\n\n# Create a prompt template for the examples\nexample_prompt = PromptTemplate(\n    input_variables=[\"input\", \"output\"],\n    template=\"Input: {input}\\nOutput: {output}\",\n)\n\n# Create the FewShotPromptTemplate\nprompt = FewShotPromptTemplate(\n    examples=examples,\n    example_prompt=example_prompt,\n    prefix=\"Translate English sentences to French.\",\n    suffix=\"Translate English to French: {input}\",\n    input_variables=[\"input\"],\n    example_separator=\"\\n\\n\",\n)\n\n# Initialize the language model\nllm = ChatOpenAI()\n\n# Format the prompt and invoke the LLM\nformatted_prompt = prompt.format(input=\"What is your name?\")\nresponse = llm.invoke(formatted_prompt)\n\nprint(response.content)\n```\n\n----------------------------------------\n\nTITLE: Install Dependencies\nDESCRIPTION: Installs the necessary Python packages for the guide, including langchain-community, lxml, faiss-cpu, and langchain-openai. SOURCE: https://python.langchain.com/docs/how_to/extraction_long_text/\n\nLANGUAGE: python\nCODE:\n```\n%pip install -qU langchain-community lxml faiss-cpu langchain-openai\n```\n\n----------------------------------------\n\nTITLE: Maximal Marginal Relevance (MMR) Example Selector Setup\nDESCRIPTION: Demonstrates how to initialize and use the MaxMarginalRelevanceExampleSelector. This selector picks examples that are semantically similar to the input while also ensuring diversity among the selected examples. It requires a list of examples, an embedding model (OpenAIEmbeddings), and a vector store (FAISS). SOURCE: https://python.langchain.com/docs/how_to/example_selectors_mmr/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.example_selectors import (\n    MaxMarginalRelevanceExampleSelector,\n    SemanticSimilarityExampleSelector,\n)\nfrom langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\nfrom langchain_openai import OpenAIEmbeddings\n\nexample_prompt = PromptTemplate(\n    input_variables=[\"input\", \"output\"],\n    template=\"Input: {input}\\nOutput: {output}\",\n)\n\n# Examples of a pretend task of creating antonyms.",
    "chunk_length": 2587
  },
  {
    "chunk_id": 82,
    "source": "python_langchain_llms_data",
    "content": "examples = [\n    {\"input\": \"happy\", \"output\": \"sad\"},\n    {\"input\": \"tall\", \"output\": \"short\"},\n    {\"input\": \"energetic\", \"output\": \"lethargic\"},\n    {\"input\": \"sunny\", \"output\": \"gloomy\"},\n    {\"input\": \"windy\", \"output\": \"calm\"},\n]\n\nexample_selector = MaxMarginalRelevanceExampleSelector.from_examples(\n    # The list of examples available to select from. examples,\n    # The embedding class used to produce embeddings which are used to measure semantic similarity. OpenAIEmbeddings(),\n    # The VectorStore class that is used to store the embeddings and do a similarity search over. FAISS,\n    # The number of examples to produce. k=2,\n)\n\nmmr_prompt = FewShotPromptTemplate(\n    # We provide an ExampleSelector instead of examples. example_selector=example_selector,\n    example_prompt=example_prompt,\n    prefix=\"Give the antonym of every input\",\n    suffix=\"Input: {adjective}\\nOutput:\",\n    input_variables=[\"adjective\"],\n)\n\n# Input is a feeling, so should select the happy/sad example as the first one\nprint(mmr_prompt.format(adjective=\"worried\"))\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: Practical guides on implementing specific features and functionalities within LangChain applications. SOURCE: https://python.langchain.com/docs/how_to/callbacks_async/\n\nLANGUAGE: APIDOC\nCODE:\n```\nLangChain How-to Guides:\n  - How to use tools in a chain\n    URL: /docs/how_to/tools_chain/\n  - How to use a vectorstore as a retriever\n    URL: /docs/how_to/vectorstore_retriever/\n  - How to add memory to chatbots\n    URL: /docs/how_to/chatbots_memory/\n  - How to use example selectors\n    URL: /docs/how_to/example_selectors/\n  - How to add a semantic layer over graph database\n    URL: /docs/how_to/graph_semantic/\n  - How to invoke runnables in parallel\n    URL: /docs/how_to/parallel/\n  - How to stream chat model responses\n    URL: /docs/how_to/chat_streaming/\n  - How to add default invocation args to a Runnable\n    URL: /docs/how_to/binding/\n  - How to add retrieval to chatbots\n    URL: /docs/how_to/chatbots_retrieval/\n  - How to use few shot examples in chat models\n    URL: /docs/how_to/few_shot_examples_chat/\n  - How to do tool/function calling\n    URL: /docs/how_to/function_calling/\n  - How to install LangChain packages\n    URL: /docs/how_to/installation/\n  - How to add examples to the prompt for query analysis\n    URL: /docs/how_to/query_few_shot/\n  - How to use few shot examples\n    URL: /docs/how_to/few_shot_examples/\n  - How to run custom functions\n    URL: /docs/how_to/functions/\n  - How to use output parsers to parse an LLM response into structured format\n    URL: /docs/how_to/output_parser_structured/\n  - How to handle cases where no queries are generated\n    URL: /docs/how_to/query_no_queries/\n  - How to route between sub-chains\n    URL: /docs/how_to/routing/\n  - How to return structured data from a model\n    URL: /docs/how_to/structured_output/\n```\n\n----------------------------------------\n\nTITLE: LangChain Tutorials\nDESCRIPTION: A collection of tutorials covering various LangChain use cases, from building simple LLM applications to complex agents and RAG systems.",
    "chunk_length": 3150
  },
  {
    "chunk_id": 83,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/how_to/example_selectors_ngram/\n\nLANGUAGE: APIDOC\nCODE:\n```\nLangChain Tutorials:\n  - Introduction: Getting started with LangChain. - LLM Chain: Building a simple LLM application. - Chatbot: Creating a chatbot with LangChain. - RAG: Building Retrieval Augmented Generation applications (Part 1 & 2). - Agents: Developing agents with LangChain. - SQL QA: Question answering over SQL data. - Summarization: Summarizing text using LangChain. - Semantic Search: Building a semantic search engine. ```\n\n----------------------------------------\n\nTITLE: How to use few shot examples in chat models\nDESCRIPTION: This guide explains how to provide few-shot examples to chat models in LangChain to improve their performance on specific tasks. SOURCE: https://python.langchain.com/docs/tutorials/extraction/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate, MessagesPlaceholder\nfrom langchain_openai import ChatOpenAI\n\n# Example messages\n# system_message = SystemMessagePromptTemplate.from_template(\"You are a helpful assistant.\")\n# human_message = HumanMessagePromptTemplate.from_template(\"User: {input}\\nAssistant:\")\n\n# Create a chat prompt with few-shot examples\n# chat_prompt = ChatPromptTemplate.from_messages([\n#     system_message,\n#     (\"human\", \"What is the capital of France?\"),\n#     (\"ai\", \"The capital of France is Paris.\"),\n#     human_message\n# ])\n\n# Initialize the chat model\n# llm = ChatOpenAI()\n\n# Create the chain\n# chain = chat_prompt | llm\n\n# Example usage:\n# response = chain.invoke({\"input\": \"What is the capital of Germany?\"})\n# print(response.content)\n```\n\n----------------------------------------\n\nTITLE: How to use few shot examples in chat models\nDESCRIPTION: This guide explains how to provide few-shot examples to chat models in LangChain to improve their performance on specific tasks. SOURCE: https://python.langchain.com/docs/how_to/chatbots_tools/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_community.chat_models import ChatOpenAI\nfrom langchain.memory import ChatMessageHistory\n\nchat = ChatOpenAI()\n\nmessage_history = ChatMessageHistory()\nmessage_history.add_user_message(\"Hi\")\nmessage_history.add_ai_message(\"Hello!\")\n\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant.\"),\n    MessagesPlaceholder(variable_name=\"history\"),\n    (\"human\", \"{input}\")\n])\n\nchain = prompt | chat\n\n# result = chain.invoke({\"history\": message_history.messages, \"input\": \"How are you?\"})\n```\n\n----------------------------------------\n\nTITLE: Define Few-Shot Examples\nDESCRIPTION: Creates a list of dictionaries, where each dictionary represents a question-answer pair for few-shot learning.",
    "chunk_length": 2835
  },
  {
    "chunk_id": 84,
    "source": "python_langchain_llms_data",
    "content": "These examples are used to train or guide the LLM. SOURCE: https://python.langchain.com/docs/how_to/few_shot_examples/\n\nLANGUAGE: python\nCODE:\n```\nexamples = [\n    {\n        \"question\": \"Who lived longer, Muhammad Ali or Alan Turing?\",\n        \"answer\": \"\"\"Are follow up questions needed here: Yes.Follow up: How old was Muhammad Ali when he died?Intermediate answer: Muhammad Ali was 74 years old when he died.Follow up: How old was Alan Turing when he died?Intermediate answer: Alan Turing was 41 years old when he died.So the final answer is: Muhammad Ali\"\"\",\n    },\n    {\n        \"question\": \"When was the founder of craigslist born?\",\n        \"answer\": \"\"\"Are follow up questions needed here: Yes.Follow up: Who was the founder of craigslist?Intermediate answer: Craigslist was founded by Craig Newmark.Follow up: When was Craig Newmark born?Intermediate answer: Craig Newmark was born on December 6, 1952.So the final answer is: December 6, 1952\"\"\",\n    },\n    {\n        \"question\": \"Who was the maternal grandfather of George Washington?\",\n        \"answer\": \"\"\"Are follow up questions needed here: Yes.Follow up: Who was the mother of George Washington?Intermediate answer: The mother of George Washington was Mary Ball Washington.Follow up: Who was the father of Mary Ball Washington?Intermediate answer: The father of Mary Ball Washington was Joseph Ball.So the final answer is: Joseph Ball\"\"\",\n    },\n    {\n        \"question\": \"Are both the directors of Jaws and Casino Royale from the same country?\",\n        \"answer\": \"\"\"Are follow up questions needed here: Yes.Follow up: Who is the director of Jaws?Intermediate Answer: The director of Jaws is Steven Spielberg.Follow up: Where is Steven Spielberg from?Intermediate Answer: The United States.Follow up: Who is the director of Casino Royale?Intermediate Answer: The director of Casino Royale is Martin Campbell.Follow up: Where is Martin Campbell from?Intermediate Answer: New Zealand.So the final answer is: No\"\"\",\n    },\n]\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides Overview\nDESCRIPTION: A collection of how-to guides for LangChain, offering practical instructions on implementing specific features and functionalities.",
    "chunk_length": 2219
  },
  {
    "chunk_id": 85,
    "source": "python_langchain_llms_data",
    "content": "These guides cover topics such as tool usage, memory, parallel execution, streaming, and more. SOURCE: https://python.langchain.com/docs/how_to/sql_large_db/\n\nLANGUAGE: APIDOC\nCODE:\n```\nHow-to guides:\n  - How to use tools in a chain\n  - How to use a vectorstore as a retriever\n  - How to add memory to chatbots\n  - How to use example selectors\n  - How to add a semantic layer over graph database\n  - How to invoke runnables in parallel\n  - How to stream chat model responses\n  - How to add default invocation args to a Runnable\n  - How to add retrieval to chatbots\n  - How to use few shot examples in chat models\n  - How to do tool/function calling\n  - How to install LangChain packages\n  - How to add examples to the prompt for query analysis\n  - How to use few shot examples\n  - How to run custom functions\n  - How to use output parsers to parse an LLM response into structured format\n  - How to handle cases where no queries are generated\n  - How to route between sub-chains\n  - How to return structured data from a model\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools. This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/how_to/example_selectors_similarity/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant.",
    "chunk_length": 2088
  },
  {
    "chunk_id": 86,
    "source": "python_langchain_llms_data",
    "content": "Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools. This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/how_to/callbacks_constructor/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: Practical guides on implementing specific features and functionalities within LangChain, such as using tools, vectorstores, memory, and output parsers.",
    "chunk_length": 2681
  },
  {
    "chunk_id": 87,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/concepts/tools/\n\nLANGUAGE: APIDOC\nCODE:\n```\nHow-to Guides:\n  /docs/how_to/\n\nGuides cover:\n- Installing LangChain packages\n- Using tools and chains\n- Integrating vectorstores as retrievers\n- Adding memory to chatbots\n- Implementing function calling and structured output\n- Parallel execution and streaming responses\n- Prompt engineering techniques (few-shot examples, example selectors)\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools. This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/how_to/migrate_agent/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides Overview\nDESCRIPTION: This section provides guides on how to implement specific features and functionalities within LangChain applications.",
    "chunk_length": 2385
  },
  {
    "chunk_id": 88,
    "source": "python_langchain_llms_data",
    "content": "Topics include using tools, memory, vectorstores, streaming, and more. SOURCE: https://python.langchain.com/docs/how_to/pydantic_compatibility/\n\nLANGUAGE: markdown\nCODE:\n```\n*   [How to use tools in a chain](/docs/how_to/tools_chain/)\n*   [How to use a vectorstore as a retriever](/docs/how_to/vectorstore_retriever/)\n*   [How to add memory to chatbots](/docs/how_to/chatbots_memory/)\n*   [How to use example selectors](/docs/how_to/example_selectors/)\n*   [How to add a semantic layer over graph database](/docs/how_to/graph_semantic/)\n*   [How to invoke runnables in parallel](/docs/how_to/parallel/)\n*   [How to stream chat model responses](/docs/how_to/chat_streaming/)\n*   [How to add default invocation args to a Runnable](/docs/how_to/binding/)\n*   [How to add retrieval to chatbots](/docs/how_to/chatbots_retrieval/)\n*   [How to use few shot examples in chat models](/docs/how_to/few_shot_examples_chat/)\n*   [How to do tool/function calling](/docs/how_to/function_calling/)\n*   [How to install LangChain packages](/docs/how_to/installation/)\n*   [How to add examples to the prompt for query analysis](/docs/how_to/query_few_shot/)\n*   [How to use few shot examples](/docs/how_to/few_shot_examples/)\n*   [How to run custom functions](/docs/how_to/functions/)\n*   [How to use output parsers to parse an LLM response into structured format](/docs/how_to/output_parser_structured/)\n*   [How to handle cases where no queries are generated](/docs/how_to/query_no_queries/)\n*   [How to route between sub-chains](/docs/how_to/routing/)\n*   [How to return structured data from a model](/docs/how_to/structured_output/)\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: This section provides practical guides on implementing specific features within LangChain, such as using tools, vectorstores, memory, prompt selectors, parallel execution, streaming, and function calling. SOURCE: https://python.langchain.com/docs/how_to/tool_calling/\n\nLANGUAGE: python\nCODE:\n```\nHow to use tools in a chain\nHow to use a vectorstore as a retriever\nHow to add memory to chatbots\nHow to use example selectors\nHow to add a semantic layer over graph database\nHow to invoke runnables in parallel\nHow to stream chat model responses\nHow to add default invocation args to a Runnable\nHow to add retrieval to chatbots\nHow to use few shot examples in chat models\nHow to do tool/function calling\nHow to install LangChain packages\nHow to add examples to the prompt for query analysis\nHow to use few shot examples\nHow to run custom functions\nHow to use output parsers to parse an LLM response into structured format\nHow to handle cases where no queries are generated\nHow to route between sub-chains\nHow to return structured data from a model\n```\n\n----------------------------------------\n\nTITLE: Create PromptTemplate Formatter\nDESCRIPTION: Configures a PromptTemplate to format few-shot examples into a string.",
    "chunk_length": 2923
  },
  {
    "chunk_id": 89,
    "source": "python_langchain_llms_data",
    "content": "This template takes a 'question' and 'answer' as input. SOURCE: https://python.langchain.com/docs/how_to/few_shot_examples/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.prompts import PromptTemplate\n\nexample_prompt = PromptTemplate.from_template(\"Question: {question}\\n{answer}\")\n```\n\n----------------------------------------\n\nTITLE: Install Langchain Packages\nDESCRIPTION: Installs or upgrades the necessary Langchain packages for SQL database interaction and OpenAI integration. Also includes optional setup for LangSmith tracing. SOURCE: https://python.langchain.com/docs/how_to/sql_large_db/\n\nLANGUAGE: python\nCODE:\n```\n%pip install --upgrade --quiet  langchain langchain-community langchain-openai\n\n# Uncomment the below to use LangSmith. Not required. # import getpass\n# import os\n# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n```\n\n----------------------------------------\n\nTITLE: Prompt Engineering Example\nDESCRIPTION: Provides an example of constructing and using prompts for LLMs. Effective prompt engineering is crucial for guiding LLM responses and achieving desired outcomes. SOURCE: https://python.langchain.com/docs/how_to/summarize_refine/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.prompts import PromptTemplate\n\nprompt = PromptTemplate(\n    input_variables=[\"topic\"],\n    template=\"Tell me a short joke about {topic}.\",\n)\n\nformatted_prompt = prompt.format(topic=\"cats\")\nprint(formatted_prompt)\n```\n\n----------------------------------------\n\nTITLE: Using Toolkits\nDESCRIPTION: Provides guidance on how to leverage pre-built toolkits within Langchain for various tasks. Toolkits bundle related tools and chains to simplify complex workflows. SOURCE: https://python.langchain.com/docs/how_to/installation/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.agents import load_tools\nfrom langchain.llms import OpenAI\n\nllm = OpenAI(temperature=0)\n# Load tools for a specific toolkit, e.g., 'python-repl'\ntools = load_tools([\"python-repl\"], llm=llm)\n# Further steps to use these tools with an agent...",
    "chunk_length": 2059
  },
  {
    "chunk_id": 90,
    "source": "python_langchain_llms_data",
    "content": "```\n\n----------------------------------------\n\nTITLE: How to use few shot examples in chat models\nDESCRIPTION: This guide demonstrates how to provide few-shot examples directly to chat models to improve their performance on specific tasks by showing desired input-output patterns. SOURCE: https://python.langchain.com/docs/how_to/extraction_examples/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate, AIMessagePromptTemplate\nfrom langchain_openai import ChatOpenAI\nfrom langchain.chains import LLMChain\n\n# Define few-shot examples as chat messagessystem_message = SystemMessagePromptTemplate.from_template(\"You are a helpful assistant.\")\nhuman_message1 = HumanMessagePromptTemplate.from_template(\"Input: What is 2+2?\")\nai_message1 = AIMessagePromptTemplate.from_template(\"Output: 4\")\nhuman_message2 = HumanMessagePromptTemplate.from_template(\"Input: What is 5*3?\")\nai_message2 = AIMessagePromptTemplate.from_template(\"Output: 15\")\n\n# Create the final prompt template including exampleschat_prompt = ChatPromptTemplate.from_messages([\n    system_message,\n    human_message1, ai_message1,\n    human_message2, ai_message2,\n    (\"human\", \"{input}\")\n])\n\n# Initialize LLM and create chainllm = ChatOpenAI(model=\"gpt-3.5-turbo\")\nchain = LLMChain(llm=llm, prompt=chat_prompt)\n\n# Example usage\nesult = chain.invoke({\"input\": \"What is 10/2?\"})\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: Guides on specific functionalities and implementation details within LangChain, such as using tools, vectorstores, memory, parallel execution, and streaming. SOURCE: https://python.langchain.com/docs/concepts/retrieval/\n\nLANGUAGE: python\nCODE:\n```\nHow to use tools in a chain\nHow to use a vectorstore as a retriever\nHow to add memory to chatbots\nHow to use example selectors\nHow to add a semantic layer over graph database\nHow to invoke runnables in parallel\nHow to stream chat model responses\nHow to add default invocation args to a Runnable\nHow to add retrieval to chatbots\nHow to use few shot examples in chat models\nHow to do tool/function calling\nHow to install LangChain packages\nHow to add examples to the prompt for query analysis\nHow to use few shot examples\nHow to run custom functions\nHow to use output parsers to parse an LLM response into structured format\nHow to handle cases where no queries are generated\nHow to route between sub-chains\nHow to return structured data from a model\nHow to summarize text through parallelization\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides - Overview\nDESCRIPTION: This section lists various practical guides for using LangChain, covering common tasks and functionalities.",
    "chunk_length": 2780
  },
  {
    "chunk_id": 91,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/how_to/markdown_header_metadata_splitter/\n\nLANGUAGE: APIDOC\nCODE:\n```\nHow-to Guides:\n  - Trimming Messages: Managing message length and content. - Vector Stores: Creating and querying vector databases. - Pydantic Compatibility: Ensuring compatibility with Pydantic models. - Migrating Chains: Guidance for upgrading from older chain implementations. - Migrating Memory: Adapting memory components for new versions or frameworks like LangGraph. ```\n\n----------------------------------------\n\nTITLE: openGauss VectorStore Integration\nDESCRIPTION: This notebook covers how to get started with the openGauss VectorStore. SOURCE: https://python.langchain.com/docs/integrations/vectorstores/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.vectorstores import OpenGaussVectorStore\n\n# Example usage (assuming openGauss connection)\n# vector_store = OpenGaussVectorStore(table_name=\"my_table\")\n# results = vector_store.similarity_search(\"query text\")\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: Practical guides for common LangChain tasks, including how to trim messages and how to create and query vector stores. SOURCE: https://python.langchain.com/docs/how_to/passthrough/\n\nLANGUAGE: python\nCODE:\n```\n## How-to Guide Topics:\n- How to trim messages\n- How to create and query vector stores\n- How to pass through arguments from one step to the next\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools. This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/how_to/tool_runtime/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant.",
    "chunk_length": 2481
  },
  {
    "chunk_id": 92,
    "source": "python_langchain_llms_data",
    "content": "Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: Practical guides on implementing specific features and functionalities within LangChain. SOURCE: https://python.langchain.com/docs/how_to/MultiQueryRetriever/\n\nLANGUAGE: markdown\nCODE:\n```\n*   [How-to guides](/docs/how_to/)\n    \n    *   [How-to guides](/docs/how_to/)\n    *   [How to use tools in a chain](/docs/how_to/tools_chain/)\n    *   [How to use a vectorstore as a retriever](/docs/how_to/vectorstore_retriever/)\n    *   [How to add memory to chatbots](/docs/how_to/chatbots_memory/)\n    *   [How to use example selectors](/docs/how_to/example_selectors/)\n    *   [How to add a semantic layer over graph database](/docs/how_to/graph_semantic/)\n    *   [How to invoke runnables in parallel](/docs/how_to/parallel/)\n    *   [How to stream chat model responses](/docs/how_to/chat_streaming/)\n    *   [How to add default invocation args to a Runnable](/docs/how_to/binding/)\n    *   [How to add retrieval to chatbots](/docs/how_to/chatbots_retrieval/)\n    *   [How to use few shot examples in chat models](/docs/how_to/few_shot_examples_chat/)\n    *   [How to do tool/function calling](/docs/how_to/function_calling/)\n    *   [How to install LangChain packages](/docs/how_to/installation/)\n    *   [How to add examples to the prompt for query analysis](/docs/how_to/query_few_shot/)\n    *   [How to use few shot examples](/docs/how_to/few_shot_examples/)\n    *   [How to run custom functions](/docs/how_to/functions/)\n    *   [How to use output parsers to parse an LLM response into structured format](/docs/how_to/output_parser_structured/)\n    *   [How to handle cases where no queries are generated](/docs/how_to/query_no_queries/)\n    *   [How to route between sub-chains](/docs/how_to/routing/)\n    *   [How to return structured data from a model](/docs/how_to/structured_output/)\n```\n\n----------------------------------------\n\nTITLE: SemanticSimilarityExampleSelector Initialization and Usage\nDESCRIPTION: Shows how to initialize a SemanticSimilarityExampleSelector using examples, an embedding model (OpenAIEmbeddings), and a vector store (Chroma), and then use it to select the most similar example to a given input question.",
    "chunk_length": 2918
  },
  {
    "chunk_id": 93,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/how_to/few_shot_examples/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_chroma import Chroma\nfrom langchain_core.example_selectors import SemanticSimilarityExampleSelector\nfrom langchain_openai import OpenAIEmbeddings\n\n# Assuming 'examples' is defined elsewhere\n# examples = [...] \n\nexample_selector = SemanticSimilarityExampleSelector.from_examples(\n    # This is the list of examples available to select from. examples,\n    # This is the embedding class used to produce embeddings which are used to measure semantic similarity. OpenAIEmbeddings(),\n    # This is the VectorStore class that is used to store the embeddings and do a similarity search over. Chroma,\n    # This is the number of examples to produce. k=1,\n)\n\n# Select the most similar example to the input. question = \"Who was the father of Mary Ball Washington?\"\nselected_examples = example_selector.select_examples({\"question\": question})\n\nprint(f\"Examples most similar to the input: {question}\")\nfor example in selected_examples:\n    print(\"\\n\")\n    for k, v in example.items():\n        print(f\"{k}: {v}\")\n```\n\n----------------------------------------\n\nTITLE: LangChain Tutorials Overview\nDESCRIPTION: A collection of tutorials demonstrating how to build various LLM-powered applications with LangChain. SOURCE: https://python.langchain.com/docs/how_to/document_loader_web/\n\nLANGUAGE: APIDOC\nCODE:\n```\nTutorials:\n  - Build a Question Answering application over a Graph Database\n  - Build a simple LLM application with chat models and prompt templates\n  - Build a Chatbot\n  - Build a Retrieval Augmented Generation (RAG) App: Part 2\n  - Build an Extraction Chain\n  - Build an Agent\n  - Tagging\n  - Build a Retrieval Augmented Generation (RAG) App: Part 1\n  - Build a semantic search engine\n  - Build a Question/Answering system over SQL data\n  - Summarize Text\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides Overview\nDESCRIPTION: This section outlines various how-to guides for implementing specific features and functionalities within LangChain.",
    "chunk_length": 2080
  },
  {
    "chunk_id": 94,
    "source": "python_langchain_llms_data",
    "content": "It covers essential aspects for building robust LLM applications. SOURCE: https://python.langchain.com/docs/how_to/configure/\n\nLANGUAGE: markdown\nCODE:\n```\n*   [How to use tools in a chain](/docs/how_to/tools_chain/)\n*   [How to use a vectorstore as a retriever](/docs/how_to/vectorstore_retriever/)\n*   [How to add memory to chatbots](/docs/how_to/chatbots_memory/)\n*   [How to use example selectors](/docs/how_to/example_selectors/)\n*   [How to add a semantic layer over graph database](/docs/how_to/graph_semantic/)\n*   [How to invoke runnables in parallel](/docs/how_to/parallel/)\n*   [How to stream chat model responses](/docs/how_to/chat_streaming/)\n*   [How to add default invocation args to a Runnable](/docs/how_to/binding/)\n*   [How to add retrieval to chatbots](/docs/how_to/chatbots_retrieval/)\n*   [How to use few shot examples in chat models](/docs/how_to/few_shot_examples_chat/)\n*   [How to do tool/function calling](/docs/how_to/function_calling/)\n*   [How to install LangChain packages](/docs/how_to/installation/)\n*   [How to add examples to the prompt for query analysis](/docs/how_to/query_few_shot/)\n*   [How to use few shot examples](/docs/how_to/few_shot_examples/)\n*   [How to run custom functions](/docs/how_to/functions/)\n*   [How to use output parsers to parse an LLM response into structured format](/docs/how_to/output_parser_structured/)\n*   [How to handle cases where no queries are generated](/docs/how_to/query_no_queries/)\n*   [How to route between sub-chains](/docs/how_to/routing/)\n*   [How to return structured data from a model](/docs/how_to/structured_output/)\n```\n\n----------------------------------------\n\nTITLE: Toolkit Usage Example\nDESCRIPTION: Illustrates the general pattern for using LangChain toolkits by initializing a toolkit and retrieving its list of available tools. SOURCE: https://python.langchain.com/docs/how_to/tools_builtin/\n\nLANGUAGE: python\nCODE:\n```\n# Initialize a toolkit\ntoolkit = ExampleTookit(...)\n# Get list of tools\ntools = toolkit.get_tools()\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools.",
    "chunk_length": 2221
  },
  {
    "chunk_id": 95,
    "source": "python_langchain_llms_data",
    "content": "This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/how_to/qa_sources/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: Practical guides on implementing specific features and functionalities within LangChain. SOURCE: https://python.langchain.com/docs/versions/migrating_chains/\n\nLANGUAGE: APIDOC\nCODE:\n```\nHow-to guides:\n  - How to use tools in a chain: /docs/how_to/tools_chain/\n  - How to use a vectorstore as a retriever: /docs/how_to/vectorstore_retriever/\n  - How to add memory to chatbots: /docs/how_to/chatbots_memory/\n  - How to use example selectors: /docs/how_to/example_selectors/\n  - How to add a semantic layer over graph database: /docs/how_to/graph_semantic/\n  - How to invoke runnables in parallel: /docs/how_to/parallel/\n  - How to stream chat model responses: /docs/how_to/chat_streaming/\n  - How to add default invocation args to a Runnable: /docs/how_to/binding/\n  - How to add retrieval to chatbots: /docs/how_to/chatbots_retrieval/\n  - How to use few shot examples in chat models: /docs/how_to/few_shot_examples_chat/\n  - How to do tool/function calling: /docs/how_to/function_calling/\n  - How to install LangChain packages: /docs/how_to/installation/\n  - How to add examples to the prompt for query analysis: /docs/how_to/query_few_shot/\n  - How to use few shot examples: /docs/how_to/few_shot_examples/\n  - How to run custom functions: /docs/how_to/functions/\n  - How to use output parsers to parse an LLM response into structured format: /docs/how_to/output_parser_structured/\n  - How to handle cases where no queries are generated: /docs/how_to/query_no_queries/\n  - How to route between sub-chains: /docs/how_to/routing/\n  - How to return structured data from a model: /docs/how_to/structured_output/\n  - How to summarize text through parallelization: /docs/how_to/summarize_map_reduce/\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: Practical guides on implementing specific features and functionalities within LangChain.",
    "chunk_length": 3492
  },
  {
    "chunk_id": 96,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/how_to/multimodal_inputs/\n\nLANGUAGE: APIDOC\nCODE:\n```\nHow-to Guides:\n  - How to use tools in a chain\n    Description: Explains the integration and usage of tools within LangChain chains. Path: /docs/how_to/tools_chain/\n  - How to use a vectorstore as a retriever\n    Description: Details on configuring and using vectorstores for retrieval. Path: /docs/how_to/vectorstore_retriever/\n  - How to add memory to chatbots\n    Description: Covers methods for incorporating memory into chatbot applications. Path: /docs/how_to/chatbots_memory/\n  - How to use example selectors\n    Description: Guide on utilizing example selectors for prompt engineering. Path: /docs/how_to/example_selectors/\n  - How to add a semantic layer over graph database\n    Description: Steps for building semantic layers on top of graph databases. Path: /docs/how_to/graph_semantic/\n  - How to invoke runnables in parallel\n    Description: Demonstrates parallel execution of LangChain runnables. Path: /docs/how_to/parallel/\n  - How to stream chat model responses\n    Description: Explains how to stream responses from chat models. Path: /docs/how_to/chat_streaming/\n  - How to add default invocation args to a Runnable\n    Description: Shows how to set default arguments for Runnable invocations. Path: /docs/how_to/binding/\n  - How to add retrieval to chatbots\n    Description: Guide on integrating retrieval mechanisms into chatbots. Path: /docs/how_to/chatbots_retrieval/\n  - How to use few shot examples in chat models\n    Description: Demonstrates using few-shot examples with chat models. Path: /docs/how_to/few_shot_examples_chat/\n  - How to do tool/function calling\n    Description: Covers the implementation of tool and function calling capabilities. Path: /docs/how_to/function_calling/\n  - How to install LangChain packages\n    Description: Instructions for installing necessary LangChain packages. Path: /docs/how_to/installation/\n  - How to add examples to the prompt for query analysis\n    Description: Techniques for adding examples to prompts for better query analysis.",
    "chunk_length": 2097
  },
  {
    "chunk_id": 97,
    "source": "python_langchain_llms_data",
    "content": "Path: /docs/how_to/query_few_shot/\n  - How to use few shot examples\n    Description: General guide on using few-shot examples in LangChain. Path: /docs/how_to/few_shot_examples/\n  - How to run custom functions\n    Description: Explains how to execute custom functions within LangChain. Path: /docs/how_to/functions/\n  - How to use output parsers to parse an LLM response into structured format\n    Description: Details on using output parsers to structure LLM responses. Path: /docs/how_to/output_parser_structured/\n  - How to handle cases where no queries are generated\n    Description: Strategies for managing scenarios with no generated queries. Path: /docs/how_to/query_no_queries/\n  - How to route between sub-chains\n    Description: Guide on implementing routing logic between different sub-chains. Path: /docs/how_to/routing/\n  - How to return structured data from a model\n    Description: Techniques for obtaining structured output from language models. Path: /docs/how_to/structured_output/\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools. This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/how_to/tool_choice/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: How to use few shot examples in chat models\nDESCRIPTION: This how-to guide explains how to provide few-shot examples to chat models in LangChain.",
    "chunk_length": 2937
  },
  {
    "chunk_id": 98,
    "source": "python_langchain_llms_data",
    "content": "This technique helps improve the model's performance on specific tasks. SOURCE: https://python.langchain.com/docs/tutorials/sql_qa/\n\nLANGUAGE: python\nCODE:\n```\nprint(\"How-to: How to use few shot examples in chat models\")\n# Further implementation details would follow here. ```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides Overview\nDESCRIPTION: This section provides practical guides on how to implement specific features and functionalities within LangChain. It covers topics like using tools, vectorstores, memory, parallel execution, and more. SOURCE: https://python.langchain.com/docs/how_to/chat_model_caching/\n\nLANGUAGE: python\nCODE:\n```\nHow to use tools in a chain\nHow to use a vectorstore as a retriever\nHow to add memory to chatbots\nHow to use example selectors\nHow to add a semantic layer over graph database\nHow to invoke runnables in parallel\nHow to stream chat model responses\nHow to add default invocation args to a Runnable\nHow to add retrieval to chatbots\nHow to use few shot examples in chat models\nHow to do tool/function calling\nHow to install LangChain packages\nHow to add examples to the prompt for query analysis\nHow to use few shot examples\nHow to run custom functions\nHow to use output parsers to parse an LLM response into structured format\nHow to handle cases where no queries are generated\nHow to route between sub-chains\nHow to return structured data from a model\n```\n\n----------------------------------------\n\nTITLE: Setup for Tool-Calling Models\nDESCRIPTION: Provides guidance on setting up local LLMs, recommending a model fine-tuned for tool-calling, such as Hermes-2-Pro-Llama-3-8B-GGUF from NousResearch. It also links to further guides on running LLMs locally and using them with RAG. SOURCE: https://python.langchain.com/docs/integrations/chat/llamacpp/\n\nLANGUAGE: python\nCODE:\n```\nSetup Recommendation:\nUse a model fine-tuned for tool-calling. Recommended Model: Hermes-2-Pro-Llama-3-8B-GGUF (NousResearch)\n\nFurther Guides:\n- Run LLMs locally: https://python.langchain.com/v0.1/docs/guides/development/local_llms/\n- Using local models with RAG: https://python.langchain.com/v0.1/docs/use_cases/question_answering/local_retrieval_qa/\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: Practical guides on implementing specific features and functionalities within LangChain applications, such as using tools, memory, and output parsers.",
    "chunk_length": 2439
  },
  {
    "chunk_id": 99,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/how_to/qa_chat_history_how_to/\n\nLANGUAGE: APIDOC\nCODE:\n```\nHow to use tools in a chain:\n  /docs/how_to/tools_chain/\nHow to use a vectorstore as a retriever:\n  /docs/how_to/vectorstore_retriever/\nHow to add memory to chatbots:\n  /docs/how_to/chatbots_memory/\nHow to use example selectors:\n  /docs/how_to/example_selectors/\nHow to add a semantic layer over graph database:\n  /docs/how_to/graph_semantic/\nHow to invoke runnables in parallel:\n  /docs/how_to/parallel/\nHow to stream chat model responses:\n  /docs/how_to/chat_streaming/\nHow to add default invocation args to a Runnable:\n  /docs/how_to/binding/\nHow to add retrieval to chatbots:\n  /docs/how_to/chatbots_retrieval/\nHow to use few shot examples in chat models:\n  /docs/how_to/few_shot_examples_chat/\nHow to do tool/function calling:\n  /docs/how_to/function_calling/\nHow to install LangChain packages:\n  /docs/how_to/installation/\nHow to add examples to the prompt for query analysis:\n  /docs/how_to/query_few_shot/\nHow to use few shot examples:\n  /docs/how_to/few_shot_examples/\nHow to run custom functions:\n  /docs/how_to/functions/\nHow to use output parsers to parse an LLM response into structured format:\n  /docs/how_to/output_parser_structured/\nHow to handle cases where no queries are generated:\n  /docs/how_to/query_no_queries/\nHow to route between sub-chains:\n  /docs/how_to/routing/\nHow to return structured data from a model:\n  /docs/how_to/structured_output/\nHow to summarize text through parallelization:\n  /docs/how_to/summarize_map_reduce/\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: Practical guides on implementing specific features and functionalities within LangChain, such as adding memory, using tools, and handling streaming responses. SOURCE: https://python.langchain.com/docs/how_to/document_loader_html/\n\nLANGUAGE: APIDOC\nCODE:\n```\nHow-to guides:\n  /docs/how_to/\n\n  - How to use tools in a chain: /docs/how_to/tools_chain/\n  - How to use a vectorstore as a retriever: /docs/how_to/vectorstore_retriever/\n  - How to add memory to chatbots: /docs/how_to/chatbots_memory/\n  - How to use example selectors: /docs/how_to/example_selectors/\n  - How to add a semantic layer over graph database: /docs/how_to/graph_semantic/\n  - How to invoke runnables in parallel: /docs/how_to/parallel/\n  - How to stream chat model responses: /docs/how_to/chat_streaming/\n  - How to add default invocation args to a Runnable: /docs/how_to/binding/\n  - How to add retrieval to chatbots: /docs/how_to/chatbots_retrieval/\n  - How to use few shot examples in chat models: /docs/how_to/few_shot_examples_chat/\n  - How to do tool/function calling: /docs/how_to/function_calling/\n  - How to install LangChain packages: /docs/how_to/installation/\n  - How to add examples to the prompt for query analysis: /docs/how_to/query_few_shot/\n  - How to use few shot examples: /docs/how_to/few_shot_examples/\n  - How to run custom functions: /docs/how_to/functions/\n  - How to use output parsers to parse an LLM response into structured format: /docs/how_to/output_parser_structured/\n  - How to handle cases where no queries are generated: /docs/how_to/query_no_queries/\n  - How to route between sub-chains: /docs/how_to/routing/\n  - How to return structured data from a model: /docs/how_to/structured_output/\n  - How to summarize text through parallelization: /docs/how_to/summarize_map_reduce/\n```\n\n----------------------------------------\n\nTITLE: Set up threads\nDESCRIPTION: Instructions on how to properly set up and trace operations involving threads within an application using LangSmith.",
    "chunk_length": 3626
  },
  {
    "chunk_id": 100,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://docs.smith.langchain.com/how_to_guides/tracing/\n\nLANGUAGE: APIDOC\nCODE:\n```\n/observability/how_to_guides/threads\n```\n\n----------------------------------------\n\nTITLE: LangChain Tutorials Overview\nDESCRIPTION: Provides a list of tutorials for building various LLM applications with LangChain, covering topics from basic LLM applications to advanced agents and RAG systems. SOURCE: https://python.langchain.com/docs/concepts/callbacks/\n\nLANGUAGE: python\nCODE:\n```\nBuild a Question Answering application over a Graph Database\nBuild a simple LLM application with chat models and prompt templates\nBuild a Chatbot\nBuild a Retrieval Augmented Generation (RAG) App: Part 2\nBuild an Extraction Chain\nBuild an Agent\nTagging\nBuild a Retrieval Augmented Generation (RAG) App: Part 1\nBuild a semantic search engine\nBuild a Question/Answering system over SQL data\nSummarize Text\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: This section provides practical guides on implementing specific features and functionalities within LangChain, such as using tools, memory, vectorstores, and parallel execution. SOURCE: https://python.langchain.com/docs/how_to/hybrid/\n\nLANGUAGE: python\nCODE:\n```\nHow to use tools in a chain\nHow to use a vectorstore as a retriever\nHow to add memory to chatbots\nHow to use example selectors\nHow to add a semantic layer over graph database\nHow to invoke runnables in parallel\nHow to stream chat model responses\nHow to add default invocation args to a Runnable\nHow to add retrieval to chatbots\nHow to use few shot examples in chat models\nHow to do tool/function calling\nHow to install LangChain packages\nHow to add examples to the prompt for query analysis\nHow to use few shot examples\nHow to run custom functions\nHow to use output parsers to parse an LLM response into structured format\nHow to handle cases where no queries are generated\nHow to route between sub-chains\nHow to return structured data from a model\nHow to summarize text through parallelization\n```\n\n----------------------------------------\n\nTITLE: How to use few shot examples in chat models\nDESCRIPTION: Demonstrates how to incorporate few-shot examples when interacting with chat models.",
    "chunk_length": 2224
  },
  {
    "chunk_id": 101,
    "source": "python_langchain_llms_data",
    "content": "This is useful for guiding the model's responses and improving accuracy for specific tasks. SOURCE: https://python.langchain.com/docs/how_to/few_shot_examples/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n\n# Example messages for few-shot learning\nexample_messages = [\n    {'role': 'user', 'content': 'What is the capital of France?'},\n    {'role': 'assistant', 'content': 'Paris'},\n    {'role': 'user', 'content': 'What is the capital of Germany?'},\n    {'role': 'assistant', 'content': 'Berlin'}\n]\n\n# Create a FewShotChatMessagePromptTemplate\nexample_prompt = FewShotChatMessagePromptTemplate.from_examples(\n    examples=example_messages,\n    input_variables=[\"input\"],\n    example_separator=\"\\n---\",\n    suffix=\"User: {input}\\nAssistant:\",\n    input_type=\"human\"\n)\n\n# Create a ChatPromptTemplate with the few-shot examples\nchat_prompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant.\"),\n    example_prompt\n])\n\n# You can then format this prompt with new user input\nformatted_prompt = chat_prompt.format_messages(\n    input=\"What is the capital of Spain?\"\n)\n\n# The formatted_prompt can be passed to a chat model\n# print(formatted_prompt)\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools. This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/how_to/sql_csv/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant.",
    "chunk_length": 2284
  },
  {
    "chunk_id": 102,
    "source": "python_langchain_llms_data",
    "content": "Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides Overview\nDESCRIPTION: This section provides practical guides on implementing specific features and functionalities within LangChain. It covers topics such as using tools, memory, vectorstores, parallel execution, streaming, and more. SOURCE: https://python.langchain.com/docs/concepts/why_langchain/\n\nLANGUAGE: python\nCODE:\n```\nHow to use tools in a chain\nHow to use a vectorstore as a retriever\nHow to add memory to chatbots\nHow to use example selectors\nHow to add a semantic layer over graph database\nHow to invoke runnables in parallel\nHow to stream chat model responses\nHow to add default invocation args to a Runnable\nHow to add retrieval to chatbots\nHow to use few shot examples in chat models\nHow to do tool/function calling\nHow to install LangChain packages\nHow to add examples to the prompt for query analysis\nHow to use few shot examples\nHow to run custom functions\nHow to use output parsers to parse an LLM response into structured format\nHow to handle cases where no queries are generated\nHow to route between sub-chains\nHow to return structured data from a model\nHow to summarize text through parallelization\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools.",
    "chunk_length": 2099
  },
  {
    "chunk_id": 103,
    "source": "python_langchain_llms_data",
    "content": "This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/how_to/serialization/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools. This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/how_to/sql_query_checking/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant.",
    "chunk_length": 2584
  },
  {
    "chunk_id": 104,
    "source": "python_langchain_llms_data",
    "content": "Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: How to use few shot examples in chat models\nDESCRIPTION: This guide explains how to provide few-shot examples directly to chat models within LangChain, helping them understand the desired output format and behavior. SOURCE: https://python.langchain.com/docs/how_to/extraction_parse/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate, SystemMessagePromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage\nfrom langchain_openai import ChatOpenAI\n\n# Define few-shot examples as message lists\nexample_1 = [\n    HumanMessage(content=\"Translate 'I am a student' to French.\"),\n    AIMessage(content=\"Je suis un tudiant.\")\n]\n\nexample_2 = [\n    HumanMessage(content=\"Translate 'Hello world' to Spanish.\"),\n    AIMessage(content=\"Hola mundo.\")\n]\n\n# Define the system prompt\nsystem_prompt = SystemMessagePromptTemplate.from_template(\"You are a helpful translation assistant.\")\n\n# Combine system prompt, examples, and the final human message\nchat_prompt = ChatPromptTemplate.from_messages([\n    system_prompt,\n    *example_1,  # Unpack the first example\n    *example_2,  # Unpack the second example\n    HumanMessagePromptTemplate.from_template(\"Translate '{text}' to French.\")\n])\n\n# Initialize the chat model\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n\n# Create the chain\nchain = chat_prompt | llm\n\n# Example usage\nresponse = chain.invoke({\"text\": \"Thank you very much\"})\nprint(response.content)\n```\n\n----------------------------------------\n\nTITLE: Setup and Data Loading with Pandas\nDESCRIPTION: Installs necessary LangChain and Pandas libraries, sets up environment variables for LangSmith (optional), downloads the Titanic dataset, and loads it into a Pandas DataFrame, displaying its shape and columns.",
    "chunk_length": 2541
  },
  {
    "chunk_id": 105,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/how_to/sql_csv/\n\nLANGUAGE: python\nCODE:\n```\n%pip install -qU langchain langchain-openai langchain-community langchain-experimental pandas\n\n# Using LangSmith is recommended but not required. Uncomment below lines to use. # import os\n# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n\n!wget https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv -O titanic.csv\n\nimport pandas as pd\ndf = pd.read_csv(\"titanic.csv\")\nprint(df.shape)\nprint(df.columns.tolist())\n```\n\n----------------------------------------\n\nTITLE: Set up threads\nDESCRIPTION: Instructions on how to properly set up and trace operations involving threads within an application using LangSmith. SOURCE: https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain/\n\nLANGUAGE: APIDOC\nCODE:\n```\n/observability/how_to_guides/threads\n```\n\n----------------------------------------\n\nTITLE: LangChain Tutorials Overview\nDESCRIPTION: Lists various tutorials available for LangChain, covering a wide range of use cases from basic LLM applications to complex agents and retrieval-augmented generation (RAG) systems. SOURCE: https://python.langchain.com/docs/concepts/few_shot_prompting/\n\nLANGUAGE: APIDOC\nCODE:\n```\nTutorials:\n  - Build a Question Answering application over a Graph Database\n  - Build a simple LLM application with chat models and prompt templates\n  - Build a Chatbot\n  - Build a Retrieval Augmented Generation (RAG) App: Part 2\n  - Build an Extraction Chain\n  - Build an Agent\n  - Tagging\n  - Build a Retrieval Augmented Generation (RAG) App: Part 1\n  - Build a semantic search engine\n  - Build a Question/Answering system over SQL data\n  - Summarize Text\n\nThese tutorials provide practical examples and step-by-step guidance on implementing various LLM-powered features using LangChain. ```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: Practical guides on implementing specific features and functionalities within LangChain, such as using tools, memory, vectorstores, and parallel execution.",
    "chunk_length": 2126
  },
  {
    "chunk_id": 106,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/tutorials/chatbot/\n\nLANGUAGE: APIDOC\nCODE:\n```\nHow-to guides:\n  - How to use tools in a chain\n  - How to use a vectorstore as a retriever\n  - How to add memory to chatbots\n  - How to use example selectors\n  - How to add a semantic layer over graph database\n  - How to invoke runnables in parallel\n  - How to stream chat model responses\n  - How to add default invocation args to a Runnable\n  - How to add retrieval to chatbots\n  - How to use few shot examples in chat models\n  - How to do tool/function calling\n  - How to install LangChain packages\n  - How to add examples to the prompt for query analysis\n  - How to use few shot examples\n  - How to run custom functions\n  - How to use output parsers to parse an LLM response into structured format\n  - How to handle cases where no queries are generated\n  - How to route between sub-chains\n  - How to return structured data from a model\n  - How to summarize text through parallelization\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: Practical guides on implementing specific features and functionalities within LangChain, such as using tools, memory, output parsers, and parallel execution. SOURCE: https://python.langchain.com/docs/concepts/text_llms/\n\nLANGUAGE: python\nCODE:\n```\nHow-to guides:\n  How to use tools in a chain: /docs/how_to/tools_chain/\n  How to use a vectorstore as a retriever: /docs/how_to/vectorstore_retriever/\n  How to add memory to chatbots: /docs/how_to/chatbots_memory/\n  How to use example selectors: /docs/how_to/example_selectors/\n  How to add a semantic layer over graph database: /docs/how_to/graph_semantic/\n  How to invoke runnables in parallel: /docs/how_to/parallel/\n  How to stream chat model responses: /docs/how_to/chat_streaming/\n  How to add default invocation args to a Runnable: /docs/how_to/binding/\n  How to add retrieval to chatbots: /docs/how_to/chatbots_retrieval/\n  How to use few shot examples in chat models: /docs/how_to/few_shot_examples_chat/\n  How to do tool/function calling: /docs/how_to/function_calling/\n  How to install LangChain packages: /docs/how_to/installation/\n  How to add examples to the prompt for query analysis: /docs/how_to/query_few_shot/\n  How to use few shot examples: /docs/how_to/few_shot_examples/\n  How to run custom functions: /docs/how_to/functions/\n  How to use output parsers to parse an LLM response into structured format: /docs/how_to/output_parser_structured/\n  How to handle cases where no queries are generated: /docs/how_to/query_no_queries/\n  How to route between sub-chains: /docs/how_to/routing/\n  How to return structured data from a model: /docs/how_to/structured_output/\n  How to summarize text through parallelization: /docs/how_to/summarize_map_reduce/\n```\n\n----------------------------------------\n\nTITLE: LangChain Tutorials\nDESCRIPTION: A collection of tutorials demonstrating how to build various LLM applications with LangChain, including QA, chatbots, RAG, and agents.",
    "chunk_length": 3006
  },
  {
    "chunk_id": 107,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/how_to/example_selectors_similarity/\n\nLANGUAGE: APIDOC\nCODE:\n```\nTutorials:\n  - Build a Question Answering application over a Graph Database: /docs/tutorials/graph/\n  - Build a simple LLM application with chat models and prompt templates: /docs/tutorials/llm_chain/\n  - Build a Chatbot: /docs/tutorials/chatbot/\n  - Build a Retrieval Augmented Generation (RAG) App: Part 2: /docs/tutorials/qa_chat_history/\n  - Build an Extraction Chain: /docs/tutorials/extraction/\n  - Build an Agent: /docs/tutorials/agents/\n  - Tagging: /docs/tutorials/classification/\n  - Build a Retrieval Augmented Generation (RAG) App: Part 1: /docs/tutorials/rag/\n  - Build a semantic search engine: /docs/tutorials/retrievers/\n  - Build a Question/Answering system over SQL data: /docs/tutorials/sql_qa/\n  - Summarize Text: /docs/tutorials/summarization/\n```\n\n----------------------------------------\n\nTITLE: LangChain Tool Calling - Few-Shot Prompting\nDESCRIPTION: Demonstrates how to use few-shot examples within the prompt to guide the LLM on how and when to use tools effectively. This improves the accuracy and reliability of tool calls. SOURCE: https://python.langchain.com/docs/how_to/convert_runnable_to_tool/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import AgentExecutor, create_tool_calling_agent\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\n@tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather for a city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n@tool\ndef get_time(city: str) -> str:\n    \"\"\"Get the current time for a city.\"\"\"\n    return f\"The time in {city} is 10:00 AM.\"\n\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\ntools = [get_weather, get_time]\n\n# Few-shot examples embedded in the prompt\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    (\"human\", \"What is the weather in Paris?\"),\n    (\"assistant\", \"{\\\"tool\\\": \\\"get_weather\\\", \\\"tool_input\\\": {\\\"city\\\": \\\"Paris\\\"}}\"),\n    (\"human\", \"What time is it in Tokyo?\"),\n    (\"assistant\", \"{\\\"tool\\\": \\\"get_time\\\", \\\"tool_input\\\": {\\\"city\n```\n\n----------------------------------------\n\nTITLE: LangChain Tool Calling - Few-Shot Prompting\nDESCRIPTION: Demonstrates how to use few-shot examples within the prompt to guide the LLM on how and when to use tools effectively.",
    "chunk_length": 2457
  },
  {
    "chunk_id": 108,
    "source": "python_langchain_llms_data",
    "content": "This improves the accuracy and reliability of tool calls. SOURCE: https://python.langchain.com/docs/tutorials/sql_qa/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import AgentExecutor, create_tool_calling_agent\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\n@tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather for a city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n@tool\ndef get_time(city: str) -> str:\n    \"\"\"Get the current time for a city.\"\"\"\n    return f\"The time in {city} is 10:00 AM.\"\n\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\ntools = [get_weather, get_time]\n\n# Few-shot examples embedded in the prompt\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    (\"human\", \"What is the weather in Paris?\"),\n    (\"assistant\", \"{\\\"tool\\\": \\\"get_weather\\\", \\\"tool_input\\\": {\\\"city\\\": \\\"Paris\\\"}}\"),\n    (\"human\", \"What time is it in Tokyo?\"),\n    (\"assistant\", \"{\\\"tool\\\": \\\"get_time\\\", \\\"tool_input\\\": {\\\"city\n```\n\n----------------------------------------\n\nTITLE: How to use few shot examples in chat models\nDESCRIPTION: This guide explains how to incorporate few-shot examples into chat models within LangChain. It covers techniques for providing context and examples to improve model performance and response quality. This is crucial for tasks requiring specific output formats or nuanced understanding. SOURCE: https://python.langchain.com/docs/how_to/few_shot_examples_chat/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n\n# Example messages\nexample_messages = [\n    (\"human\", \"What is the capital of France?\"),\n    (\"ai\", \"The capital of France is Paris.\")\n]\n\n# Create a FewShotChatMessagePromptTemplate\nexample_prompt = FewShotChatMessagePromptTemplate.from_messages(example_messages)\n\n# Create a ChatPromptTemplate with the few-shot examples\nchat_prompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant.\"),\n    example_prompt,\n    (\"human\", \"{input}\")\n])\n\n# Example usage with a chat model (assuming 'chat_model' is initialized)\n# formatted_prompt = chat_prompt.format_messages(input=\"What is the capital of Germany?\")\n# response = chat_model.invoke(formatted_prompt)\n# print(response.content)\n```\n\n----------------------------------------\n\nTITLE: Extraction Techniques\nDESCRIPTION: Guides on using reference examples and handling long text for extraction, as well as performing extraction using only prompting.",
    "chunk_length": 2620
  },
  {
    "chunk_id": 109,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/how_to/callbacks_runtime/\n\nLANGUAGE: python\nCODE:\n```\n# How to use reference examples when doing extraction\n# /docs/how_to/extraction_examples/\n\n# How to handle long text when doing extraction\n# /docs/how_to/extraction_long_text/\n\n# How to use prompting alone (no tool calling) to do extraction\n# /docs/how_to/extraction_parse/\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: This section provides practical guides on implementing specific features and functionalities within LangChain, such as using tools, memory, vectorstores, and handling parallel execution. SOURCE: https://python.langchain.com/docs/how_to/debugging/\n\nLANGUAGE: python\nCODE:\n```\nHow to use tools in a chain\nHow to use a vectorstore as a retriever\nHow to add memory to chatbots\nHow to use example selectors\nHow to add a semantic layer over graph database\nHow to invoke runnables in parallel\nHow to stream chat model responses\nHow to add default invocation args to a Runnable\nHow to add retrieval to chatbots\nHow to use few shot examples in chat models\nHow to do tool/function calling\nHow to install LangChain packages\nHow to add examples to the prompt for query analysis\nHow to use few shot examples\nHow to run custom functions\nHow to use output parsers to parse an LLM response into structured format\nHow to handle cases where no queries are generated\nHow to route between sub-chains\nHow to return structured data from a model\nHow to summarize text through parallelization\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: Practical guides on implementing specific features and functionalities within LangChain. These cover topics such as using tools, managing memory, integrating vectorstores, and handling streaming responses. SOURCE: https://python.langchain.com/docs/how_to/callbacks_custom_events/\n\nLANGUAGE: APIDOC\nCODE:\n```\nHow-to guides:\n  - How to use tools in a chain: /docs/how_to/tools_chain/\n  - How to use a vectorstore as a retriever: /docs/how_to/vectorstore_retriever/\n  - How to add memory to chatbots: /docs/how_to/chatbots_memory/\n  - How to use example selectors: /docs/how_to/example_selectors/\n  - How to add a semantic layer over graph database: /docs/how_to/graph_semantic/\n  - How to invoke runnables in parallel: /docs/how_to/parallel/\n  - How to stream chat model responses: /docs/how_to/chat_streaming/\n  - How to add default invocation args to a Runnable: /docs/how_to/binding/\n  - How to add retrieval to chatbots: /docs/how_to/chatbots_retrieval/\n  - How to use few shot examples in chat models: /docs/how_to/few_shot_examples_chat/\n  - How to do tool/function calling: /docs/how_to/function_calling/\n  - How to install LangChain packages: /docs/how_to/installation/\n  - How to add examples to the prompt for query analysis: /docs/how_to/query_few_shot/\n  - How to use few shot examples: /docs/how_to/few_shot_examples/\n  - How to run custom functions: /docs/how_to/functions/\n  - How to use output parsers to parse an LLM response into structured format: /docs/how_to/output_parser_structured/\n  - How to handle cases where no queries are generated: /docs/how_to/query_no_queries/\n  - How to route between sub-chains: /docs/how_to/routing/\n  - How to return structured data from a model: /docs/how_to/structured_output/\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools.",
    "chunk_length": 3559
  },
  {
    "chunk_id": 110,
    "source": "python_langchain_llms_data",
    "content": "This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/tutorials/rag/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides Overview\nDESCRIPTION: This section provides guides on how to implement specific features and functionalities within LangChain. It covers topics like using tools, memory, output parsers, and more. SOURCE: https://python.langchain.com/docs/how_to/llm_token_usage_tracking/\n\nLANGUAGE: python\nCODE:\n```\nHow to use tools in a chain\nHow to use a vectorstore as a retriever\nHow to add memory to chatbots\nHow to use example selectors\nHow to add a semantic layer over graph database\nHow to invoke runnables in parallel\nHow to stream chat model responses\nHow to add default invocation args to a Runnable\nHow to add retrieval to chatbots\nHow to use few shot examples in chat models\nHow to do tool/function calling\nHow to install LangChain packages\nHow to add examples to the prompt for query analysis\nHow to use few shot examples\nHow to run custom functions\nHow to use output parsers to parse an LLM response into structured format\nHow to handle cases where no queries are generated\nHow to route between sub-chains\nHow to return structured data from a model\n```\n\n----------------------------------------\n\nTITLE: Running LangChain Models Locally\nDESCRIPTION: Instructions on how to configure and run LangChain models using local LLM providers.",
    "chunk_length": 2833
  },
  {
    "chunk_id": 111,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/how_to/example_selectors/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.llms import Ollama\n\n# Ensure Ollama is installed and a model (e.g., 'llama2') is pulled\n# ollama pull llama2\n\nllm = Ollama(model=\"llama2\")\n\n# You can then use this local LLM like any other LangChain LLM\n# print(llm.invoke(\"Why is the sky blue?\"))\n\n# Other local LLM integrations include HuggingFaceHub, GPT4All, etc. ```\n\n----------------------------------------\n\nTITLE: Extraction Techniques\nDESCRIPTION: Guides on using reference examples and handling long text for extraction, as well as performing extraction using only prompting. SOURCE: https://python.langchain.com/docs/how_to/qa_citations/\n\nLANGUAGE: python\nCODE:\n```\n# How to use reference examples when doing extraction\n# /docs/how_to/extraction_examples/\n\n# How to handle long text when doing extraction\n# /docs/how_to/extraction_long_text/\n\n# How to use prompting alone (no tool calling) to do extraction\n# /docs/how_to/extraction_parse/\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools. This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/how_to/extraction_examples/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant.",
    "chunk_length": 2075
  },
  {
    "chunk_id": 112,
    "source": "python_langchain_llms_data",
    "content": "Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: Integrate Examples into Prompt\nDESCRIPTION: Demonstrates how to integrate a list of formatted example messages into a LangChain prompt using `prompt.partial`. This allows the model to condition its responses on the provided examples, improving output quality for specific tasks. SOURCE: https://python.langchain.com/docs/how_to/query_few_shot/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.prompts import MessagesPlaceholder\n\n# Assuming 'prompt' is a pre-defined PromptTemplate and 'example_msgs' is generated by tool_example_to_messages\nquery_analyzer_with_examples = (\n    {\"question\": RunnablePassthrough()}\n    | prompt.partial(examples=example_msgs)\n    | structured_llm\n)\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: Practical guides on implementing specific features and functionalities in LangChain, such as using tools, memory, output parsers, and parallel execution. SOURCE: https://python.langchain.com/docs/concepts/tool_calling/\n\nLANGUAGE: APIDOC\nCODE:\n```\nHow-to Guides:\n\n- **How to use tools in a chain**: Integrate external tools into LangChain workflows. - **How to use a vectorstore as a retriever**: Configure vector stores for retrieval. - **How to add memory to chatbots**: Implement stateful conversations. - **How to use example selectors**: Manage few-shot examples for prompts.",
    "chunk_length": 2095
  },
  {
    "chunk_id": 113,
    "source": "python_langchain_llms_data",
    "content": "- **How to add a semantic layer over graph database**: Enhance graph database querying. - **How to invoke runnables in parallel**: Execute multiple components concurrently. - **How to stream chat model responses**: Enable real-time streaming of LLM outputs. - **How to add default invocation args to a Runnable**: Set default arguments for runnables. - **How to add retrieval to chatbots**: Integrate retrieval mechanisms into chatbots. - **How to use few shot examples in chat models**: Improve LLM performance with examples. - **How to do tool/function calling**: Implement LLM function calling. - **How to install LangChain packages**: Guide to installing LangChain components. - **How to add examples to the prompt for query analysis**: Enhance query understanding with examples. - **How to use few shot examples**: General guide to few-shot learning. - **How to run custom functions**: Execute user-defined functions. - **How to use output parsers to parse an LLM response into structured format**: Convert LLM output to structured data. - **How to handle cases where no queries are generated**: Manage scenarios with no query output. - **How to route between sub-chains**: Implement conditional logic in chains. - **How to return structured data from a model**: Ensure models output structured formats. - **How to summarize text through parallelization**: Efficient text summarization using parallel processing. ```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: Guides on specific functionalities within LangChain, such as using tools, memory, output parsers, and parallel execution. SOURCE: https://python.langchain.com/docs/how_to/output_parser_yaml/\n\nLANGUAGE: python\nCODE:\n```\nHow to use tools in a chain: /docs/how_to/tools_chain/\nHow to use a vectorstore as a retriever: /docs/how_to/vectorstore_retriever/\nHow to add memory to chatbots: /docs/how_to/chatbots_memory/\nHow to use example selectors: /docs/how_to/example_selectors/\nHow to add a semantic layer over graph database: /docs/how_to/graph_semantic/\nHow to invoke runnables in parallel: /docs/how_to/parallel/\nHow to stream chat model responses: /docs/how_to/chat_streaming/\nHow to add default invocation args to a Runnable: /docs/how_to/binding/\nHow to add retrieval to chatbots: /docs/how_to/chatbots_retrieval/\nHow to use few shot examples in chat models: /docs/how_to/few_shot_examples_chat/\nHow to do tool/function calling: /docs/how_to/function_calling/\nHow to add examples to the prompt for query analysis: /docs/how_to/query_few_shot/\nHow to use few shot examples: /docs/how_to/few_shot_examples/\nHow to run custom functions: /docs/how_to/functions/\nHow to use output parsers to parse an LLM response into structured format: /docs/how_to/output_parser_structured/\nHow to handle cases where no queries are generated: /docs/how_to/query_no_queries/\nHow to route between sub-chains: /docs/how_to/routing/\nHow to return structured data from a model: /docs/how_to/structured_output/\nHow to summarize text through parallelization: /docs/how_to/summarize_map_reduce/\n```\n\n----------------------------------------\n\nTITLE: Upstage Chat Integration\nDESCRIPTION: Covers getting started with Upstage chat models.",
    "chunk_length": 3219
  },
  {
    "chunk_id": 114,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/integrations/chat/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_upstage import ChatUpstage\n\n# Example usage (requires Upstage API key)\n# llm = ChatUpstage(api_key=\"YOUR_UPSTAGE_API_KEY\")\n# response = llm.invoke(\"What is the capital of Japan?\")\n# print(response)\n```\n\n----------------------------------------\n\nTITLE: Extraction Techniques\nDESCRIPTION: Guides on using reference examples and handling long text for extraction, as well as performing extraction using only prompting. SOURCE: https://python.langchain.com/docs/how_to/tool_configure/\n\nLANGUAGE: python\nCODE:\n```\n# How to use reference examples when doing extraction\n# /docs/how_to/extraction_examples/\n\n# How to handle long text when doing extraction\n# /docs/how_to/extraction_long_text/\n\n# How to use prompting alone (no tool calling) to do extraction\n# /docs/how_to/extraction_parse/\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools. This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/how_to/graph_constructing/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: Trace without setting environment variables\nDESCRIPTION: A guide on how to enable LangSmith tracing without explicitly setting environment variables, potentially simplifying setup in certain deployment scenarios.",
    "chunk_length": 2898
  },
  {
    "chunk_id": 115,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://docs.smith.langchain.com/how_to_guides/tracing/\n\nLANGUAGE: APIDOC\nCODE:\n```\n/observability/how_to_guides/trace_without_env_vars\n```\n\n----------------------------------------\n\nTITLE: Extraction Techniques\nDESCRIPTION: Guides on using reference examples and handling long text for extraction, as well as performing extraction using only prompting. SOURCE: https://python.langchain.com/docs/how_to/sql_prompting/\n\nLANGUAGE: python\nCODE:\n```\n# How to use reference examples when doing extraction\n# /docs/how_to/extraction_examples/\n\n# How to handle long text when doing extraction\n# /docs/how_to/extraction_long_text/\n\n# How to use prompting alone (no tool calling) to do extraction\n# /docs/how_to/extraction_parse/\n```\n\n----------------------------------------\n\nTITLE: LangChain Tool Calling - Few-Shot Prompting\nDESCRIPTION: Demonstrates how to use few-shot examples within the prompt to guide the LLM on how and when to use tools effectively. This improves the accuracy and reliability of tool calls. SOURCE: https://python.langchain.com/docs/how_to/query_multiple_queries/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import AgentExecutor, create_tool_calling_agent\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\n@tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather for a city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n@tool\ndef get_time(city: str) -> str:\n    \"\"\"Get the current time for a city.\"\"\"\n    return f\"The time in {city} is 10:00 AM.\"\n\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\ntools = [get_weather, get_time]\n\n# Few-shot examples embedded in the prompt\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    (\"human\", \"What is the weather in Paris?\"),\n    (\"assistant\", \"{\\\"tool\\\": \\\"get_weather\\\", \\\"tool_input\\\": {\\\"city\\\": \\\"Paris\\\"}}\"),\n    (\"human\", \"What time is it in Tokyo?\"),\n    (\"assistant\", \"{\\\"tool\\\": \\\"get_time\\\", \\\"tool_input\\\": {\\\"city\n```\n\n----------------------------------------\n\nTITLE: Redis Vector Store Integration\nDESCRIPTION: This notebook covers how to get started with the Redis vector store.",
    "chunk_length": 2248
  },
  {
    "chunk_id": 116,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/integrations/vectorstores/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.vectorstores import Redis\n\n# Example usage (assuming Redis client is configured)\n# vector_store = Redis(index_name=\"my_index\")\n# results = vector_store.similarity_search(\"query text\")\n```\n\n----------------------------------------\n\nTITLE: LangChain Tool Calling - Few-Shot Prompting\nDESCRIPTION: Demonstrates how to use few-shot examples within the prompt to guide the LLM on how and when to use tools effectively. This improves the accuracy and reliability of tool calls. SOURCE: https://python.langchain.com/docs/how_to/example_selectors_mmr/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import AgentExecutor, create_tool_calling_agent\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\n@tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather for a city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n@tool\ndef get_time(city: str) -> str:\n    \"\"\"Get the current time for a city.\"\"\"\n    return f\"The time in {city} is 10:00 AM.\"\n\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\ntools = [get_weather, get_time]\n\n# Few-shot examples embedded in the prompt\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    (\"human\", \"What is the weather in Paris?\"),\n    (\"assistant\", \"{\\\"tool\\\": \\\"get_weather\\\", \\\"tool_input\\\": {\\\"city\\\": \\\"Paris\\\"}}\"),\n    (\"human\", \"What time is it in Tokyo?\"),\n    (\"assistant\", \"{\\\"tool\\\": \\\"get_time\\\", \\\"tool_input\\\": {\\\"city\n```\n\n----------------------------------------\n\nTITLE: LangChain Tutorials Overview\nDESCRIPTION: A collection of tutorials demonstrating how to build various applications using LangChain, from simple LLM applications to complex agents and RAG systems. SOURCE: https://python.langchain.com/docs/how_to/output_parser_string/\n\nLANGUAGE: APIDOC\nCODE:\n```\nTutorials:\n  /docs/tutorials/\n\nThis section offers practical guides for building different types of applications with LangChain.",
    "chunk_length": 2137
  },
  {
    "chunk_id": 117,
    "source": "python_langchain_llms_data",
    "content": "Each tutorial focuses on a specific use case and provides step-by-step instructions and code examples. Featured Tutorials:\n- Build a Question Answering application over a Graph Database\n- Build a simple LLM application with chat models and prompt templates\n- Build a Chatbot\n- Build a Retrieval Augmented Generation (RAG) App: Part 2\n- Build an Extraction Chain\n- Build an Agent\n- Tagging\n- Build a Retrieval Augmented Generation (RAG) App: Part 1\n- Build a semantic search engine\n- Build a Question/Answering system over SQL data\n- Summarize Text\n\nThese tutorials are designed to help users quickly get started with LangChain and explore its capabilities. ```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools. This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/how_to/time_weighted_vectorstore/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: Few-Shot Prompting for Tool Behavior\nDESCRIPTION: Shows how to use few-shot examples to enhance a model's ability to call tools correctly.",
    "chunk_length": 2601
  },
  {
    "chunk_id": 118,
    "source": "python_langchain_llms_data",
    "content": "By including example queries and responses in the prompt, the model learns from demonstrations, improving its performance on tasks like math operations. SOURCE: https://context7_llms\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\nfrom langchain.tools import tool\nfrom langchain.schema import AIMessage, HumanMessage\n\n@tool\ndef add(a: int, b: int) -> int:\n    \"\"\"Adds two numbers.\"\"\"\n    return a + b\n\n@tool\ndef multiply(a: int, b: int) -> int:\n    \"\"\"Multiplies two numbers.\"\"\"\n    return a * b\n\nllm = ChatOpenAI(temperature=0)\n\n# Define few-shot examples\nexamples = [\n    HumanMessage(content=\"What is 2 + 2?\", additional_kwargs={'tool_calls': [{'name': 'add', 'arguments': {'a': 2, 'b': 2}, 'id': 'call_1'}]}),\n    AIMessage(content=\"\", additional_kwargs={'tool_calls': [{'name': 'add', 'arguments': {'a': 2, 'b': 2}, 'id': 'call_1'}]}),\n    HumanMessage(content=\"What is 3 * 4?\", additional_kwargs={'tool_calls': [{'name': 'multiply', 'arguments': {'a': 3, 'b': 4}, 'id': 'call_2'}]}),\n    AIMessage(content=\"\", additional_kwargs={'tool_calls': [{'name': 'multiply', 'arguments': {'a': 3, 'b': 4}, 'id': 'call_2'}]}),\n]\n\n# Create a prompt template with examples\n# Note: The exact format for few-shot examples in prompts can vary. # This is a conceptual representation. # A more robust approach might involve a dedicated FewShotPromptTemplate. # Example using a simple list of messages:\n# prompt_messages = [\n#     SystemMessagePromptTemplate.from_template(\n#         \"You are a helpful assistant that can use tools.\"\n#     )\n# ]\n# prompt_messages.extend(examples)\n# prompt_messages.append(HumanMessagePromptTemplate.from_template(\"{input}\"))\n\n# chat_prompt = ChatPromptTemplate.from_messages(prompt_messages)\n\n# formatted_prompt = chat_prompt.format_messages(input=\"What is 5 + 6?\")\n\n# response = llm.invoke(formatted_prompt)\n# print(response)\n\n```\n\n----------------------------------------\n\nTITLE: Oceanbase Vector Store Integration\nDESCRIPTION: This notebook covers how to get started with the Oceanbase vector store.",
    "chunk_length": 2154
  },
  {
    "chunk_id": 119,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/integrations/vectorstores/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.vectorstores import OceanbaseVectorStore\n\n# Example usage (assuming Oceanbase connection)\n# vector_store = OceanbaseVectorStore(table_name=\"my_table\")\n# results = vector_store.similarity_search(\"query text\")\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools. This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/how_to/example_selectors_length_based/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: Few-shot Prompting in Chat Models\nDESCRIPTION: Demonstrates how to use few-shot examples with chat models in LangChain.",
    "chunk_length": 2260
  },
  {
    "chunk_id": 120,
    "source": "python_langchain_llms_data",
    "content": "This guide explains how to provide example interactions to improve the model's understanding and response generation for specific tasks. SOURCE: https://python.langchain.com/docs/concepts/few_shot_prompting/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_openai import ChatOpenAI\n\n# Example of few-shot examples for a chat model\n# This is a conceptual example, actual implementation might vary based on the specific use case. # Define a prompt template that includes few-shot examples\n# The examples guide the model on how to respond to user queries. chat_prompt_template = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant.\"),\n    # Few-shot examples:\n    (\"human\", \"What is the capital of France?\"),\n    (\"ai\", \"The capital of France is Paris.\"),\n    (\"human\", \"What is the capital of Japan?\"),\n    (\"ai\", \"The capital of Japan is Tokyo.\"),\n    # The actual user query:\n    (\"human\", \"{question}\")\n])\n\n# Initialize the chat model\nllm = ChatOpenAI()\n\n# Create a chain that combines the prompt and the model\nchain = chat_prompt_template | llm | StrOutputParser()\n\n# Invoke the chain with a question\nresponse = chain.invoke({\"question\": \"What is the capital of Germany?\"})\nprint(response)\n\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: Practical guides on implementing specific functionalities within LangChain, such as using tools, managing chat history, and parsing LLM outputs. SOURCE: https://python.langchain.com/docs/how_to/migrate_agent/\n\nLANGUAGE: python\nCODE:\n```\n# How to use tools in a chain\n# ... (details omitted for brevity)\n\n# How to use a vectorstore as a retriever\n# ... (details omitted for brevity)\n\n# How to add memory to chatbots\n# ... (details omitted for brevity)\n\n# How to use example selectors\n# ... (details omitted for brevity)\n\n# How to add a semantic layer over graph database\n# ... (details omitted for brevity)\n\n# How to invoke runnables in parallel\n# ...",
    "chunk_length": 2067
  },
  {
    "chunk_id": 121,
    "source": "python_langchain_llms_data",
    "content": "(details omitted for brevity)\n\n# How to stream chat model responses\n# ... (details omitted for brevity)\n\n# How to add default invocation args to a Runnable\n# ... (details omitted for brevity)\n\n# How to add retrieval to chatbots\n# ... (details omitted for brevity)\n\n# How to use few shot examples in chat models\n# ... (details omitted for brevity)\n\n# How to do tool/function calling\n# ... (details omitted for brevity)\n\n# How to install LangChain packages\n# ... (details omitted for brevity)\n\n# How to add examples to the prompt for query analysis\n# ... (details omitted for brevity)\n\n# How to use few shot examples\n# ... (details omitted for brevity)\n\n# How to run custom functions\n# ... (details omitted for brevity)\n\n# How to use output parsers to parse an LLM response into structured format\n# ... (details omitted for brevity)\n\n# How to handle cases where no queries are generated\n# ... (details omitted for brevity)\n\n# How to route between sub-chains\n# ... (details omitted for brevity)\n\n# How to return structured data from a model\n# ... (details omitted for brevity)\n```\n\n----------------------------------------\n\nTITLE: Extraction Techniques\nDESCRIPTION: Guides on using reference examples and handling long text for extraction, as well as performing extraction using only prompting. SOURCE: https://python.langchain.com/docs/how_to/graph_constructing/\n\nLANGUAGE: python\nCODE:\n```\n# How to use reference examples when doing extraction\n# /docs/how_to/extraction_examples/\n\n# How to handle long text when doing extraction\n# /docs/how_to/extraction_long_text/\n\n# How to use prompting alone (no tool calling) to do extraction\n# /docs/how_to/extraction_parse/\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools. This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/how_to/summarize_stuff/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant.",
    "chunk_length": 2713
  },
  {
    "chunk_id": 122,
    "source": "python_langchain_llms_data",
    "content": "Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: LangChain Tutorials Overview\nDESCRIPTION: Provides a list of tutorials for building various applications with LangChain, such as question answering, chatbots, RAG, and agents. SOURCE: https://python.langchain.com/docs/how_to/tools_few_shot/\n\nLANGUAGE: markdown\nCODE:\n```\n*   [Build a Question Answering application over a Graph Database](/docs/tutorials/graph/)\n*   [Tutorials](/docs/tutorials/)\n*   [Build a simple LLM application with chat models and prompt templates](/docs/tutorials/llm_chain/)\n*   [Build a Chatbot](/docs/tutorials/chatbot/)\n*   [Build a Retrieval Augmented Generation (RAG) App: Part 2](/docs/tutorials/qa_chat_history/)\n*   [Build an Extraction Chain](/docs/tutorials/extraction/)\n*   [Build an Agent](/docs/tutorials/agents/)\n*   [Tagging](/docs/tutorials/classification/)\n*   [Build a Retrieval Augmented Generation (RAG) App: Part 1](/docs/tutorials/rag/)\n*   [Build a semantic search engine](/docs/tutorials/retrievers/)\n*   [Build a Question/Answering system over SQL data](/docs/tutorials/sql_qa/)\n*   [Summarize Text](/docs/tutorials/summarization/)\n```\n\n----------------------------------------\n\nTITLE: Outlines Chat Integration\nDESCRIPTION: Facilitates getting started with Outlines chat models. SOURCE: https://python.langchain.com/docs/integrations/chat/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.chat_models import ChatOutlines\n\n# Example usage (requires Outlines setup)\n# llm = ChatOutlines()\n# response = llm.invoke(\"Generate a list of potential blog post titles.\")\n# print(response)\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools.",
    "chunk_length": 2486
  },
  {
    "chunk_id": 123,
    "source": "python_langchain_llms_data",
    "content": "This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/how_to/callbacks_async/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: LangChain Tool Calling - Few-Shot Prompting\nDESCRIPTION: Demonstrates how to use few-shot examples within the prompt to guide the LLM on how and when to use tools effectively. This improves the accuracy and reliability of tool calls. SOURCE: https://python.langchain.com/docs/how_to/tool_configure/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import AgentExecutor, create_tool_calling_agent\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\n@tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather for a city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n@tool\ndef get_time(city: str) -> str:\n    \"\"\"Get the current time for a city.\"\"\"\n    return f\"The time in {city} is 10:00 AM.\"\n\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\ntools = [get_weather, get_time]\n\n# Few-shot examples embedded in the prompt\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant.",
    "chunk_length": 2601
  },
  {
    "chunk_id": 124,
    "source": "python_langchain_llms_data",
    "content": "Use the available tools.\"),\n    (\"human\", \"What is the weather in Paris?\"),\n    (\"assistant\", \"{\\\"tool\\\": \\\"get_weather\\\", \\\"tool_input\\\": {\\\"city\\\": \\\"Paris\\\"}}\"),\n    (\"human\", \"What time is it in Tokyo?\"),\n    (\"assistant\", \"{\\\"tool\\\": \\\"get_time\\\", \\\"tool_input\\\": {\\\"city\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools. This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/how_to/chat_models_universal_init/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: Extraction Techniques\nDESCRIPTION: Guides on using reference examples and handling long text for extraction, as well as performing extraction using only prompting.",
    "chunk_length": 2247
  },
  {
    "chunk_id": 125,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/how_to/runnable_runtime_secrets/\n\nLANGUAGE: python\nCODE:\n```\n# How to use reference examples when doing extraction\n# /docs/how_to/extraction_examples/\n\n# How to handle long text when doing extraction\n# /docs/how_to/extraction_long_text/\n\n# How to use prompting alone (no tool calling) to do extraction\n# /docs/how_to/extraction_parse/\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: Practical guides on implementing specific features and functionalities within LangChain, such as using tools, memory, vectorstores, and parallel execution. SOURCE: https://python.langchain.com/docs/concepts/text_splitters/\n\nLANGUAGE: python\nCODE:\n```\nHow to use tools in a chain\nHow to use a vectorstore as a retriever\nHow to add memory to chatbots\nHow to use example selectors\nHow to add a semantic layer over graph database\nHow to invoke runnables in parallel\nHow to stream chat model responses\nHow to add default invocation args to a Runnable\nHow to add retrieval to chatbots\nHow to use few shot examples in chat models\nHow to do tool/function calling\nHow to install LangChain packages\nHow to add examples to the prompt for query analysis\nHow to use few shot examples\nHow to run custom functions\nHow to use output parsers to parse an LLM response into structured format\nHow to handle cases where no queries are generated\nHow to route between sub-chains\nHow to return structured data from a model\nHow to summarize text through parallelization\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: Practical guides on implementing specific features and functionalities within LangChain, such as adding memory, using tools, or streaming responses. SOURCE: https://python.langchain.com/docs/how_to/streaming/\n\nLANGUAGE: APIDOC\nCODE:\n```\nLangChain How-to Guides:\n\nThis section provides practical instructions for implementing various LangChain features. **Key How-to Guides:**\n- **Using Tools in a Chain**: Integrating external tools (e.g., search engines, calculators) into LangChain workflows.",
    "chunk_length": 2098
  },
  {
    "chunk_id": 126,
    "source": "python_langchain_llms_data",
    "content": "- **Vectorstore as Retriever**: Configuring and using vector stores for efficient retrieval. - **Adding Memory to Chatbots**: Implementing conversational memory to maintain context. - **Example Selectors**: Utilizing different strategies for selecting few-shot examples. - **Semantic Layer over Graph Database**: Adding a semantic understanding layer to graph data. - **Invoking Runnables in Parallel**: Executing multiple runnables concurrently for performance. - **Streaming Chat Model Responses**: Handling real-time streaming of LLM outputs. - **Default Invocation Args**: Setting default arguments for runnable invocations. - **Adding Retrieval to Chatbots**: Integrating retrieval mechanisms into conversational agents. - **Few-shot Examples in Chat Models**: Providing examples to guide chat model behavior. - **Tool/Function Calling**: Enabling LLMs to call external functions or tools. - **Installation**: Instructions for installing LangChain packages. - **Query Few-shot**: Using few-shot examples for query analysis. - **Few-shot Examples**: General usage of few-shot examples. - **Running Custom Functions**: Executing user-defined functions within LangChain. - **Output Parsers**: Parsing LLM responses into structured formats. - **Handling No Queries**: Strategies for managing scenarios where no queries are generated. - **Routing between Sub-chains**: Implementing logic to direct execution flow between different chains. - **Structured Output**: Ensuring LLMs return data in a predefined structure. - **Summarization through Parallelization**: Efficiently summarizing large texts using parallel processing. ```\n\n----------------------------------------\n\nTITLE: SQLiteVec VectorStore Integration\nDESCRIPTION: This notebook covers how to get started with the SQLiteVec vector store. SOURCE: https://python.langchain.com/docs/integrations/vectorstores/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.vectorstores import SQLiteVec\n\n# Example usage (assuming SQLite database with SQLiteVec)\n# vector_store = SQLiteVec(table_name=\"my_table\")\n# results = vector_store.similarity_search(\"query text\")\n```\n\n----------------------------------------\n\nTITLE: Retrieval Context Example\nDESCRIPTION: Provides an example of retrieved context, including document IDs, metadata (source, start index, section), and page content.",
    "chunk_length": 2334
  },
  {
    "chunk_id": 127,
    "source": "python_langchain_llms_data",
    "content": "This context is used to inform the LLM's response. SOURCE: https://python.langchain.com/docs/tutorials/rag/\n\nLANGUAGE: python\nCODE:\n```\n{\n    \"retrieve\": {\n        \"context\": [\n            {\n                \"id\": \"d6cef137-e1e8-4ddc-91dc-b62bd33c6020\",\n                \"metadata\": {\n                    \"source\": \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n                    \"start_index\": 39221,\n                    \"section\": \"end\"\n                },\n                \"page_content\": \"Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\"\n            },\n            {\n                \"id\": \"d1834ae1-eb6a-43d7-a023-08dfa5028799\",\n                \"metadata\": {\n                    \"source\": \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n                    \"start_index\": 39086,\n                    \"section\": \"end\"\n                },\n                \"page_content\": \"}\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:\"\n            },\n            {\n                \"id\": \"ca7f06e4-2c2e-4788-9a81-2418d82213d9\",\n                \"metadata\": {\n                    \"source\": \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n                    \"start_index\": 32942,\n                    \"section\": \"end\"\n                },\n                \"page_content\": \"}\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:\"\n            },\n            {\n                \"id\": \"1fcc2736-30f4-4ef6-90f2-c64af92118cb\",\n                \"metadata\": {\n                    \"source\": \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n                    \"start_index\": 35127,\n                    \"section\": \"end\"\n                },\n                \"page_content\": \"\\\"content\\\": \\\"You will get instructions for code to write.\\nYou will write a very long answer.",
    "chunk_length": 2691
  },
  {
    "chunk_id": 128,
    "source": "python_langchain_llms_data",
    "content": "Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\n\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\n\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\n\\nFILENAME\\n```LANG\\nCODE\\n```\\n\\nYou will start with the \\\"entrypoint\\\" file, then go to the ones that are imported by that file, and so on.\\nPlease\"\n            }\n        ]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: This section provides practical guides on implementing specific features and functionalities within LangChain. It covers topics like using tools, vectorstores, memory, prompt selectors, and more. SOURCE: https://python.langchain.com/docs/how_to/semantic-chunker/\n\nLANGUAGE: python\nCODE:\n```\nHow to use tools in a chain\nHow to use a vectorstore as a retriever\nHow to add memory to chatbots\nHow to use example selectors\nHow to add a semantic layer over graph database\nHow to invoke runnables in parallel\nHow to stream chat model responses\nHow to add default invocation args to a Runnable\nHow to add retrieval to chatbots\nHow to use few shot examples in chat models\nHow to do tool/function calling\nHow to install LangChain packages\nHow to add examples to the prompt for query analysis\nHow to use few shot examples\nHow to run custom functions\nHow to use output parsers to parse an LLM response into structured format\nHow to handle cases where no queries are generated\nHow to route between sub-chains\nHow to return structured data from a model\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools.",
    "chunk_length": 2271
  },
  {
    "chunk_id": 129,
    "source": "python_langchain_llms_data",
    "content": "This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/how_to/tool_stream_events/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: Offers practical guides on implementing specific features and functionalities within LangChain applications, such as using tools, memory, streaming, and structured output. SOURCE: https://python.langchain.com/docs/how_to/tools_error/\n\nLANGUAGE: python\nCODE:\n```\nHow to use tools in a chain\nHow to use a vectorstore as a retriever\nHow to add memory to chatbots\nHow to use example selectors\nHow to add a semantic layer over graph database\nHow to invoke runnables in parallel\nHow to stream chat model responses\nHow to add default invocation args to a Runnable\nHow to add retrieval to chatbots\nHow to use few shot examples in chat models\nHow to do tool/function calling\nHow to install LangChain packages\nHow to add examples to the prompt for query analysis\nHow to use few shot examples\nHow to run custom functions\nHow to use output parsers to parse an LLM response into structured format\nHow to handle cases where no queries are generated\nHow to route between sub-chains\nHow to return structured data from a model\nHow to summarize text through parallelization\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools.",
    "chunk_length": 2891
  },
  {
    "chunk_id": 130,
    "source": "python_langchain_llms_data",
    "content": "This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/how_to/output_parser_custom/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides Overview\nDESCRIPTION: This section outlines various how-to guides for LangChain, demonstrating practical implementation of features such as tools, memory, output parsers, and parallel execution. SOURCE: https://python.langchain.com/docs/how_to/embed_text/\n\nLANGUAGE: python\nCODE:\n```\nHow to use tools in a chain\nHow to use a vectorstore as a retriever\nHow to add memory to chatbots\nHow to use example selectors\nHow to add a semantic layer over graph database\nHow to invoke runnables in parallel\nHow to stream chat model responses\nHow to add default invocation args to a Runnable\nHow to add retrieval to chatbots\nHow to use few shot examples in chat models\nHow to do tool/function calling\nHow to install LangChain packages\nHow to add examples to the prompt for query analysis\nHow to use few shot examples\nHow to run custom functions\nHow to use output parsers to parse an LLM response into structured format\nHow to handle cases where no queries are generated\nHow to route between sub-chains\nHow to return structured data from a model\nHow to summarize text through parallelization\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools.",
    "chunk_length": 2902
  },
  {
    "chunk_id": 131,
    "source": "python_langchain_llms_data",
    "content": "This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/how_to/document_loader_directory/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: Extraction Techniques\nDESCRIPTION: Guides on using reference examples and handling long text for extraction, as well as performing extraction using only prompting. SOURCE: https://python.langchain.com/docs/how_to/pydantic_compatibility/\n\nLANGUAGE: python\nCODE:\n```\n# How to use reference examples when doing extraction\n# /docs/how_to/extraction_examples/\n\n# How to handle long text when doing extraction\n# /docs/how_to/extraction_long_text/\n\n# How to use prompting alone (no tool calling) to do extraction\n# /docs/how_to/extraction_parse/\n```\n\n----------------------------------------\n\nTITLE: Extraction Techniques\nDESCRIPTION: Guides on using reference examples and handling long text for extraction, as well as performing extraction using only prompting.",
    "chunk_length": 2353
  },
  {
    "chunk_id": 132,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/how_to/document_loader_directory/\n\nLANGUAGE: python\nCODE:\n```\n# How to use reference examples when doing extraction\n# /docs/how_to/extraction_examples/\n\n# How to handle long text when doing extraction\n# /docs/how_to/extraction_long_text/\n\n# How to use prompting alone (no tool calling) to do extraction\n# /docs/how_to/extraction_parse/\n```\n\n----------------------------------------\n\nTITLE: Extraction Techniques\nDESCRIPTION: Guides on using reference examples and handling long text for extraction, as well as performing extraction using only prompting. SOURCE: https://python.langchain.com/docs/how_to/callbacks_attach/\n\nLANGUAGE: python\nCODE:\n```\n# How to use reference examples when doing extraction\n# /docs/how_to/extraction_examples/\n\n# How to handle long text when doing extraction\n# /docs/how_to/extraction_long_text/\n\n# How to use prompting alone (no tool calling) to do extraction\n# /docs/how_to/extraction_parse/\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: Guides that explain how to perform specific tasks and implement features within LangChain. SOURCE: https://python.langchain.com/docs/how_to/graph_constructing/\n\nLANGUAGE: APIDOC\nCODE:\n```\nHow-to guides:\n  - How to use tools in a chain: /docs/how_to/tools_chain/\n  - How to use a vectorstore as a retriever: /docs/how_to/vectorstore_retriever/\n  - How to add memory to chatbots: /docs/how_to/chatbots_memory/\n  - How to use example selectors: /docs/how_to/example_selectors/\n  - How to add a semantic layer over graph database: /docs/how_to/graph_semantic/\n  - How to invoke runnables in parallel: /docs/how_to/parallel/\n  - How to stream chat model responses: /docs/how_to/chat_streaming/\n  - How to add default invocation args to a Runnable: /docs/how_to/binding/\n  - How to add retrieval to chatbots: /docs/how_to/chatbots_retrieval/\n  - How to use few shot examples in chat models: /docs/how_to/few_shot_examples_chat/\n  - How to do tool/function calling: /docs/how_to/function_calling/\n  - How to install LangChain packages: /docs/how_to/installation/\n  - How to add examples to the prompt for query analysis: /docs/how_to/query_few_shot/\n  - How to use few shot examples: /docs/how_to/few_shot_examples/\n  - How to run custom functions: /docs/how_to/functions/\n  - How to use output parsers to parse an LLM response into structured format: /docs/how_to/output_parser_structured/\n  - How to handle cases where no queries are generated: /docs/how_to/query_no_queries/\n  - How to route between sub-chains: /docs/how_to/routing/\n  - How to return structured data from a model: /docs/how_to/structured_output/\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: Practical guides on implementing specific features and functionalities within LangChain, such as using tools, memory, output parsers, and parallel execution.",
    "chunk_length": 2914
  },
  {
    "chunk_id": 133,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/how_to/code_splitter/\n\nLANGUAGE: APIDOC\nCODE:\n```\nHow-to Guides:\n  - How to use tools in a chain\n  - How to use a vectorstore as a retriever\n  - How to add memory to chatbots\n  - How to use example selectors\n  - How to add a semantic layer over graph database\n  - How to invoke runnables in parallel\n  - How to stream chat model responses\n  - How to add default invocation args to a Runnable\n  - How to add retrieval to chatbots\n  - How to use few shot examples in chat models\n  - How to do tool/function calling\n  - How to install LangChain packages\n  - How to add examples to the prompt for query analysis\n  - How to use few shot examples\n  - How to run custom functions\n  - How to use output parsers to parse an LLM response into structured format\n  - How to handle cases where no queries are generated\n  - How to route between sub-chains\n  - How to return structured data from a model\n  - How to summarize text through parallelization\n\nUsage:\n  - Implement specific LangChain features by following these guides. ```\n\n----------------------------------------\n\nTITLE: LangChain Tool Calling - Few-Shot Prompting\nDESCRIPTION: Demonstrates how to use few-shot examples within the prompt to guide the LLM on how and when to use tools effectively. This improves the accuracy and reliability of tool calls. SOURCE: https://python.langchain.com/docs/how_to/sql_prompting/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import AgentExecutor, create_tool_calling_agent\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\n@tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather for a city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n@tool\ndef get_time(city: str) -> str:\n    \"\"\"Get the current time for a city.\"\"\"\n    return f\"The time in {city} is 10:00 AM.\"\n\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\ntools = [get_weather, get_time]\n\n# Few-shot examples embedded in the prompt\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant.",
    "chunk_length": 2125
  },
  {
    "chunk_id": 134,
    "source": "python_langchain_llms_data",
    "content": "Use the available tools.\"),\n    (\"human\", \"What is the weather in Paris?\"),\n    (\"assistant\", \"{\\\"tool\\\": \\\"get_weather\\\", \\\"tool_input\\\": {\\\"city\\\": \\\"Paris\\\"}}\"),\n    (\"human\", \"What time is it in Tokyo?\"),\n    (\"assistant\", \"{\\\"tool\\\": \\\"get_time\\\", \\\"tool_input\\\": {\\\"city\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools. This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/how_to/MultiQueryRetriever/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools.",
    "chunk_length": 2231
  },
  {
    "chunk_id": 135,
    "source": "python_langchain_llms_data",
    "content": "This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/concepts/async/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: Trace without setting environment variables\nDESCRIPTION: A guide on how to enable LangSmith tracing without explicitly setting environment variables, potentially simplifying setup in certain deployment scenarios. SOURCE: https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain/\n\nLANGUAGE: APIDOC\nCODE:\n```\n/observability/how_to_guides/trace_without_env_vars\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools.",
    "chunk_length": 2165
  },
  {
    "chunk_id": 136,
    "source": "python_langchain_llms_data",
    "content": "This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/how_to/tools_few_shot/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: LangChain Tool Calling - Few-Shot Prompting\nDESCRIPTION: Demonstrates how to use few-shot examples within the prompt to guide the LLM on how and when to use tools effectively. This improves the accuracy and reliability of tool calls. SOURCE: https://python.langchain.com/docs/how_to/structured_output/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import AgentExecutor, create_tool_calling_agent\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\n@tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather for a city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n@tool\ndef get_time(city: str) -> str:\n    \"\"\"Get the current time for a city.\"\"\"\n    return f\"The time in {city} is 10:00 AM.\"\n\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\ntools = [get_weather, get_time]\n\n# Few-shot examples embedded in the prompt\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant.",
    "chunk_length": 2603
  },
  {
    "chunk_id": 137,
    "source": "python_langchain_llms_data",
    "content": "Use the available tools.\"),\n    (\"human\", \"What is the weather in Paris?\"),\n    (\"assistant\", \"{\\\"tool\\\": \\\"get_weather\\\", \\\"tool_input\\\": {\\\"city\\\": \\\"Paris\\\"}}\"),\n    (\"human\", \"What time is it in Tokyo?\"),\n    (\"assistant\", \"{\\\"tool\\\": \\\"get_time\\\", \\\"tool_input\\\": {\\\"city\n```\n\n----------------------------------------\n\nTITLE: LangChain Tutorials Overview\nDESCRIPTION: This section provides an overview of various tutorials available for LangChain, covering a wide range of applications from simple LLM interactions to complex agents and retrieval systems. SOURCE: https://python.langchain.com/docs/how_to/multimodal_prompts/\n\nLANGUAGE: python\nCODE:\n```\nBuild a Question Answering application over a Graph Database\nBuild a simple LLM application with chat models and prompt templates\nBuild a Chatbot\nBuild a Retrieval Augmented Generation (RAG) App: Part 2\nBuild an Extraction Chain\nBuild an Agent\nTagging\nBuild a Retrieval Augmented Generation (RAG) App: Part 1\nBuild a semantic search engine\nBuild a Question/Answering system over SQL data\nSummarize Text\n```\n\n----------------------------------------\n\nTITLE: LangChain Tool Calling - Few-Shot Prompting\nDESCRIPTION: Demonstrates how to use few-shot examples within the prompt to guide the LLM on how and when to use tools effectively. This improves the accuracy and reliability of tool calls. SOURCE: https://python.langchain.com/docs/how_to/markdown_header_metadata_splitter/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import AgentExecutor, create_tool_calling_agent\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\n@tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather for a city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n@tool\ndef get_time(city: str) -> str:\n    \"\"\"Get the current time for a city.\"\"\"\n    return f\"The time in {city} is 10:00 AM.\"\n\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\ntools = [get_weather, get_time]\n\n# Few-shot examples embedded in the prompt\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant.",
    "chunk_length": 2148
  },
  {
    "chunk_id": 138,
    "source": "python_langchain_llms_data",
    "content": "Use the available tools.\"),\n    (\"human\", \"What is the weather in Paris?\"),\n    (\"assistant\", \"{\\\"tool\\\": \\\"get_weather\\\", \\\"tool_input\\\": {\\\"city\\\": \\\"Paris\\\"}}\"),\n    (\"human\", \"What time is it in Tokyo?\"),\n    (\"assistant\", \"{\\\"tool\\\": \\\"get_time\\\", \\\"tool_input\\\": {\\\"city\n```\n\n----------------------------------------\n\nTITLE: LangChain Tool Calling - Few-Shot Prompting\nDESCRIPTION: Demonstrates how to use few-shot examples within the prompt to guide the LLM on how and when to use tools effectively. This improves the accuracy and reliability of tool calls. SOURCE: https://python.langchain.com/docs/how_to/example_selectors_langsmith/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import AgentExecutor, create_tool_calling_agent\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\n@tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather for a city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n@tool\ndef get_time(city: str) -> str:\n    \"\"\"Get the current time for a city.\"\"\"\n    return f\"The time in {city} is 10:00 AM.\"\n\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\ntools = [get_weather, get_time]\n\n# Few-shot examples embedded in the prompt\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    (\"human\", \"What is the weather in Paris?\"),\n    (\"assistant\", \"{\\\"tool\\\": \\\"get_weather\\\", \\\"tool_input\\\": {\\\"city\\\": \\\"Paris\\\"}}\"),\n    (\"human\", \"What time is it in Tokyo?\"),\n    (\"assistant\", \"{\\\"tool\\\": \\\"get_time\\\", \\\"tool_input\\\": {\\\"city\n```\n\n----------------------------------------\n\nTITLE: Example Selectors\nDESCRIPTION: Demonstrates various strategies for selecting examples, including using LangSmith datasets, selecting by length, maximal marginal relevance (MMR), n-gram overlap, and similarity. SOURCE: https://python.langchain.com/docs/how_to/migrate_agent/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.prompts.example_selector import SemanticSimilarityExampleSelector\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_core.example_selectors import LengthBasedExampleSelector\n\n# Example for selecting examples by length\n# example_selector = LengthBasedExampleSelector(\n#     examples=my_examples, \n#     example_prompt=example_prompt, \n#     max_length=100\n# )\n\n# Example for selecting examples by similarity\n# example_selector = SemanticSimilarityExampleSelector(\n#     vectorstore=Chroma.from_documents(my_examples, OpenAIEmbeddings()),\n#     k=2,\n#     example_prompt=example_prompt,\n#     input_keys=[\"input\"],\n# )\n\n```\n\n----------------------------------------\n\nTITLE: RunPod Chat Model Integration\nDESCRIPTION: Get started with RunPod chat models.",
    "chunk_length": 2820
  },
  {
    "chunk_id": 139,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/integrations/chat/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.chat_models import ChatRunPod\n\n# Example usage (requires RunPod API key and endpoint)\n# llm = ChatRunPod(api_key=\"YOUR_RUNPOD_API_KEY\", endpoint_url=\"YOUR_RUNPOD_ENDPOINT_URL\")\n# response = llm.invoke(\"Generate a creative story prompt.\")\n# print(response)\n```\n\n----------------------------------------\n\nTITLE: Extraction Techniques\nDESCRIPTION: Guides on using reference examples and handling long text for extraction, as well as performing extraction using only prompting. SOURCE: https://python.langchain.com/docs/how_to/query_high_cardinality/\n\nLANGUAGE: python\nCODE:\n```\n# How to use reference examples when doing extraction\n# /docs/how_to/extraction_examples/\n\n# How to handle long text when doing extraction\n# /docs/how_to/extraction_long_text/\n\n# How to use prompting alone (no tool calling) to do extraction\n# /docs/how_to/extraction_parse/\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools. This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/how_to/configure/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: Guides on performing specific tasks with LangChain, including trimming messages and creating/querying vector stores.",
    "chunk_length": 2906
  },
  {
    "chunk_id": 140,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/how_to/sql_prompting/\n\nLANGUAGE: python\nCODE:\n```\n# How-to guide links:\n# Trim messages: /docs/how_to/trim_messages/\n# Create and query vector stores: /docs/how_to/vectorstores/\n```\n\n----------------------------------------\n\nTITLE: LangChain Tool Calling - Few-Shot Prompting\nDESCRIPTION: Demonstrates how to use few-shot examples within the prompt to guide the LLM on how and when to use tools effectively. This improves the accuracy and reliability of tool calls. SOURCE: https://python.langchain.com/docs/how_to/summarize_refine/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import AgentExecutor, create_tool_calling_agent\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\n@tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather for a city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n@tool\ndef get_time(city: str) -> str:\n    \"\"\"Get the current time for a city.\"\"\"\n    return f\"The time in {city} is 10:00 AM.\"\n\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\ntools = [get_weather, get_time]\n\n# Few-shot examples embedded in the prompt\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    (\"human\", \"What is the weather in Paris?\"),\n    (\"assistant\", \"{\\\"tool\\\": \\\"get_weather\\\", \\\"tool_input\\\": {\\\"city\\\": \\\"Paris\\\"}}\"),\n    (\"human\", \"What time is it in Tokyo?\"),\n    (\"assistant\", \"{\\\"tool\\\": \\\"get_time\\\", \\\"tool_input\\\": {\\\"city\n```\n\n----------------------------------------\n\nTITLE: LangChain Tool Calling - Few-Shot Prompting\nDESCRIPTION: Demonstrates how to use few-shot examples within the prompt to guide the LLM on how and when to use tools effectively. This improves the accuracy and reliability of tool calls. SOURCE: https://python.langchain.com/docs/how_to/query_constructing_filters/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import AgentExecutor, create_tool_calling_agent\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\n@tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather for a city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n@tool\ndef get_time(city: str) -> str:\n    \"\"\"Get the current time for a city.\"\"\"\n    return f\"The time in {city} is 10:00 AM.\"\n\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\ntools = [get_weather, get_time]\n\n# Few-shot examples embedded in the prompt\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant.",
    "chunk_length": 2648
  },
  {
    "chunk_id": 141,
    "source": "python_langchain_llms_data",
    "content": "Use the available tools.\"),\n    (\"human\", \"What is the weather in Paris?\"),\n    (\"assistant\", \"{\\\"tool\\\": \\\"get_weather\\\", \\\"tool_input\\\": {\\\"city\\\": \\\"Paris\\\"}}\"),\n    (\"human\", \"What time is it in Tokyo?\"),\n    (\"assistant\", \"{\\\"tool\\\": \\\"get_time\\\", \\\"tool_input\\\": {\\\"city\n```\n\n----------------------------------------\n\nTITLE: FewShotPromptTemplate Usage\nDESCRIPTION: Demonstrates how to create a `FewShotPromptTemplate` using an `example_selector` and `example_prompt`, and then invoke it with an input. SOURCE: https://python.langchain.com/docs/how_to/few_shot_examples/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\nfrom langchain_core.example_selectors import SemanticSimilarityExampleSelector\n\n# Assuming example_selector and example_prompt are defined elsewhere\n# example_selector = SemanticSimilarityExampleSelector(...)\n# example_prompt = PromptTemplate(...)\n\nprompt = FewShotPromptTemplate(\n    example_selector=example_selector,\n    example_prompt=example_prompt,\n    suffix=\"Question: {input}\",\n    input_variables=[\"input\"],\n)\n\nprint(\n    prompt.invoke({\"input\": \"Who was the father of Mary Ball Washington?\"}).to_string()\n)\n```\n\n----------------------------------------\n\nTITLE: Python Langchain LLM Chatbot Example\nDESCRIPTION: Provides an example of building a simple chatbot using Langchain LLMs in Python. This snippet demonstrates a conversational flow where the LLM responds to user input. Ensure 'langchain' and 'openai' are installed. SOURCE: https://python.langchain.com/docs/tutorials/summarization/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.llms import OpenAI\nfrom langchain.chains import ConversationChain\nfrom langchain.memory import ConversationBufferMemory\n\n# Initialize LLM and memory\nllm = OpenAI(model_name=\"text-davinci-003\", temperature=0.7)\nmemory = ConversationBufferMemory()\n\n# Create the conversation chain\nconversation = ConversationChain(llm=llm, memory=memory, verbose=True)\n\n# Start the conversation\nprint(\"Chatbot: Hello! How can I help you today?\")\nwhile True:\n    user_input = input(\"You: \")\n    if user_input.lower() == \"quit\":\n        break\n    response = conversation.invoke(user_input)\n    print(f\"Chatbot: {response['response']}\")\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: Practical guides on implementing specific features and functionalities within LangChain applications, such as using tools, memory, and output parsers.",
    "chunk_length": 2490
  },
  {
    "chunk_id": 142,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/how_to/custom_callbacks/\n\nLANGUAGE: APIDOC\nCODE:\n```\nHow-to guides:\n  - How to use tools in a chain: /docs/how_to/tools_chain/\n  - How to use a vectorstore as a retriever: /docs/how_to/vectorstore_retriever/\n  - How to add memory to chatbots: /docs/how_to/chatbots_memory/\n  - How to use example selectors: /docs/how_to/example_selectors/\n  - How to add a semantic layer over graph database: /docs/how_to/graph_semantic/\n  - How to invoke runnables in parallel: /docs/how_to/parallel/\n  - How to stream chat model responses: /docs/how_to/chat_streaming/\n  - How to add default invocation args to a Runnable: /docs/how_to/binding/\n  - How to add retrieval to chatbots: /docs/how_to/chatbots_retrieval/\n  - How to use few shot examples in chat models: /docs/how_to/few_shot_examples_chat/\n  - How to do tool/function calling: /docs/how_to/function_calling/\n  - How to install LangChain packages: /docs/how_to/installation/\n  - How to add examples to the prompt for query analysis: /docs/how_to/query_few_shot/\n  - How to use few shot examples: /docs/how_to/few_shot_examples/\n  - How to run custom functions: /docs/how_to/functions/\n  - How to use output parsers to parse an LLM response into structured format: /docs/how_to/output_parser_structured/\n  - How to handle cases where no queries are generated: /docs/how_to/query_no_queries/\n  - How to route between sub-chains: /docs/how_to/routing/\n  - How to return structured data from a model: /docs/how_to/structured_output/\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools. This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/how_to/query_few_shot/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant.",
    "chunk_length": 2566
  },
  {
    "chunk_id": 143,
    "source": "python_langchain_llms_data",
    "content": "Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: Extraction Techniques\nDESCRIPTION: Guides on using reference examples and handling long text for extraction, as well as performing extraction using only prompting. SOURCE: https://python.langchain.com/docs/how_to/tools_human/\n\nLANGUAGE: python\nCODE:\n```\n# How to use reference examples when doing extraction\n# /docs/how_to/extraction_examples/\n\n# How to handle long text when doing extraction\n# /docs/how_to/extraction_long_text/\n\n# How to use prompting alone (no tool calling) to do extraction\n# /docs/how_to/extraction_parse/\n```\n\n----------------------------------------\n\nTITLE: Example Selectors\nDESCRIPTION: Demonstrates various strategies for selecting examples, including using LangSmith datasets, selecting by length, maximal marginal relevance (MMR), n-gram overlap, and similarity. SOURCE: https://python.langchain.com/docs/how_to/query_multiple_queries/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.prompts.example_selector import SemanticSimilarityExampleSelector\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_core.example_selectors import LengthBasedExampleSelector\n\n# Example for selecting examples by length\n# example_selector = LengthBasedExampleSelector(\n#     examples=my_examples, \n#     example_prompt=example_prompt, \n#     max_length=100\n# )\n\n# Example for selecting examples by similarity\n# example_selector = SemanticSimilarityExampleSelector(\n#     vectorstore=Chroma.from_documents(my_examples, OpenAIEmbeddings()),\n#     k=2,\n#     example_prompt=example_prompt,\n#     input_keys=[\"input\"],\n# )\n\n```\n\n----------------------------------------\n\nTITLE: Symbl.ai Nebula Chat Integration\nDESCRIPTION: Covers getting started with Nebula, Symbl.ai's chat integration.",
    "chunk_length": 2497
  },
  {
    "chunk_id": 144,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/integrations/chat/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.chat_models import ChatSymblaiNebula\n\n# Example usage (requires Symbl.ai credentials)\n# llm = ChatSymblaiNebula(app_id=\"YOUR_APP_ID\", app_secret=\"YOUR_APP_SECRET\")\n# response = llm.invoke(\"Summarize our last meeting.\")\n# print(response)\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools. This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/how_to/custom_llm/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: LangChain Tutorials Overview\nDESCRIPTION: A list of available tutorials for building various LLM applications with LangChain.",
    "chunk_length": 2267
  },
  {
    "chunk_id": 145,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/tutorials/summarization/\n\nLANGUAGE: APIDOC\nCODE:\n```\nBuild a Question Answering application over a Graph Database\nBuild a simple LLM application with chat models and prompt templates\nBuild a Chatbot\nBuild a Retrieval Augmented Generation (RAG) App: Part 2\nBuild an Extraction Chain\nBuild an Agent\nTagging\nBuild a Retrieval Augmented Generation (RAG) App: Part 1\nBuild a semantic search engine\nBuild a Question/Answering system over SQL data\nSummarize Text\n```\n\n----------------------------------------\n\nTITLE: How to Build an Agent\nDESCRIPTION: This guide provides a comprehensive overview of building agents in LangChain, which are systems that use an LLM to decide which actions to take and in what order. It covers agent types, tools, and execution. SOURCE: https://python.langchain.com/docs/how_to/output_parser_structured/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import AgentExecutor, create_tool_calling_agent\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.tools import tool\n\n@tool\ndef get_current_weather(city: str) -> str:\n    \"\"\"Get the current weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny and 25 degrees Celsius.\"\n\nllm = ChatOpenAI()\n\n# Define the prompt for the agent\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the get_current_weather tool.\"),\n    (\"human\", \"{input}\"),\n    # ... other message types ... ])\n\n# Create the agent and executor\nagent = create_tool_calling_agent(llm, [get_current_weather], prompt)\nagent_executor = AgentExecutor(agent=agent, tools=[get_current_weather])\n\n# Invoke the agent\n# print(agent_executor.invoke({\"input\": \"What's the weather in Paris?\"}))\n\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools. This improves the accuracy and reliability of tool usage.",
    "chunk_length": 2051
  },
  {
    "chunk_id": 146,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/how_to/tools_builtin/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: Details various how-to guides for LangChain, covering practical implementation aspects and common tasks. SOURCE: https://python.langchain.com/docs/concepts/tokens/\n\nLANGUAGE: APIDOC\nCODE:\n```\nHow-to guides:\n- How to use tools in a chain\n- How to use a vectorstore as a retriever\n- How to add memory to chatbots\n- How to use example selectors\n- How to add a semantic layer over graph database\n- How to invoke runnables in parallel\n- How to stream chat model responses\n- How to add default invocation args to a Runnable\n- How to add retrieval to chatbots\n- How to use few shot examples in chat models\n- How to do tool/function calling\n- How to install LangChain packages\n- How to add examples to the prompt for query analysis\n- How to use few shot examples\n- How to run custom functions\n- How to use output parsers to parse an LLM response into structured format\n- How to handle cases where no queries are generated\n- How to route between sub-chains\n- How to return structured data from a model\n- How to summarize text through parallelization\n```\n\n----------------------------------------\n\nTITLE: LangChain Tool Calling - Few-Shot Prompting\nDESCRIPTION: Demonstrates how to use few-shot examples within the prompt to guide the LLM on how and when to use tools effectively.",
    "chunk_length": 2834
  },
  {
    "chunk_id": 147,
    "source": "python_langchain_llms_data",
    "content": "This improves the accuracy and reliability of tool calls. SOURCE: https://python.langchain.com/docs/how_to/callbacks_runtime/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import AgentExecutor, create_tool_calling_agent\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\n@tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather for a city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n@tool\ndef get_time(city: str) -> str:\n    \"\"\"Get the current time for a city.\"\"\"\n    return f\"The time in {city} is 10:00 AM.\"\n\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\ntools = [get_weather, get_time]\n\n# Few-shot examples embedded in the prompt\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    (\"human\", \"What is the weather in Paris?\"),\n    (\"assistant\", \"{\\\"tool\\\": \\\"get_weather\\\", \\\"tool_input\\\": {\\\"city\\\": \\\"Paris\\\"}}\"),\n    (\"human\", \"What time is it in Tokyo?\"),\n    (\"assistant\", \"{\\\"tool\\\": \\\"get_time\\\", \\\"tool_input\\\": {\\\"city\n```\n\n----------------------------------------\n\nTITLE: Langchain Text Generation Example\nDESCRIPTION: Demonstrates basic text generation using a Langchain LLM. This snippet requires the Langchain library to be installed. SOURCE: https://python.langchain.com/docs/tutorials/graph/\n\nLANGUAGE: Python\nCODE:\n```\nfrom langchain_community.llms import OpenAI\n\n# Initialize the LLM\nllm = OpenAI(model_name=\"text-davinci-003\", temperature=0.7)\n\n# Generate text\nprompt = \"Write a short story about a robot learning to love.\"\nresponse = llm.invoke(prompt)\n\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Extraction Techniques\nDESCRIPTION: Guides on using reference examples and handling long text for extraction, as well as performing extraction using only prompting. SOURCE: https://python.langchain.com/docs/how_to/chat_model_caching/\n\nLANGUAGE: python\nCODE:\n```\n# How to use reference examples when doing extraction\n# /docs/how_to/extraction_examples/\n\n# How to handle long text when doing extraction\n# /docs/how_to/extraction_long_text/\n\n# How to use prompting alone (no tool calling) to do extraction\n# /docs/how_to/extraction_parse/\n```\n\n----------------------------------------\n\nTITLE: How to use example selectors\nDESCRIPTION: Details how to use example selectors in LangChain, which dynamically select relevant examples to include in the prompt based on the current input.",
    "chunk_length": 2511
  },
  {
    "chunk_id": 148,
    "source": "python_langchain_llms_data",
    "content": "This is crucial for efficient few-shot learning. SOURCE: https://python.langchain.com/docs/how_to/few_shot_examples/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.prompts import PromptTemplate, FewShotPromptTemplate\nfrom langchain.prompts.example_selector import SemanticSimilarityExampleSelector\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_community.embeddings import OpenAIEmbeddings\n\n# Sample examples\nexamples = [\n    {\"question\": \"What is the capital of France?\", \"answer\": \"Paris\"},\n    {\"question\": \"What is the capital of Germany?\", \"answer\": \"Berlin\"},\n    {\"question\": \"What is the capital of Spain?\", \"answer\": \"Madrid\"},\n    {\"question\": \"What is the capital of Italy?\", \"answer\": \"Rome\"},\n    {\"question\": \"What is the capital of Japan?\", \"answer\": \"Tokyo\"}\n]\n\n# Create a prompt template for each example\nexample_prompt = PromptTemplate(\n    input_variables=[\"question\", \"answer\"],\n    template=\"Question: {question}\\nAnswer: {answer}\",\n)\n\n# Set up a semantic similarity example selector\n# This requires an embedding model and a vector store\nsimilarity_selector = SemanticSimilarityExampleSelector.from_examples(\n    examples,\n    OpenAIEmbeddings(),\n    Chroma,\n    k=2,  # Number of examples to retrieve\n    input_keys=[\"question\"],\n)\n\n# Create the FewShotPromptTemplate using the selector\nexample_selector_prompt = FewShotPromptTemplate(\n    example_selector=similarity_selector,\n    example_prompt=example_prompt,\n    prefix=\"Give the answer to the following question.\",\n    suffix=\"Question: {question}\\nAnswer:\",\n    input_variables=[\"question\"],\n)\n\n# Format the prompt with a new question\nformatted_prompt = example_selector_prompt.format(question=\"What is the capital of Canada?\")\n\n# print(formatted_prompt)\n# This formatted_prompt can be sent to an LLM\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: Practical guides on implementing specific features and functionalities within LangChain applications. SOURCE: https://python.langchain.com/docs/how_to/multi_vector/\n\nLANGUAGE: APIDOC\nCODE:\n```\nHow to use tools in a chain:\n  Guide on integrating and using tools within LangChain chains.",
    "chunk_length": 2176
  },
  {
    "chunk_id": 149,
    "source": "python_langchain_llms_data",
    "content": "How to use a vectorstore as a retriever:\n  Instructions for utilizing vector stores as retrieval mechanisms in LangChain. How to add memory to chatbots:\n  Guide on implementing memory functionalities for chatbots in LangChain. How to use example selectors:\n  Instructions on using example selectors for prompt engineering in LangChain. How to add a semantic layer over graph database:\n  Guide on adding semantic capabilities to graph databases with LangChain. How to invoke runnables in parallel:\n  Instructions for executing LangChain runnables concurrently. How to stream chat model responses:\n  Guide on enabling and handling streaming responses from chat models in LangChain. How to add default invocation args to a Runnable:\n  Instructions for setting default arguments for LangChain runnables. How to add retrieval to chatbots:\n  Guide on integrating retrieval mechanisms into LangChain chatbots. How to use few shot examples in chat models:\n  Instructions for using few-shot examples with chat models in LangChain. How to do tool/function calling:\n  Guide on implementing tool and function calling capabilities in LangChain. How to install LangChain packages:\n  Instructions for installing necessary LangChain packages. How to add examples to the prompt for query analysis:\n  Guide on incorporating examples into prompts for analyzing queries. How to use few shot examples:\n  Instructions for utilizing few-shot learning examples in LangChain. How to run custom functions:\n  Guide on executing custom functions within the LangChain framework. How to use output parsers to parse an LLM response into structured format:\n  Instructions for parsing LLM responses into structured formats using LangChain's output parsers. How to handle cases where no queries are generated:\n  Guide on managing scenarios where no queries are produced. How to route between sub-chains:\n  Instructions for implementing routing logic between different sub-chains in LangChain. How to return structured data from a model:\n  Guide on configuring models to return structured data using LangChain.",
    "chunk_length": 2075
  },
  {
    "chunk_id": 150,
    "source": "python_langchain_llms_data",
    "content": "```\n\n----------------------------------------\n\nTITLE: OpenAI Chat Integration\nDESCRIPTION: Provides a quick overview for getting started with OpenAI chat models. SOURCE: https://python.langchain.com/docs/integrations/chat/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_openai import ChatOpenAI\n\n# Example usage (requires OpenAI API key)\n# llm = ChatOpenAI(api_key=\"YOUR_OPENAI_API_KEY\")\n# response = llm.invoke(\"What is the capital of Canada?\")\n# print(response)\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools. This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/how_to/tool_calling_parallel/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools.",
    "chunk_length": 2416
  },
  {
    "chunk_id": 151,
    "source": "python_langchain_llms_data",
    "content": "This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/how_to/tools_human/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: Example Selectors\nDESCRIPTION: Demonstrates various strategies for selecting examples, including using LangSmith datasets, selecting by length, maximal marginal relevance (MMR), n-gram overlap, and similarity. SOURCE: https://python.langchain.com/docs/how_to/contextual_compression/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.prompts.example_selector import SemanticSimilarityExampleSelector\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_core.example_selectors import LengthBasedExampleSelector\n\n# Example for selecting examples by length\n# example_selector = LengthBasedExampleSelector(\n#     examples=my_examples, \n#     example_prompt=example_prompt, \n#     max_length=100\n# )\n\n# Example for selecting examples by similarity\n# example_selector = SemanticSimilarityExampleSelector(\n#     vectorstore=Chroma.from_documents(my_examples, OpenAIEmbeddings()),\n#     k=2,\n#     example_prompt=example_prompt,\n#     input_keys=[\"input\"],\n# )\n\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools.",
    "chunk_length": 2797
  },
  {
    "chunk_id": 152,
    "source": "python_langchain_llms_data",
    "content": "This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/how_to/recursive_text_splitter/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools. This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/how_to/pydantic_compatibility/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant.",
    "chunk_length": 2598
  },
  {
    "chunk_id": 153,
    "source": "python_langchain_llms_data",
    "content": "Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: How to Build an Agent\nDESCRIPTION: This guide provides instructions on building agents in LangChain. It covers the core concepts of agents, including tools, language models, and agent executors, and how to combine them to create intelligent agents that can reason and act. SOURCE: https://python.langchain.com/docs/how_to/tool_artifacts/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.agents import AgentExecutor, create_react_agent\nfrom langchain.tools import Tool\nfrom langchain_core.prompts import PromptTemplate\n\n# tools = [\n#     Tool(\n#         name=\"Search\",\n#         func=lambda query: f\"Results for {query}\",\n#         description=\"useful for when you need to answer questions about current events\"\n#     )\n# ]\n\n# prompt = PromptTemplate.from_template(\"...\")\n# agent = create_react_agent(llm=..., tools=tools, prompt=prompt)\n# agent_executor = AgentExecutor(agent=agent, tools=tools)\n# agent_executor.invoke({\"input\": \"What is the weather today?\"})\n\n```\n\n----------------------------------------\n\nTITLE: LangChain Tutorials\nDESCRIPTION: A collection of tutorials demonstrating how to build various LLM applications with LangChain, from simple Q&A to complex agents. SOURCE: https://python.langchain.com/docs/how_to/document_loader_json/\n\nLANGUAGE: python\nCODE:\n```\nBuild a Question Answering application over a Graph Database: /docs/tutorials/graph/\nBuild a simple LLM application with chat models and prompt templates: /docs/tutorials/llm_chain/\nBuild a Chatbot: /docs/tutorials/chatbot/\nBuild a Retrieval Augmented Generation (RAG) App: Part 2: /docs/tutorials/qa_chat_history/\nBuild an Extraction Chain: /docs/tutorials/extraction/\nBuild an Agent: /docs/tutorials/agents/\nTagging: /docs/tutorials/classification/\nBuild a Retrieval Augmented Generation (RAG) App: Part 1: /docs/tutorials/rag/\nBuild a semantic search engine: /docs/tutorials/retrievers/\nBuild a Question/Answering system over SQL data: /docs/tutorials/sql_qa/\nSummarize Text: /docs/tutorials/summarization/\n```\n\n----------------------------------------\n\nTITLE: Basic LLM Interaction Example\nDESCRIPTION: Demonstrates a simple interaction with an LLM using Langchain.",
    "chunk_length": 2885
  },
  {
    "chunk_id": 154,
    "source": "python_langchain_llms_data",
    "content": "This snippet shows how to initialize an LLM and get a response. SOURCE: https://python.langchain.com/docs/integrations/chat/ollama/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.llms import OpenAI\n\n# Initialize the LLM\nllm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=\"YOUR_API_KEY\")\n\n# Get a response from the LLM\nresponse = llm.invoke(\"What is Langchain?\")\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools. This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/how_to/chat_streaming/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: LangChain Tool Calling - Few-Shot Prompting\nDESCRIPTION: Demonstrates how to use few-shot examples within the prompt to guide the LLM on how and when to use tools effectively.",
    "chunk_length": 2362
  },
  {
    "chunk_id": 155,
    "source": "python_langchain_llms_data",
    "content": "This improves the accuracy and reliability of tool calls. SOURCE: https://python.langchain.com/docs/how_to/document_loader_office_file/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import AgentExecutor, create_tool_calling_agent\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\n@tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather for a city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n@tool\ndef get_time(city: str) -> str:\n    \"\"\"Get the current time for a city.\"\"\"\n    return f\"The time in {city} is 10:00 AM.\"\n\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\ntools = [get_weather, get_time]\n\n# Few-shot examples embedded in the prompt\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    (\"human\", \"What is the weather in Paris?\"),\n    (\"assistant\", \"{\\\"tool\\\": \\\"get_weather\\\", \\\"tool_input\\\": {\\\"city\\\": \\\"Paris\\\"}}\"),\n    (\"human\", \"What time is it in Tokyo?\"),\n    (\"assistant\", \"{\\\"tool\\\": \\\"get_time\\\", \\\"tool_input\\\": {\\\"city\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: Practical guides on implementing specific functionalities within LangChain, such as using tools, memory, and output parsers. SOURCE: https://python.langchain.com/docs/how_to/output_parser_retry/\n\nLANGUAGE: python\nCODE:\n```\n# How to use tools in a chain\n# ... (implementation details)\n\n# How to use a vectorstore as a retriever\n# ... (implementation details)\n\n# How to add memory to chatbots\n# ... (implementation details)\n\n# How to use example selectors\n# ... (implementation details)\n\n# How to add a semantic layer over graph database\n# ... (implementation details)\n\n# How to invoke runnables in parallel\n# ... (implementation details)\n\n# How to stream chat model responses\n# ... (implementation details)\n\n# How to add default invocation args to a Runnable\n# ... (implementation details)\n\n# How to add retrieval to chatbots\n# ...",
    "chunk_length": 2050
  },
  {
    "chunk_id": 156,
    "source": "python_langchain_llms_data",
    "content": "(implementation details)\n\n# How to use few shot examples in chat models\n# ... (implementation details)\n\n# How to do tool/function calling\n# ... (implementation details)\n\n# How to install LangChain packages\n# ... (implementation details)\n\n# How to add examples to the prompt for query analysis\n# ... (implementation details)\n\n# How to use few shot examples\n# ... (implementation details)\n\n# How to run custom functions\n# ... (implementation details)\n\n# How to use output parsers to parse an LLM response into structured format\n# ... (implementation details)\n\n# How to handle cases where no queries are generated\n# ... (implementation details)\n\n# How to route between sub-chains\n# ... (implementation details)\n\n# How to return structured data from a model\n# ... (implementation details)\n```\n\n----------------------------------------\n\nTITLE: LangChain Tool Calling - Few-Shot Prompting\nDESCRIPTION: Demonstrates how to use few-shot examples within the prompt to guide the LLM on how and when to use tools effectively. This improves the accuracy and reliability of tool calls. SOURCE: https://python.langchain.com/docs/how_to/llm_token_usage_tracking/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import AgentExecutor, create_tool_calling_agent\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\n@tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather for a city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n@tool\ndef get_time(city: str) -> str:\n    \"\"\"Get the current time for a city.\"\"\"\n    return f\"The time in {city} is 10:00 AM.\"\n\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\ntools = [get_weather, get_time]\n\n# Few-shot examples embedded in the prompt\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    (\"human\", \"What is the weather in Paris?\"),\n    (\"assistant\", \"{\\\"tool\\\": \\\"get_weather\\\", \\\"tool_input\\\": {\\\"city\\\": \\\"Paris\\\"}}\"),\n    (\"human\", \"What time is it in Tokyo?\"),\n    (\"assistant\", \"{\\\"tool\\\": \\\"get_time\\\", \\\"tool_input\\\": {\\\"city\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools.",
    "chunk_length": 2349
  },
  {
    "chunk_id": 157,
    "source": "python_langchain_llms_data",
    "content": "This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/how_to/prompts_partial/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: Setup OpenAI Embeddings\nDESCRIPTION: Installs the langchain-openai package and sets up the OpenAIEmbeddings model. Requires an OpenAI API key to be set as an environment variable. SOURCE: https://python.langchain.com/docs/how_to/embed_text/\n\nLANGUAGE: python\nCODE:\n```\npip install -qU langchain-openai\n\nimport getpass\nimport os\n\nif not os.environ.get(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n```\n\n----------------------------------------\n\nTITLE: SambaNovaCloud Chat Integration\nDESCRIPTION: Helps users get started with SambaNovaCloud chat models.",
    "chunk_length": 2309
  },
  {
    "chunk_id": 158,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/integrations/chat/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.chat_models import ChatSambaNovaCloud\n\n# Example usage (requires SambaNovaCloud credentials)\n# llm = ChatSambaNovaCloud(api_key=\"YOUR_SAMBANovacloud_API_KEY\")\n# response = llm.invoke(\"What are the latest advancements in AI?\")\n# print(response)\n```\n\n----------------------------------------\n\nTITLE: LangChain Tutorials\nDESCRIPTION: A collection of tutorials demonstrating how to build various applications using LangChain, from simple LLM applications to complex agents and retrieval systems. SOURCE: https://python.langchain.com/docs/how_to/tool_runtime/\n\nLANGUAGE: python\nCODE:\n```\n# Example: Build a simple LLM application with chat models and prompt templates\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_community.llms import OpenAI\n\n# Initialize the LLM\nllm = OpenAI()\n\n# Define a prompt template\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant.\"),\n    (\"user\", \"{topic}\")\n])\n\n# Define a chain\nchain = prompt | llm | StrOutputParser()\n\n# Run the chain\nresponse = chain.invoke({\"topic\": \"What is LangChain?\"})\nprint(response)\n\n```\n\nLANGUAGE: python\nCODE:\n```\n# Example: Build a Question Answering application over a Graph Database\n# This involves setting up a graph database, loading data, and creating a LangChain\n# interface to query it using natural language. # Specific implementation details would depend on the chosen graph database (e.g., Neo4j)\n# and the graph-specific LangChain components. # Placeholder for graph QA setup\nprint(\"Setting up Graph QA application...\")\n\n```\n\nLANGUAGE: python\nCODE:\n```\n# Example: Build a Retrieval Augmented Generation (RAG) App: Part 1\n# This tutorial covers setting up a retriever, loading documents, and creating a basic RAG chain. from langchain_community.document_loaders import TextLoader\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_community.embeddings import OpenAIEmbeddings\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnableSequence\nfrom langchain_community.llms import OpenAI\n\n# Load documents\nloader = TextLoader(\"path/to/your/document.txt\")\ndocs = loader.load()\n\n# Create embeddings and vector store\nembeddings = OpenAIEmbeddings()\nvectorstore = FAISS.from_documents(docs, embeddings)\nretriever = vectorstore.as_retriever()\n\n# Define prompt template\nprompt = ChatPromptTemplate.from_template(\n    \"Answer the question based on the context:\\n\\n{context}\\n\\nQuestion: {question}\"\n)\n\n# Initialize LLM\nllm = OpenAI()\n\n# Create the RAG chain\nrag_chain = (\n    {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | llm\n)\n\n# Example usage\n# query = \"What is the main topic of the document?\"\n# print(rag_chain.invoke(query))\nprint(\"RAG setup complete.",
    "chunk_length": 2980
  },
  {
    "chunk_id": 159,
    "source": "python_langchain_llms_data",
    "content": "Ready for invocation.\")\n\n```\n\nLANGUAGE: python\nCODE:\n```\n# Example: Build an Agent\n# This involves defining tools, creating an LLM agent, and running the agent. from langchain.agents import AgentExecutor, create_tool_calling_agent\nfrom langchain_community.tools import DuckDuckGoSearchRun\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\n# Initialize LLM and Tool\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\")\nsearch = DuckDuckGoSearchRun()\n\n# Define the agent's prompt\n# Note: The prompt structure might vary based on the agent type and creation method. # This is a simplified example. agent_prompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant.\"),\n    (\"user\", \"{input}\"),\n    (\"placeholder\", \"{agent_scratchpad}\"),\n])\n\n# Create the agent (example using create_tool_calling_agent)\n# This requires a list of tools and the LLM\ntools = [search]\nagent = create_tool_calling_agent(llm, tools, agent_prompt)\n\n# Create an Agent Executor\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\n# Run the agent\n# agent_executor.invoke({\"input\": \"What is the weather in San Francisco?\"})\nprint(\"Agent setup complete. Ready for invocation.\")\n\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools. This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/how_to/streaming_llm/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant.",
    "chunk_length": 2273
  },
  {
    "chunk_id": 160,
    "source": "python_langchain_llms_data",
    "content": "Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools. This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/how_to/summarize_map_reduce/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools.",
    "chunk_length": 2646
  },
  {
    "chunk_id": 161,
    "source": "python_langchain_llms_data",
    "content": "This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/concepts/rag/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: Perplexity Chat Integration\nDESCRIPTION: Helps users get started with Perplexity chat models. SOURCE: https://python.langchain.com/docs/integrations/chat/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.chat_models import ChatPerplexity\n\n# Example usage (requires Perplexity API key)\n# llm = ChatPerplexity(api_key=\"YOUR_PERPLEXITY_API_KEY\")\n# response = llm.invoke(\"What are the main causes of climate change?\")\n# print(response)\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools.",
    "chunk_length": 2221
  },
  {
    "chunk_id": 162,
    "source": "python_langchain_llms_data",
    "content": "This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/how_to/tools_model_specific/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: Practical guides on implementing specific features and functionalities within LangChain. SOURCE: https://python.langchain.com/docs/concepts/architecture/\n\nLANGUAGE: APIDOC\nCODE:\n```\nHow-to guides:\n  - How to use tools in a chain: /docs/how_to/tools_chain/\n  - How to use a vectorstore as a retriever: /docs/how_to/vectorstore_retriever/\n  - How to add memory to chatbots: /docs/how_to/chatbots_memory/\n  - How to use example selectors: /docs/how_to/example_selectors/\n  - How to add a semantic layer over graph database: /docs/how_to/graph_semantic/\n  - How to invoke runnables in parallel: /docs/how_to/parallel/\n  - How to stream chat model responses: /docs/how_to/chat_streaming/\n  - How to add default invocation args to a Runnable: /docs/how_to/binding/\n  - How to add retrieval to chatbots: /docs/how_to/chatbots_retrieval/\n  - How to use few shot examples in chat models: /docs/how_to/few_shot_examples_chat/\n  - How to do tool/function calling: /docs/how_to/function_calling/\n  - How to install LangChain packages: /docs/how_to/installation/\n  - How to add examples to the prompt for query analysis: /docs/how_to/query_few_shot/\n  - How to use few shot examples: /docs/how_to/few_shot_examples/\n  - How to run custom functions: /docs/how_to/functions/\n  - How to use output parsers to parse an LLM response into structured format: /docs/how_to/output_parser_structured/\n  - How to handle cases where no queries are generated: /docs/how_to/query_no_queries/\n  - How to route between sub-chains: /docs/how_to/routing/\n  - How to return structured data from a model: /docs/how_to/structured_output/\n  - How to summarize text through parallelization: /docs/how_to/summarize_map_reduce/\n```\n\n----------------------------------------\n\nTITLE: OCIModelDeployment Chat Integration\nDESCRIPTION: Helps users get started with OCIModelDeployment chat models.",
    "chunk_length": 3482
  },
  {
    "chunk_id": 163,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/integrations/chat/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.chat_models import ChatOCIModelDeployment\n\n# Example usage (requires OCI credentials)\n# llm = ChatOCIModelDeployment(endpoint=\"your-oci-endpoint\", deployment_id=\"your-deployment-id\")\n# response = llm.invoke(\"Write a Python function to calculate factorial.\")\n# print(response)\n```\n\n----------------------------------------\n\nTITLE: LangChain Tool Calling - Few-Shot Prompting\nDESCRIPTION: Demonstrates how to use few-shot examples within the prompt to guide the LLM on how and when to use tools effectively. This improves the accuracy and reliability of tool calls. SOURCE: https://python.langchain.com/docs/how_to/vectorstore_retriever/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import AgentExecutor, create_tool_calling_agent\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\n@tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather for a city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n@tool\ndef get_time(city: str) -> str:\n    \"\"\"Get the current time for a city.\"\"\"\n    return f\"The time in {city} is 10:00 AM.\"\n\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\ntools = [get_weather, get_time]\n\n# Few-shot examples embedded in the prompt\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    (\"human\", \"What is the weather in Paris?\"),\n    (\"assistant\", \"{\\\"tool\\\": \\\"get_weather\\\", \\\"tool_input\\\": {\\\"city\\\": \\\"Paris\\\"}}\"),\n    (\"human\", \"What time is it in Tokyo?\"),\n    (\"assistant\", \"{\\\"tool\\\": \\\"get_time\\\", \\\"tool_input\\\": {\\\"city\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools. This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/how_to/merge_message_runs/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant.",
    "chunk_length": 2796
  },
  {
    "chunk_id": 164,
    "source": "python_langchain_llms_data",
    "content": "Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: This section provides practical guides on how to perform specific tasks using LangChain. It includes instructions on trimming messages and creating and querying vector stores. SOURCE: https://python.langchain.com/docs/how_to/extraction_examples/\n\nLANGUAGE: markdown\nCODE:\n```\n*   [How to trim messages](/docs/how_to/trim_messages/)\n*   [How to create and query vector stores](/docs/how_to/vectorstores/)\n```\n\n----------------------------------------\n\nTITLE: LangChain Tool Calling - Few-Shot Prompting\nDESCRIPTION: Demonstrates how to use few-shot examples within the prompt to guide the LLM on how and when to use tools effectively. This improves the accuracy and reliability of tool calls. SOURCE: https://python.langchain.com/docs/how_to/output_parser_string/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import AgentExecutor, create_tool_calling_agent\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\n@tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather for a city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n@tool\ndef get_time(city: str) -> str:\n    \"\"\"Get the current time for a city.\"\"\"\n    return f\"The time in {city} is 10:00 AM.\"\n\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\ntools = [get_weather, get_time]\n\n# Few-shot examples embedded in the prompt\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant.",
    "chunk_length": 2260
  },
  {
    "chunk_id": 165,
    "source": "python_langchain_llms_data",
    "content": "Use the available tools.\"),\n    (\"human\", \"What is the weather in Paris?\"),\n    (\"assistant\", \"{\\\"tool\\\": \\\"get_weather\\\", \\\"tool_input\\\": {\\\"city\\\": \\\"Paris\\\"}}\"),\n    (\"human\", \"What time is it in Tokyo?\"),\n    (\"assistant\", \"{\\\"tool\\\": \\\"get_time\\\", \\\"tool_input\\\": {\\\"city\n```\n\n----------------------------------------\n\nTITLE: Data Extraction with Examples\nDESCRIPTION: Demonstrates data extraction using the configured runnable with provided few-shot examples. It invokes the runnable multiple times with the same text to show improved consistency in output. SOURCE: https://python.langchain.com/docs/how_to/extraction_examples/\n\nLANGUAGE: python\nCODE:\n```\nfor _ in range(5):\n    text = \"The solar system is large, but earth has only 1 moon.\"\n    print(runnable.invoke({\"text\": text, \"examples\": messages}))\n```\n\n----------------------------------------\n\nTITLE: LangChain Tool Calling - Few-Shot Prompting\nDESCRIPTION: Demonstrates how to use few-shot examples within the prompt to guide the LLM on how and when to use tools effectively. This improves the accuracy and reliability of tool calls. SOURCE: https://python.langchain.com/docs/how_to/custom_callbacks/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import AgentExecutor, create_tool_calling_agent\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\n@tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather for a city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n@tool\ndef get_time(city: str) -> str:\n    \"\"\"Get the current time for a city.\"\"\"\n    return f\"The time in {city} is 10:00 AM.\"\n\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\ntools = [get_weather, get_time]\n\n# Few-shot examples embedded in the prompt\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    (\"human\", \"What is the weather in Paris?\"),\n    (\"assistant\", \"{\\\"tool\\\": \\\"get_weather\\\", \\\"tool_input\\\": {\\\"city\\\": \\\"Paris\\\"}}\"),\n    (\"human\", \"What time is it in Tokyo?\"),\n    (\"assistant\", \"{\\\"tool\\\": \\\"get_time\\\", \\\"tool_input\\\": {\\\"city\n```\n\n----------------------------------------\n\nTITLE: Migrating from v0.0 Chains\nDESCRIPTION: This guide provides instructions and examples for migrating from v0.0 chains to newer versions of LangChain.",
    "chunk_length": 2363
  },
  {
    "chunk_id": 166,
    "source": "python_langchain_llms_data",
    "content": "It covers specific chain types and their migration paths. SOURCE: https://python.langchain.com/docs/how_to/query_high_cardinality/\n\nLANGUAGE: python\nCODE:\n```\n# Example: Migrating LLMChain\nfrom langchain.chains import LLMChain\nfrom langchain_openai import OpenAI\nfrom langchain.prompts import PromptTemplate\n\n# Old v0.0 style (conceptual)\n# llm = OpenAI(temperature=0)\n# prompt = PromptTemplate(input_variables=[\"topic\"], template=\"Tell me a joke about {topic}\")\n# llm_chain = LLMChain(prompt=prompt, llm=llm)\n\n# New style (conceptual)\n# from langchain_core.runnables import RunnableSequence\n# llm = OpenAI(temperature=0)\n# prompt = PromptTemplate(input_variables=[\"topic\"], template=\"Tell me a joke about {topic}\")\n# llm_chain = prompt | llm\n\n# Specific migration examples for other chains like ConstitutionalChain, ConversationalChain, etc., would follow similar patterns. ```\n\n----------------------------------------\n\nTITLE: LangChain Example Selectors\nDESCRIPTION: Dynamically select and format examples for few-shot prompting to enhance few-shot learning performance. Implements classes for example selection and formatting. SOURCE: https://context7_llms\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.prompts.example_selector import SemanticSimilarityExampleSelector\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import OpenAIEmbeddings\n\n# Example using SemanticSimilarityExampleSelector\nexample_selector = SemanticSimilarityExampleSelector.from_examples(\n    examples, \n    OpenAIEmbeddings(), \n    Chroma, \n    k=1\n)\n\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: Guides on specific functionalities and implementation details within LangChain. SOURCE: https://python.langchain.com/docs/how_to/time_weighted_vectorstore/\n\nLANGUAGE: APIDOC\nCODE:\n```\nHow to use tools in a chain: /docs/how_to/tools_chain/\nHow to use a vectorstore as a retriever: /docs/how_to/vectorstore_retriever/\nHow to add memory to chatbots: /docs/how_to/chatbots_memory/\nHow to use example selectors: /docs/how_to/example_selectors/\nHow to add a semantic layer over graph database: /docs/how_to/graph_semantic/\nHow to invoke runnables in parallel: /docs/how_to/parallel/\nHow to stream chat model responses: /docs/how_to/chat_streaming/\nHow to add default invocation args to a Runnable: /docs/how_to/binding/\nHow to add retrieval to chatbots: /docs/how_to/chatbots_retrieval/\nHow to use few shot examples in chat models: /docs/how_to/few_shot_examples_chat/\nHow to do tool/function calling: /docs/how_to/function_calling/\nHow to install LangChain packages: /docs/how_to/installation/\nHow to add examples to the prompt for query analysis: /docs/how_to/query_few_shot/\nHow to use few shot examples: /docs/how_to/few_shot_examples/\nHow to run custom functions: /docs/how_to/functions/\nHow to use output parsers to parse an LLM response into structured format: /docs/how_to/output_parser_structured/\nHow to handle cases where no queries are generated: /docs/how_to/query_no_queries/\nHow to route between sub-chains: /docs/how_to/routing/\nHow to return structured data from a model: /docs/how_to/structured_output/\n```\n\n----------------------------------------\n\nTITLE: LangChain Tool Calling - Few-Shot Prompting\nDESCRIPTION: Demonstrates how to use few-shot examples within the prompt to guide the LLM on how and when to use tools effectively.",
    "chunk_length": 3380
  },
  {
    "chunk_id": 167,
    "source": "python_langchain_llms_data",
    "content": "This improves the accuracy and reliability of tool calls. SOURCE: https://python.langchain.com/docs/how_to/multi_vector/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import AgentExecutor, create_tool_calling_agent\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\n@tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather for a city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n@tool\ndef get_time(city: str) -> str:\n    \"\"\"Get the current time for a city.\"\"\"\n    return f\"The time in {city} is 10:00 AM.\"\n\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\ntools = [get_weather, get_time]\n\n# Few-shot examples embedded in the prompt\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    (\"human\", \"What is the weather in Paris?\"),\n    (\"assistant\", \"{\\\"tool\\\": \\\"get_weather\\\", \\\"tool_input\\\": {\\\"city\\\": \\\"Paris\\\"}}\"),\n    (\"human\", \"What time is it in Tokyo?\"),\n    (\"assistant\", \"{\\\"tool\\\": \\\"get_time\\\", \\\"tool_input\\\": {\\\"city\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools. This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/how_to/extraction_parse/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant.",
    "chunk_length": 2165
  },
  {
    "chunk_id": 168,
    "source": "python_langchain_llms_data",
    "content": "Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: Example Data Document Loader\nDESCRIPTION: Loads example data for testing and demonstration. This loader provides sample documents. SOURCE: https://python.langchain.com/docs/integrations/chat/google_vertex_ai_palm/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.document_loaders import ExampleLoader\n\nloader = ExampleLoader(file_path=\"path/to/example_data.txt\")\ndocuments = loader.load()\n```\n\n----------------------------------------\n\nTITLE: Setup and Database Connection\nDESCRIPTION: Installs necessary packages, sets up environment variables (optional for LangSmith), and connects to a SQLite Chinook database using SQLDatabase. It then prints the dialect, usable table names, and a sample query result. SOURCE: https://python.langchain.com/docs/how_to/sql_prompting/\n\nLANGUAGE: python\nCODE:\n```\n%pip install --upgrade --quiet  langchain langchain-community langchain-experimental langchain-openai\n\n# Uncomment the below to use LangSmith. Not required. # import getpass\n# import os\n# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n\nfrom langchain_community.utilities import SQLDatabasedb = SQLDatabase.from_uri(\"sqlite:///Chinook.db\", sample_rows_in_table_info=3)\nprint(db.dialect)\nprint(db.get_usable_table_names())\nprint(db.run(\"SELECT * FROM Artist LIMIT 10;\"))\n```\n\n----------------------------------------\n\nTITLE: Dynamic Few-Shot Example Selection with FAISS\nDESCRIPTION: Demonstrates how to use SemanticSimilarityExampleSelector with FAISS and OpenAIEmbeddings to dynamically select relevant examples for SQL query generation.",
    "chunk_length": 2333
  },
  {
    "chunk_id": 169,
    "source": "python_langchain_llms_data",
    "content": "This helps in providing contextually appropriate few-shot examples to the language model. SOURCE: https://python.langchain.com/docs/how_to/sql_prompting/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.example_selectors import SemanticSimilarityExampleSelector\nfrom langchain_openai import OpenAIEmbeddings\n\n# Assuming 'examples' is a list of dictionaries with 'input' and 'query' keys\n# examples = [\n#     {'input': 'List all artists.', 'query': 'SELECT * FROM Artist;'},\n#     {'input': 'How many employees are there', 'query': 'SELECT COUNT(*) FROM \"Employee\"'},\n#     ... # ]\n\nexample_selector = SemanticSimilarityExampleSelector.from_examples(\n    examples,\n    OpenAIEmbeddings(),\n    FAISS,\n    k=5,\n    input_keys=[\"input\"],\n)\n\n# Example usage of the selector:\n# selected_examples = example_selector.select_examples({\"input\": \"how many artists are there?\"})\n# print(selected_examples)\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools. This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/how_to/runnable_runtime_secrets/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools.",
    "chunk_length": 2903
  },
  {
    "chunk_id": 170,
    "source": "python_langchain_llms_data",
    "content": "This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/how_to/query_multiple_retrievers/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: LangChain Tool Calling - Few-Shot Prompting\nDESCRIPTION: Demonstrates how to use few-shot examples within the prompt to guide the LLM on how and when to use tools effectively. This improves the accuracy and reliability of tool calls. SOURCE: https://python.langchain.com/docs/how_to/sql_large_db/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import AgentExecutor, create_tool_calling_agent\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\n@tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather for a city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n@tool\ndef get_time(city: str) -> str:\n    \"\"\"Get the current time for a city.\"\"\"\n    return f\"The time in {city} is 10:00 AM.\"\n\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\ntools = [get_weather, get_time]\n\n# Few-shot examples embedded in the prompt\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant.",
    "chunk_length": 2609
  },
  {
    "chunk_id": 171,
    "source": "python_langchain_llms_data",
    "content": "Use the available tools.\"),\n    (\"human\", \"What is the weather in Paris?\"),\n    (\"assistant\", \"{\\\"tool\\\": \\\"get_weather\\\", \\\"tool_input\\\": {\\\"city\\\": \\\"Paris\\\"}}\"),\n    (\"human\", \"What time is it in Tokyo?\"),\n    (\"assistant\", \"{\\\"tool\\\": \\\"get_time\\\", \\\"tool_input\\\": {\\\"city\n```\n\n----------------------------------------\n\nTITLE: Example Selectors\nDESCRIPTION: Demonstrates various strategies for selecting examples, including using LangSmith datasets, selecting by length, maximal marginal relevance (MMR), n-gram overlap, and similarity. SOURCE: https://python.langchain.com/docs/how_to/callbacks_async/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.prompts.example_selector import SemanticSimilarityExampleSelector\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_core.example_selectors import LengthBasedExampleSelector\n\n# Example for selecting examples by length\n# example_selector = LengthBasedExampleSelector(\n#     examples=my_examples, \n#     example_prompt=example_prompt, \n#     max_length=100\n# )\n\n# Example for selecting examples by similarity\n# example_selector = SemanticSimilarityExampleSelector(\n#     vectorstore=Chroma.from_documents(my_examples, OpenAIEmbeddings()),\n#     k=2,\n#     example_prompt=example_prompt,\n#     input_keys=[\"input\"],\n# )\n\n```\n\n----------------------------------------\n\nTITLE: Qwen QwQ Chat Integration\nDESCRIPTION: Helps users get started with QwQ chat models. SOURCE: https://python.langchain.com/docs/integrations/chat/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.chat_models import ChatQwQ\n\n# Example usage (requires QwQ API key)\n# llm = ChatQwQ(api_key=\"YOUR_QWQ_API_KEY\")\n# response = llm.invoke(\"Write a haiku about nature.\")\n# print(response)\n```\n\n----------------------------------------\n\nTITLE: LangChain Version Migration Guides\nDESCRIPTION: Guides for migrating between different versions of LangChain, focusing on chain and memory upgrades. SOURCE: https://python.langchain.com/docs/how_to/sql_prompting/\n\nLANGUAGE: python\nCODE:\n```\n# Version migration links:\n# Pydantic compatibility: /docs/how_to/pydantic_compatibility/\n# Migrating from v0.0 chains: /docs/versions/migrating_chains/\n#   - ConstitutionalChain: /docs/versions/migrating_chains/constitutional_chain/\n#   - ConversationalChain: /docs/versions/migrating_chains/conversation_chain/\n#   - ConversationalRetrievalChain: /docs/versions/migrating_chains/conversation_retrieval_chain/\n#   - LLMChain: /docs/versions/migrating_chains/llm_chain/\n#   - LLMMathChain: /docs/versions/migrating_chains/llm_math_chain/\n#   - LLMRouterChain: /docs/versions/migrating_chains/llm_router_chain/\n#   - MapReduceDocumentsChain: /docs/versions/migrating_chains/map_reduce_chain/\n#   - MapRerankDocumentsChain: /docs/versions/migrating_chains/map_rerank_docs_chain/\n#   - MultiPromptChain: /docs/versions/migrating_chains/multi_prompt_chain/\n#   - RefineDocumentsChain: /docs/versions/migrating_chains/refine_docs_chain/\n#   - RetrievalQA: /docs/versions/migrating_chains/retrieval_qa/\n#   - StuffDocumentsChain: /docs/versions/migrating_chains/stuff_docs_chain/\n# Upgrading to LangGraph memory: /docs/versions/migrating_memory/\n#   - BaseChatMessageHistory with LangGraph: /docs/versions/migrating_memory/chat_history/\n#   - ConversationBufferMemory or ConversationStringBufferMemory: /docs/versions/migrating_memory/conversation_buffer_memory/\n#   - ConversationBufferWindowMemory or ConversationTokenBufferMemory: /docs/versions/migrating_memory/conversation_buffer_window_memory/\n#   - ConversationSummaryMemory or ConversationSummaryBufferMemory: /docs/versions/migrating_memory/conversation_summary_memory/\n#   - Long-Term Memory Agent: /docs/versions/migrating_memory/long_term_memory_agent/\n# Release policy: /docs/versions/release_policy/\n```\n\n----------------------------------------\n\nTITLE: Langchain LLM Text Generation Example\nDESCRIPTION: Demonstrates how to use Langchain to generate text using a specified LLM.",
    "chunk_length": 3999
  },
  {
    "chunk_id": 172,
    "source": "python_langchain_llms_data",
    "content": "This snippet shows the basic setup and usage for text generation. SOURCE: https://python.langchain.com/docs/tutorials/summarization/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.llms import OpenAI\n\n# Initialize the LLM\nllm = OpenAI(model_name=\"text-davinci-003\", temperature=0.7)\n\n# Generate text\nprompt = \"Write a short story about a robot learning to love.\"\nresponse = llm(prompt)\n\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: LangChain Tool Calling - Few-Shot Prompting\nDESCRIPTION: Demonstrates how to use few-shot examples within the prompt to guide the LLM on how and when to use tools effectively. This improves the accuracy and reliability of tool calls. SOURCE: https://python.langchain.com/docs/how_to/query_high_cardinality/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import AgentExecutor, create_tool_calling_agent\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\n@tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather for a city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n@tool\ndef get_time(city: str) -> str:\n    \"\"\"Get the current time for a city.\"\"\"\n    return f\"The time in {city} is 10:00 AM.\"\n\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\ntools = [get_weather, get_time]\n\n# Few-shot examples embedded in the prompt\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    (\"human\", \"What is the weather in Paris?\"),\n    (\"assistant\", \"{\\\"tool\\\": \\\"get_weather\\\", \\\"tool_input\\\": {\\\"city\\\": \\\"Paris\\\"}}\"),\n    (\"human\", \"What time is it in Tokyo?\"),\n    (\"assistant\", \"{\\\"tool\\\": \\\"get_time\\\", \\\"tool_input\\\": {\\\"city\n```\n\n----------------------------------------\n\nTITLE: Reka Chat Integration\nDESCRIPTION: Provides a quick overview for getting started with Reka chat models. SOURCE: https://python.langchain.com/docs/integrations/chat/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.chat_models import ChatReka\n\n# Example usage (requires Reka API key)\n# llm = ChatReka(api_key=\"YOUR_REKA_API_KEY\")\n# response = llm.invoke(\"Explain the concept of quantum entanglement.\")\n# print(response)\n```\n\n----------------------------------------\n\nTITLE: LangChain Tutorials Overview\nDESCRIPTION: This section lists various tutorials available for LangChain, covering a wide range of applications from simple LLM chains to complex agents and RAG systems.",
    "chunk_length": 2482
  },
  {
    "chunk_id": 173,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/how_to/qa_citations/\n\nLANGUAGE: markdown\nCODE:\n```\n*   [Build a Question Answering application over a Graph Database](/docs/tutorials/graph/)\n*   [Build a simple LLM application with chat models and prompt templates](/docs/tutorials/llm_chain/)\n*   [Build a Chatbot](/docs/tutorials/chatbot/)\n*   [Build a Retrieval Augmented Generation (RAG) App: Part 2](/docs/tutorials/qa_chat_history/)\n*   [Build an Extraction Chain](/docs/tutorials/extraction/)\n*   [Build an Agent](/docs/tutorials/agents/)\n*   [Tagging](/docs/tutorials/classification/)\n*   [Build a Retrieval Augmented Generation (RAG) App: Part 1](/docs/tutorials/rag/)\n*   [Build a semantic search engine](/docs/tutorials/retrievers/)\n*   [Build a Question/Answering system over SQL data](/docs/tutorials/sql_qa/)\n*   [Summarize Text](/docs/tutorials/summarization/)\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: Practical guides on implementing specific features and functionalities within LangChain. SOURCE: https://python.langchain.com/docs/how_to/markdown_header_metadata_splitter/\n\nLANGUAGE: python\nCODE:\n```\n# How-to: Use tools in a chain\n# Explains how to integrate external tools into LangChain chains. # How-to: Use a vectorstore as a retriever\n# Details on configuring and using vectorstores for retrieval. # How-to: Add memory to chatbots\n# Demonstrates methods for incorporating memory into chatbot applications. # How-to: Use example selectors\n# Covers the usage of example selectors for prompt engineering. # How-to: Add a semantic layer over graph database\n# Guides on building semantic interfaces for graph databases. # How-to: Invoke runnables in parallel\n# Shows how to execute LangChain runnables concurrently. # How-to: Stream chat model responses\n# Details on implementing streaming for chat model outputs. # How-to: Add default invocation args to a Runnable\n# Explains how to set default arguments for runnables. # How-to: Add retrieval to chatbots\n# Covers integrating retrieval mechanisms into chatbots.",
    "chunk_length": 2074
  },
  {
    "chunk_id": 174,
    "source": "python_langchain_llms_data",
    "content": "# How-to: Use few shot examples in chat models\n# Demonstrates using few-shot examples with chat models. # How-to: Do tool/function calling\n# Explains how to implement tool and function calling capabilities. # How-to: Install LangChain packages\n# Provides instructions for installing necessary LangChain packages. # How-to: Add examples to the prompt for query analysis\n# Shows how to enhance query analysis with few-shot examples. # How-to: Use few shot examples\n# General guide on utilizing few-shot examples in prompts. # How-to: Run custom functions\n# Details on executing custom functions within LangChain workflows. # How-to: Use output parsers to parse an LLM response into structured format\n# Covers parsing LLM outputs into structured data. # How-to: Handle cases where no queries are generated\n# Strategies for managing scenarios with no generated queries. # How-to: Route between sub-chains\n# Explains how to implement routing logic between different chains. # How-to: Return structured data from a model\n# Guides on configuring models to return structured output. ```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools. This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/how_to/output_parser_fixing/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant.",
    "chunk_length": 2130
  },
  {
    "chunk_id": 175,
    "source": "python_langchain_llms_data",
    "content": "Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: Guides that explain how to implement specific features and functionalities within LangChain, such as adding memory to chatbots or using vectorstores as retrievers. SOURCE: https://python.langchain.com/docs/how_to/output_parser_string/\n\nLANGUAGE: APIDOC\nCODE:\n```\nHow-to Guides:\n  /docs/how_to/\n\nThis section provides detailed instructions on implementing specific functionalities and patterns within LangChain. These guides are useful for understanding advanced concepts and customizing your LLM applications. Key How-to Guides:\n- How to use tools in a chain\n- How to use a vectorstore as a retriever\n- How to add memory to chatbots\n- How to use example selectors\n- How to add a semantic layer over graph database\n- How to invoke runnables in parallel\n- How to stream chat model responses\n- How to add default invocation args to a Runnable\n- How to add retrieval to chatbots\n- How to use few shot examples in chat models\n- How to do tool/function calling\n- How to install LangChain packages\n- How to add examples to the prompt for query analysis\n- How to use few shot examples\n- How to run custom functions\n- How to use output parsers to parse an LLM response into structured format\n- How to handle cases where no queries are generated\n- How to route between sub-chains\n- How to return structured data from a model\n\nThese guides offer practical solutions to common challenges encountered when building LLM applications.",
    "chunk_length": 2201
  },
  {
    "chunk_id": 176,
    "source": "python_langchain_llms_data",
    "content": "```\n\n----------------------------------------\n\nTITLE: LangChain Tutorials\nDESCRIPTION: A collection of tutorials demonstrating how to build various LLM applications using LangChain. SOURCE: https://python.langchain.com/docs/how_to/multimodal_inputs/\n\nLANGUAGE: APIDOC\nCODE:\n```\nTutorials:\n  - Build a Question Answering application over a Graph Database\n    Description: Guide on creating QA systems that query graph databases. Path: /docs/tutorials/graph/\n  - Build a simple LLM application with chat models and prompt templates\n    Description: Introduces basic LLM application development using chat models and prompt templates. Path: /docs/tutorials/llm_chain/\n  - Build a Chatbot\n    Description: Steps to create a conversational chatbot application. Path: /docs/tutorials/chatbot/\n  - Build a Retrieval Augmented Generation (RAG) App: Part 2\n    Description: Continues the RAG application development, focusing on advanced features. Path: /docs/tutorials/qa_chat_history/\n  - Build an Extraction Chain\n    Description: Demonstrates how to extract structured information using LangChain chains. Path: /docs/tutorials/extraction/\n  - Build an Agent\n    Description: Guide to building intelligent agents that can use tools. Path: /docs/tutorials/agents/\n  - Tagging\n    Description: Covers classification and tagging functionalities. Path: /docs/tutorials/classification/\n  - Build a Retrieval Augmented Generation (RAG) App: Part 1\n    Description: Introduces the fundamentals of building RAG applications. Path: /docs/tutorials/rag/\n  - Build a semantic search engine\n    Description: How to implement semantic search capabilities. Path: /docs/tutorials/retrievers/\n  - Build a Question/Answering system over SQL data\n    Description: Guide for creating QA systems that interact with SQL databases. Path: /docs/tutorials/sql_qa/\n  - Summarize Text\n    Description: Demonstrates text summarization techniques. Path: /docs/tutorials/summarization/\n```\n\n----------------------------------------\n\nTITLE: NVIDIA AI Endpoints Chat Integration\nDESCRIPTION: Facilitates getting started with NVIDIA chat models.",
    "chunk_length": 2107
  },
  {
    "chunk_id": 177,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/integrations/chat/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_nvidia_ai_endpoints.chat_models import ChatNVIDIA\n\n# Example usage (requires NVIDIA API key)\n# llm = ChatNVIDIA(api_key=\"YOUR_NVIDIA_API_KEY\", model=\"ai-endpoints/llama2-13b\")\n# response = llm.invoke(\"Describe the process of photosynthesis.\")\n# print(response)\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: A collection of practical guides on implementing various features within LangChain, such as using tools, vectorstores, memory, and handling different output formats. SOURCE: https://python.langchain.com/docs/tutorials/graph/\n\nLANGUAGE: APIDOC\nCODE:\n```\nLangChain How-to Guides:\n\nTools:\n  - How to use tools in a chain: Integrates external tools (like search engines or calculators) into LangChain workflows. Vectorstores:\n  - How to use a vectorstore as a retriever: Configures and utilizes vector databases for efficient similarity search and retrieval. Memory:\n  - How to add memory to chatbots: Implements conversational memory to maintain context in chatbot applications. Example Selectors:\n  - How to use example selectors: Manages and selects few-shot examples for LLM prompts. Graph Databases:\n  - How to add a semantic layer over graph database: Connects LLMs to graph databases for semantic querying. Parallel Invocation:\n  - How to invoke runnables in parallel: Executes multiple LangChain components concurrently for improved performance. Streaming:\n  - How to stream chat model responses: Enables real-time streaming of responses from chat models. Binding Arguments:\n  - How to add default invocation args to a Runnable: Sets default parameters for runnable invocations. Retrieval for Chatbots:\n  - How to add retrieval to chatbots: Integrates retrieval mechanisms into chatbot functionalities. Few-Shot Examples:\n  - How to use few shot examples in chat models: Provides examples to LLMs for better response generation. - How to use few shot examples: General guide on using few-shot learning.",
    "chunk_length": 2062
  },
  {
    "chunk_id": 178,
    "source": "python_langchain_llms_data",
    "content": "- How to add examples to the prompt for query analysis: Structures prompts with examples for specific analytical tasks. Function Calling:\n  - How to do tool/function calling: Implements LLM-based function calling for structured output and actions. Installation:\n  - How to install LangChain packages: Instructions for installing the necessary LangChain libraries. Custom Functions:\n  - How to run custom functions: Executes user-defined functions within LangChain workflows. Output Parsers:\n  - How to use output parsers to parse an LLM response into structured format: Extracts and structures data from LLM outputs. - How to return structured data from a model: Ensures LLM outputs conform to a predefined structure. Routing:\n  - How to route between sub-chains: Directs execution flow between different chains based on conditions. Query Handling:\n  - How to handle cases where no queries are generated: Manages scenarios where LLMs fail to produce relevant queries. ```\n\n----------------------------------------\n\nTITLE: Example Data Document Loader\nDESCRIPTION: Loads example data for testing and demonstration. This loader provides sample documents. SOURCE: https://python.langchain.com/docs/integrations/chat/bedrock/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.document_loaders import ExampleLoader\n\nloader = ExampleLoader(file_path=\"path/to/example_data.txt\")\ndocuments = loader.load()\n```\n\n----------------------------------------\n\nTITLE: LangChain Tool Calling - Few-Shot Prompting\nDESCRIPTION: Demonstrates how to use few-shot examples within the prompt to guide the LLM on how and when to use tools effectively. This improves the accuracy and reliability of tool calls. SOURCE: https://python.langchain.com/docs/how_to/indexing/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import AgentExecutor, create_tool_calling_agent\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\n@tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather for a city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n@tool\ndef get_time(city: str) -> str:\n    \"\"\"Get the current time for a city.\"\"\"\n    return f\"The time in {city} is 10:00 AM.\"\n\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\ntools = [get_weather, get_time]\n\n# Few-shot examples embedded in the prompt\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant.",
    "chunk_length": 2463
  },
  {
    "chunk_id": 179,
    "source": "python_langchain_llms_data",
    "content": "Use the available tools.\"),\n    (\"human\", \"What is the weather in Paris?\"),\n    (\"assistant\", \"{\\\"tool\\\": \\\"get_weather\\\", \\\"tool_input\\\": {\\\"city\\\": \\\"Paris\\\"}}\"),\n    (\"human\", \"What time is it in Tokyo?\"),\n    (\"assistant\", \"{\\\"tool\\\": \\\"get_time\\\", \\\"tool_input\\\": {\\\"city\n```\n\n----------------------------------------\n\nTITLE: OCIGenAI Chat Integration\nDESCRIPTION: Provides a quick overview for getting started with OCIGenAI chat models. SOURCE: https://python.langchain.com/docs/integrations/chat/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.chat_models import ChatOCIGenAI\n\n# Example usage (requires OCI credentials)\n# llm = ChatOCIGenAI(compartment_id=\"your-compartment-id\", model_id=\"your-model-id\")\n# response = llm.invoke(\"Explain the theory of relativity.\")\n# print(response)\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools. This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/how_to/custom_chat_model/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: Naver Chat Integration\nDESCRIPTION: Provides a quick overview for getting started with Naver chat models.",
    "chunk_length": 2700
  },
  {
    "chunk_id": 180,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/integrations/chat/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.chat_models import ChatNaver\n\n# Example usage (requires Naver API key)\n# llm = ChatNaver(api_key=\"YOUR_NAVER_API_KEY\")\n# response = llm.invoke(\"Translate 'hello' to Korean.\")\n# print(response)\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: This section provides practical guides on implementing specific features and functionalities within LangChain, such as using tools, vectorstores, memory, parallel execution, streaming, and function calling. SOURCE: https://python.langchain.com/docs/how_to/fallbacks/\n\nLANGUAGE: python\nCODE:\n```\nHow to use tools in a chain\nHow to use a vectorstore as a retriever\nHow to add memory to chatbots\nHow to use example selectors\nHow to add a semantic layer over graph database\nHow to invoke runnables in parallel\nHow to stream chat model responses\nHow to add default invocation args to a Runnable\nHow to add retrieval to chatbots\nHow to use few shot examples in chat models\nHow to do tool/function calling\nHow to install LangChain packages\nHow to add examples to the prompt for query analysis\nHow to use few shot examples\nHow to run custom functions\nHow to use output parsers to parse an LLM response into structured format\nHow to handle cases where no queries are generated\nHow to route between sub-chains\nHow to return structured data from a model\n```\n\n----------------------------------------\n\nTITLE: Invoke Chain with Examples\nDESCRIPTION: Shows how to invoke a LangChain chain that has been configured with few-shot examples. The input is processed by the chain, leveraging the provided examples for better context and output generation. SOURCE: https://python.langchain.com/docs/how_to/query_few_shot/\n\nLANGUAGE: python\nCODE:\n```\nquery_analyzer_with_examples.invoke(\"what's the difference between web voyager and reflection agents? do both use langgraph?\")\n```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: Guides on specific tasks within LangChain, including how to trim messages and how to create and query vector stores.",
    "chunk_length": 2161
  },
  {
    "chunk_id": 181,
    "source": "python_langchain_llms_data",
    "content": "SOURCE: https://python.langchain.com/docs/how_to/query_few_shot/\n\n\n\n----------------------------------------\n\nTITLE: LangChain Tool Calling - Few-Shot Prompting\nDESCRIPTION: Demonstrates how to use few-shot examples within the prompt to guide the LLM on how and when to use tools effectively. This improves the accuracy and reliability of tool calls. SOURCE: https://python.langchain.com/docs/how_to/tool_calling/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import AgentExecutor, create_tool_calling_agent\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\n@tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather for a city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n@tool\ndef get_time(city: str) -> str:\n    \"\"\"Get the current time for a city.\"\"\"\n    return f\"The time in {city} is 10:00 AM.\"\n\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\ntools = [get_weather, get_time]\n\n# Few-shot examples embedded in the prompt\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    (\"human\", \"What is the weather in Paris?\"),\n    (\"assistant\", \"{\\\"tool\\\": \\\"get_weather\\\", \\\"tool_input\\\": {\\\"city\\\": \\\"Paris\\\"}}\"),\n    (\"human\", \"What time is it in Tokyo?\"),\n    (\"assistant\", \"{\\\"tool\\\": \\\"get_time\\\", \\\"tool_input\\\": {\\\"city\n```\n\n----------------------------------------\n\nTITLE: Together AI Chat Integration\nDESCRIPTION: Helps users get started with Together AI chat models. SOURCE: https://python.langchain.com/docs/integrations/chat/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.chat_models import ChatTogetherAI\n\n# Example usage (requires Together AI API key)\n# llm = ChatTogetherAI(api_key=\"YOUR_TOGETHERAI_API_KEY\")\n# response = llm.invoke(\"Write a poem about the sea.\")\n# print(response)\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools.",
    "chunk_length": 2092
  },
  {
    "chunk_id": 182,
    "source": "python_langchain_llms_data",
    "content": "This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/how_to/agent_executor/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools. This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/how_to/chat_model_caching/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant.",
    "chunk_length": 2585
  },
  {
    "chunk_id": 183,
    "source": "python_langchain_llms_data",
    "content": "Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: LangChain Tool Calling - Few-Shot Prompting\nDESCRIPTION: Demonstrates how to use few-shot examples within the prompt to guide the LLM on how and when to use tools effectively. This improves the accuracy and reliability of tool calls. SOURCE: https://python.langchain.com/docs/how_to/tools_prompting/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import AgentExecutor, create_tool_calling_agent\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\n@tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather for a city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n@tool\ndef get_time(city: str) -> str:\n    \"\"\"Get the current time for a city.\"\"\"\n    return f\"The time in {city} is 10:00 AM.\"\n\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\ntools = [get_weather, get_time]\n\n# Few-shot examples embedded in the prompt\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    (\"human\", \"What is the weather in Paris?\"),\n    (\"assistant\", \"{\\\"tool\\\": \\\"get_weather\\\", \\\"tool_input\\\": {\\\"city\\\": \\\"Paris\\\"}}\"),\n    (\"human\", \"What time is it in Tokyo?\"),\n    (\"assistant\", \"{\\\"tool\\\": \\\"get_time\\\", \\\"tool_input\\\": {\\\"city\n```\n\n----------------------------------------\n\nTITLE: Use Few-Shot Prompting with Tool Calling\nDESCRIPTION: Demonstrates how to use few-shot examples within prompts to guide an LLM in correctly calling tools.",
    "chunk_length": 2246
  },
  {
    "chunk_id": 184,
    "source": "python_langchain_llms_data",
    "content": "This improves the accuracy and reliability of tool usage. SOURCE: https://python.langchain.com/docs/how_to/chat_token_usage_tracking/\n\nLANGUAGE: python\nCODE:\n```\n# Example of few-shot prompting with tool calling:\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\n# Define a tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a given city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n# Convert the tool to a format compatible with OpenAI's function calling\nweather_tool = convert_to_openai_tool(get_weather)\n\n# Create a prompt with few-shot examples\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    # Few-shot example 1\n    HumanMessage(\"What's the weather like in San Francisco?\"),\n    AIMessage(\"\", tool_calls=[{\"name\": \"get_weather\", \"arguments\": {\"city\": \"San Francisco\"}, \"id\": \"call_abc\"}])\n    # Few-shot example 2 (optional)\n    # HumanMessage(\"Tell me about Paris.\"),\n    # AIMessage(\"\", tool_calls=[...])\n    # The actual user query\n    (\"human\", \"{query}\")\n])\n\n# Initialize the model with tool calling capabilities\nmodel = ChatOpenAI(model=\"gpt-4\", tools=[weather_tool])\n\n# Create the chain\nchain = prompt | model\n\n# Example query\n# query = \"What is the weather in London?\"\n# response = chain.invoke({\"query\": query})\n# print(response.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: Using Toolkits\nDESCRIPTION: Provides guidance on how to leverage pre-built toolkits within Langchain for various tasks. Toolkits bundle related tools and chains to simplify complex workflows. SOURCE: https://python.langchain.com/docs/how_to/example_selectors_similarity/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.agents import load_tools\nfrom langchain.llms import OpenAI\n\nllm = OpenAI(temperature=0)\n# Load tools for a specific toolkit, e.g., 'python-repl'\ntools = load_tools([\"python-repl\"], llm=llm)\n# Further steps to use these tools with an agent...",
    "chunk_length": 2152
  },
  {
    "chunk_id": 185,
    "source": "python_langchain_llms_data",
    "content": "```\n\n----------------------------------------\n\nTITLE: LangChain How-to Guides\nDESCRIPTION: A collection of how-to guides for implementing specific features and functionalities in LangChain. SOURCE: https://python.langchain.com/docs/concepts/document_loaders/\n\nLANGUAGE: MARKDOWN\nCODE:\n```\nHow-to guides\n    *   [How-to guides](/docs/how_to/)\n    *   [How to use tools in a chain](/docs/how_to/tools_chain/)\n    *   [How to use a vectorstore as a retriever](/docs/how_to/vectorstore_retriever/)\n    *   [How to add memory to chatbots](/docs/how_to/chatbots_memory/)\n    *   [How to use example selectors](/docs/how_to/example_selectors/)\n    *   [How to add a semantic layer over graph database](/docs/how_to/graph_semantic/)\n    *   [How to invoke runnables in parallel](/docs/how_to/parallel/)\n    *   [How to stream chat model responses](/docs/how_to/chat_streaming/)\n    *   [How to add default invocation args to a Runnable](/docs/how_to/binding/)\n    *   [How to add retrieval to chatbots](/docs/how_to/chatbots_retrieval/)\n    *   [How to use few shot examples in chat models](/docs/how_to/few_shot_examples_chat/)\n    *   [How to do tool/function calling](/docs/how_to/function_calling/)\n    *   [How to install LangChain packages](/docs/how_to/installation/)\n    *   [How to add examples to the prompt for query analysis](/docs/how_to/query_few_shot/)\n    *   [How to use few shot examples](/docs/how_to/few_shot_examples/)\n    *   [How to run custom functions](/docs/how_to/functions/)\n    *   [How to use output parsers to parse an LLM response into structured format](/docs/how_to/output_parser_structured/)\n    *   [How to handle cases where no queries are generated](/docs/how_to/query_no_queries/)\n    *   [How to route between sub-chains](/docs/how_to/routing/)\n    *   [How to return structured data from a model](/docs/how_to/structured_output/)\n    *   [How to summarize text through parallelization](/docs/how_to/summarize_map_reduce/)\n```\n\n----------------------------------------\n\nTITLE: Select Examples with Langchain (LangSmith, Length, MMR, N-gram, Similarity)\nDESCRIPTION: Demonstrates how to select examples using Langchain, including selecting examples from a LangSmith dataset, by length, by maximal marginal relevance (MMR), by n-gram overlap, and by similarity.",
    "chunk_length": 2291
  },
  {
    "chunk_id": 186,
    "source": "python_langchain_llms_data",
    "content": "These examples showcase different strategies for example selection, allowing developers to optimize the performance of their LLM applications. The examples cover various selection criteria and techniques. SOURCE: https://python.langchain.com/docs/how_to/multimodal_prompts/\n\nLANGUAGE: Python\nCODE:\n```\n# Placeholder for code examples for selecting examples with Langchain. # Actual code examples would be placed here. ```\n\n----------------------------------------\n\nTITLE: LangChain Google Generative AI Setup\nDESCRIPTION: Installs the LangChain Google Generative AI integration package, enabling the use of Google's Gemini models within LangChain applications. SOURCE: https://python.langchain.com/docs/how_to/sql_csv/\n\nLANGUAGE: python\nCODE:\n```\npip install -qU \"langchain[google-genai]\"\n```\n\n----------------------------------------\n\nTITLE: LangChain Tool Calling - Few-Shot Prompting\nDESCRIPTION: Demonstrates how to use few-shot examples within the prompt to guide the LLM on how and when to use tools effectively. This improves the accuracy and reliability of tool calls. SOURCE: https://python.langchain.com/docs/how_to/multimodal_inputs/\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import AgentExecutor, create_tool_calling_agent\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\n@tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather for a city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n@tool\ndef get_time(city: str) -> str:\n    \"\"\"Get the current time for a city.\"\"\"\n    return f\"The time in {city} is 10:00 AM.\"\n\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\ntools = [get_weather, get_time]\n\n# Few-shot examples embedded in the prompt\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Use the available tools.\"),\n    (\"human\", \"What is the weather in Paris?\"),\n    (\"assistant\", \"{\\\"tool\\\": \\\"get_weather\\\", \\\"tool_input\\\": {\\\"city\\\": \\\"Paris\\\"}}\"),\n    (\"human\", \"What time is it in Tokyo?\"),\n    (\"assistant\", \"{\\\"tool\\\": \\\"get_time\\\", \\\"tool_input\\\": {\\\"city\n```",
    "chunk_length": 2141
  }
]
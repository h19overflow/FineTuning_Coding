[
  {
    "chunk_id": 1,
    "source": "pydantic_ai_llms_data",
    "content": "========================\nCODE SNIPPETS\n========================\nTITLE: Run Pydantic AI Example with Zero Setup using uv\nDESCRIPTION: This advanced one-liner command uses `uv` to run a Pydantic AI example (`pydantic_model`) by setting the OpenAI API key and installing dependencies on the fly. It's ideal for quick testing without prior setup. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/examples/index.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nOPENAI_API_KEY='your-api-key' \\\n  uv run --with \"pydantic-ai[examples]\" \\\n  -m pydantic_ai_examples.pydantic_model\n```\n\n----------------------------------------\n\nTITLE: Install Pydantic AI with examples\nDESCRIPTION: Installs the `pydantic-ai-examples` package via the `examples` optional group. This makes it easy to customize and run the provided Pydantic AI examples. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/install.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip/uv-add \"pydantic-ai[examples]\"\n```\n\n----------------------------------------\n\nTITLE: Install Pydantic AI Example Dependencies\nDESCRIPTION: Commands to install the necessary extra dependencies for running Pydantic AI examples. This includes the `examples` optional dependency group, which can be installed via `pip` or `uv` for PyPI installations, or `uv sync` if cloning the repository. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/examples/index.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip/uv-add \"pydantic-ai[examples]\"\n```\n\nLANGUAGE: bash\nCODE:\n```\nuv sync --extra examples\n```\n\n----------------------------------------\n\nTITLE: Copy Pydantic AI Examples to Local Directory\nDESCRIPTION: This command copies the Pydantic AI example files to a specified local directory (e.g., `examples/`). This allows users to modify, experiment with, and develop upon the examples without affecting the installed package. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/examples/index.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython/uv-run -m pydantic_ai_examples --copy-to examples/\n```\n\n----------------------------------------\n\nTITLE: Perform slim installation for Pydantic AI with multiple optional groups\nDESCRIPTION: Demonstrates how to install `pydantic-ai-slim` with multiple optional groups simultaneously.",
    "chunk_length": 2279
  },
  {
    "chunk_id": 2,
    "source": "pydantic_ai_llms_data",
    "content": "This allows for including dependencies for several specific models and features in a single installation command. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/install.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip/uv-add \"pydantic-ai-slim[openai,vertexai,logfire]\"\n```\n\n----------------------------------------\n\nTITLE: Run Pydantic AI Flight Booking Example (Bash)\nDESCRIPTION: Command to execute the Pydantic AI flight booking example. This command assumes that all necessary dependencies are installed and environment variables are properly configured as per the project's setup instructions. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/examples/flight-booking.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython/uv-run -m pydantic_ai_examples.flight_booking\n```\n\n----------------------------------------\n\nTITLE: Run Pydantic AI Bank Support Agent Example\nDESCRIPTION: Command to execute the Pydantic AI bank support agent example. This requires prior installation of dependencies and setting up environment variables as per the project's usage instructions. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/examples/bank-support.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython/uv-run -m pydantic_ai_examples.bank_support\n```\n\n----------------------------------------\n\nTITLE: Run Pydantic AI Data Analyst Example Script\nDESCRIPTION: Execute the Pydantic AI data analyst example script from the command line. This command uses `python/uv-run` to launch the `pydantic_ai_examples.data_analyst` module, assuming necessary dependencies are installed and environment variables are configured as per the project's setup instructions. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/examples/data-analyst.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython/uv-run -m pydantic_ai_examples.data_analyst\n```\n\n----------------------------------------\n\nTITLE: Run Specific Pydantic AI Example (pydantic_model)\nDESCRIPTION: This command demonstrates how to run the `pydantic_model` example specifically. It uses `python` or `uv run` to execute the module, showcasing a common use case for running individual examples.",
    "chunk_length": 2158
  },
  {
    "chunk_id": 3,
    "source": "pydantic_ai_llms_data",
    "content": "SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/examples/index.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython/uv-run -m pydantic_ai_examples.pydantic_model\n```\n\n----------------------------------------\n\nTITLE: Run Pydantic AI Examples\nDESCRIPTION: Commands to execute Pydantic AI examples using either `python` or `uv run`. This includes a general command for running any example module, a specific command for the `pydantic_model` example, and a convenient one-liner for `uv` that handles dependency installation and API key setting. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/examples/index.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython/uv-run -m pydantic_ai_examples.<example_module_name>\n```\n\nLANGUAGE: bash\nCODE:\n```\npython/uv-run -m pydantic_ai_examples.pydantic_model\n```\n\nLANGUAGE: bash\nCODE:\n```\nOPENAI_API_KEY='your-api-key' \\\n  uv run --with \"pydantic-ai[examples]\" \\\n  -m pydantic_ai_examples.pydantic_model\n```\n\n----------------------------------------\n\nTITLE: Install and Run clai with uv\nDESCRIPTION: These commands demonstrate how to install `clai` globally using `uv tool install` and then run it. After installation, `clai` can be invoked directly from the command line to start an interactive AI chat session. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/clai/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nuv tool install clai\n... clai\n```\n\n----------------------------------------\n\nTITLE: Serve Pydantic AI Documentation Locally\nDESCRIPTION: Runs the `mkdocs serve` command via `uv` to start a local web server, allowing contributors to preview and test changes to the project's documentation pages. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/contributing.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nuv run mkdocs serve\n```\n\n----------------------------------------\n\nTITLE: Install Pydantic AI Dependencies and Pre-commit Hooks\nDESCRIPTION: Command to install all project dependencies and set up pre-commit hooks using the `make` utility. This command streamlines the setup process for development.",
    "chunk_length": 2084
  },
  {
    "chunk_id": 4,
    "source": "pydantic_ai_llms_data",
    "content": "SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/contributing.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmake install\n```\n\n----------------------------------------\n\nTITLE: Perform slim installation for Pydantic AI with OpenAI model\nDESCRIPTION: Installs the `pydantic-ai-slim` package with only the `openai` optional group. This is recommended when you intend to use only the `OpenAIModel` and wish to avoid installing superfluous packages. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/install.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip/uv-add \"pydantic-ai-slim[openai]\"\n```\n\n----------------------------------------\n\nTITLE: Start Pydantic AI AG-UI Backend\nDESCRIPTION: Initiates the Pydantic AI AG-UI example backend application. This command uses `uv-run` to execute the specified Python module, making the backend services available for the frontend to communicate with. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/examples/ag-ui.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython/uv-run -m pydantic_ai_examples.ag_ui\n```\n\n----------------------------------------\n\nTITLE: Install Pydantic AI core package\nDESCRIPTION: Installs the main `pydantic-ai` package and its core dependencies, including libraries required to use all models. This installation requires Python 3.9 or newer. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/install.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip/uv-add pydantic-ai\n```\n\n----------------------------------------\n\nTITLE: Install Gradio and Run Weather Agent UI\nDESCRIPTION: This sequence of commands first installs the required Gradio library, then launches the web-based user interface for the Pydantic AI weather agent. The UI provides a chat-based interaction for the agent, requiring Python 3.10+. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/examples/weather-agent.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install gradio>=5.9.0\npython/uv-run -m pydantic_ai_examples.weather_agent_gradio\n```\n\n----------------------------------------\n\nTITLE: Pydantic AI Slim Install Optional Groups\nDESCRIPTION: This section details the available optional groups for `pydantic-ai-slim`, allowing users to install only the necessary dependencies for specific models or features, thereby avoiding superfluous packages.",
    "chunk_length": 2321
  },
  {
    "chunk_id": 5,
    "source": "pydantic_ai_llms_data",
    "content": "Each group corresponds to a set of external libraries. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/install.md#_snippet_4\n\nLANGUAGE: APIDOC\nCODE:\n```\npydantic-ai-slim Optional Groups:\n  logfire: Installs `logfire` for Logfire integration. evals: Installs `pydantic-evals`. openai: Installs `openai`. vertexai: Installs `google-auth` and `requests`. google: Installs `google-genai`. anthropic: Installs `anthropic`. groq: Installs `groq`. mistral: Installs `mistralai`. cohere: Installs `cohere`. bedrock: Installs `boto3`. huggingface: Installs `huggingface-hub[inference]`. duckduckgo: Installs `ddgs`. tavily: Installs `tavily-python`. cli: Installs `rich`, `prompt-toolkit`, and `argcomplete`. mcp: Installs `mcp`. a2a: Installs `fasta2a`. ag-ui: Installs `ag-ui-protocol` and `starlette`. ```\n\n----------------------------------------\n\nTITLE: Copy Pydantic AI Examples to Local Directory\nDESCRIPTION: Command to copy the Pydantic AI example files to a specified local directory. This allows users to easily modify and experiment with the examples without affecting the original installed package files. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/examples/index.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython/uv-run -m pydantic_ai_examples --copy-to examples/\n```\n\n----------------------------------------\n\nTITLE: Install Pydantic AI with Logfire integration\nDESCRIPTION: Installs Pydantic AI with the optional `logfire` group, enabling integration with Pydantic Logfire. This allows for enhanced viewing and understanding of agent runs. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/install.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip/uv-add \"pydantic-ai[logfire]\"\n```\n\n----------------------------------------\n\nTITLE: Pydantic AI Hello World Example\nDESCRIPTION: This minimal example demonstrates how to initialize and run a basic Pydantic AI agent. It configures the agent to use a specific LLM model (Gemini 1.5 Flash) and registers a static system prompt. The agent then synchronously runs a query, and its output is printed.",
    "chunk_length": 2088
  },
  {
    "chunk_id": 6,
    "source": "pydantic_ai_llms_data",
    "content": "This showcases the fundamental steps for setting up and interacting with a Pydantic AI agent. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/index.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\n\nagent = Agent(  # (1)! 'google-gla:gemini-1.5-flash',\n    system_prompt='Be concise, reply with one sentence.',  # (2)! )\n\nresult = agent.run_sync('Where does \"hello world\" come from?')  # (3)! print(result.output)\n\"\"\"\nThe first known use of \"hello, world\" was in a 1974 textbook about the C programming language. \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Run Question Graph Example\nDESCRIPTION: Executes the `pydantic_ai_examples.question_graph` module using `python/uv-run`. This command initiates the question graph application, which is designed for asking and evaluating questions. Users must ensure that all necessary dependencies are installed and environment variables are correctly configured as per the project's usage instructions before running. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/examples/question-graph.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython/uv-run -m pydantic_ai_examples.question_graph\n```\n\n----------------------------------------\n\nTITLE: Install and Run Pydantic AI CLI Globally with uv\nDESCRIPTION: Install the `clai` CLI globally using `uv`'s tool installation feature. After installation, run `clai` to start an interactive chat session with the AI model. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/cli.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nuv tool install clai\n... clai\n```\n\n----------------------------------------\n\nTITLE: Install Pydantic-AI with Groq Support\nDESCRIPTION: This command installs the `pydantic-ai-slim` package along with the necessary `groq` optional dependencies, enabling Groq model integration. It uses either `pip` or `uv` for package management. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/groq.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip/uv-add \"pydantic-ai-slim[groq]\"\n```\n\n----------------------------------------\n\nTITLE: Run Pydantic AI Stream Whales Example Script\nDESCRIPTION: This command executes the `stream_whales.py` example script, which demonstrates streaming structured responses from GPT-4 and displaying them dynamically.",
    "chunk_length": 2318
  },
  {
    "chunk_id": 7,
    "source": "pydantic_ai_llms_data",
    "content": "It requires dependencies to be installed and environment variables to be set up beforehand, as detailed in the project's usage instructions. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/examples/stream-whales.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython/uv-run -m pydantic_ai_examples.stream_whales\n```\n\n----------------------------------------\n\nTITLE: Install and Run clai with pip\nDESCRIPTION: These commands show how to install `clai` using `pip`, the Python package installer, and then run it. Once installed, `clai` can be executed to initiate an interactive chat session with an AI model. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/clai/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install clai\n... clai\n```\n\n----------------------------------------\n\nTITLE: Install Pydantic AI with Google Dependencies\nDESCRIPTION: This command installs `pydantic-ai-slim` along with its `google` optional dependencies, which are required to use `GoogleModel` and access Google's Gemini models. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/google.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip/uv-add \"pydantic-ai-slim[google]\"\n```\n\n----------------------------------------\n\nTITLE: Run Pydantic AI Weather Agent\nDESCRIPTION: This command executes the main Pydantic AI weather agent script. It initializes the agent, allowing it to process user queries by leveraging configured tools and API keys. Ensure dependencies are installed and environment variables are set before running. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/examples/weather-agent.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython/uv-run -m pydantic_ai_examples.weather_agent\n```\n\n----------------------------------------\n\nTITLE: Install Pydantic AI with AG-UI Dependencies\nDESCRIPTION: Instructions for installing Pydantic AI with AG-UI extra and Uvicorn for running ASGI applications, using pip or uv. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/ag-ui.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip/uv-add 'pydantic-ai-slim[ag-ui]'\npip/uv-add uvicorn\n```\n\n----------------------------------------\n\nTITLE: Execute Pydantic AI SQL Generation Example\nDESCRIPTION: Commands to execute the Pydantic AI SQL generation script.",
    "chunk_length": 2273
  },
  {
    "chunk_id": 8,
    "source": "pydantic_ai_llms_data",
    "content": "The first command runs the example with default settings, while the second demonstrates passing a custom prompt string. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/examples/sql-gen.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython/uv-run -m pydantic_ai_examples.sql_gen\n```\n\nLANGUAGE: bash\nCODE:\n```\npython/uv-run -m pydantic_ai_examples.sql_gen \"find me errors\"\n```\n\n----------------------------------------\n\nTITLE: Example Prompt for Haiku Generation\nDESCRIPTION: A simple text prompt demonstrating how to instruct a generative AI model to create a haiku on a specific subject, such as Formula 1. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/examples/ag-ui.md#_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nGenerate a haiku about formula 1\n```\n\n----------------------------------------\n\nTITLE: Install Pre-commit with uv\nDESCRIPTION: Command to install the `pre-commit` tool using `uv`, a fast Python package installer and resolver. This tool helps manage and run pre-commit hooks for code quality. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/contributing.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nuv tool install pre-commit\n```\n\n----------------------------------------\n\nTITLE: Run Pydantic AI with OpenTelemetry Example\nDESCRIPTION: Terminal command to execute a Python script (`raw_otel.py`) that demonstrates Pydantic AI's integration with OpenTelemetry. It ensures necessary dependencies like `pydantic-ai-slim[openai]`, `opentelemetry-sdk`, and `opentelemetry-exporter-otlp` are included for a complete setup. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/logfire.md#_snippet_8\n\nLANGUAGE: txt\nCODE:\n```\nuv run \\\n  --with 'pydantic-ai-slim[openai]' \\\n  --with opentelemetry-sdk \\\n  --with opentelemetry-exporter-otlp \\\n  raw_otel.py\n```\n\n----------------------------------------\n\nTITLE: Run pydantic-ai stream_markdown example\nDESCRIPTION: Execute the pydantic-ai example script to stream markdown output. This command uses `python` or `uv-run` to launch the module. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/examples/stream-markdown.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython/uv-run -m pydantic_ai_examples.stream_markdown\n```\n\n----------------------------------------\n\nTITLE: Install Pydantic-AI with Bedrock Support\nDESCRIPTION: Instructions for installing the `pydantic-ai-slim` package with the `bedrock` optional group, which provides necessary dependencies for integrating with AWS Bedrock.",
    "chunk_length": 2487
  },
  {
    "chunk_id": 9,
    "source": "pydantic_ai_llms_data",
    "content": "SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/bedrock.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip/uv-add \"pydantic-ai-slim[bedrock]\"\n```\n\n----------------------------------------\n\nTITLE: Comprehensive Pydantic-AI Agent with Asynchronous Dependencies, Tools, and Output Validators\nDESCRIPTION: Illustrates a complete Pydantic-AI agent setup utilizing asynchronous dependencies. This example integrates an `httpx.AsyncClient` with an `async` system prompt, an `async` tool, and an `async` output validator, showcasing how `RunContext` can be passed to all these components. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/dependencies.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dataclasses import dataclass\n\nimport httpx\n\nfrom pydantic_ai import Agent, ModelRetry, RunContext\n\n\n@dataclass\nclass MyDeps:\n    api_key: str\n    http_client: httpx.AsyncClient\n\n\nagent = Agent(\n    'openai:gpt-4o',\n    deps_type=MyDeps,\n)\n\n\n@agent.system_prompt\nasync def get_system_prompt(ctx: RunContext[MyDeps]) -> str:\n    response = await ctx.deps.http_client.get('https://example.com')\n    response.raise_for_status()\n    return f'Prompt: {response.text}'\n\n\n@agent.tool  # (1)! async def get_joke_material(ctx: RunContext[MyDeps], subject: str) -> str:\n    response = await ctx.deps.http_client.get(\n        'https://example.com#jokes',\n        params={'subject': subject},\n        headers={'Authorization': f'Bearer {ctx.deps.api_key}'},\n    )\n    response.raise_for_status()\n    return response.text\n\n\n@agent.output_validator  # (2)! async def validate_output(ctx: RunContext[MyDeps], output: str) -> str:\n    response = await ctx.deps.http_client.post(\n        'https://example.com#validate',\n        headers={'Authorization': f'Bearer {ctx.deps.api_key}'},\n        params={'query': output},\n    )\n    if response.status_code == 400:\n        raise ModelRetry(f'invalid response: {response.text}')\n    response.raise_for_status()\n    return output\n\n\nasync def main():\n    async with httpx.AsyncClient() as client:\n        deps = MyDeps('foobar', client)\n        result = await agent.run('Tell me a joke.', deps=deps)\n        print(result.output)\n        #> Did you hear about the toothpaste scandal?",
    "chunk_length": 2226
  },
  {
    "chunk_id": 10,
    "source": "pydantic_ai_llms_data",
    "content": "They called it Colgate. ```\n\n----------------------------------------\n\nTITLE: Install pydantic-ai-slim with Hugging Face support\nDESCRIPTION: This command installs the `pydantic-ai-slim` package along with the `huggingface` optional group, providing necessary dependencies for integrating with Hugging Face models and inference providers. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/huggingface.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip/uv-add \"pydantic-ai-slim[huggingface]\"\n```\n\n----------------------------------------\n\nTITLE: Running Pydantic-AI Agents Example\nDESCRIPTION: This Python example demonstrates how to use `pydantic_ai.Agent` to perform various types of runs: `agent.run_sync()` for a synchronous call, `agent.run()` for an asynchronous call (awaitable), and `agent.run_stream()` for streaming text output asynchronously. It showcases how to get a completed response or stream parts of it. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/agents.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nresult_sync = agent.run_sync('What is the capital of Italy?')\nprint(result_sync.output)\n\n\nasync def main():\n    result = await agent.run('What is the capital of France?')\n    print(result.output)\n\n    async with agent.run_stream('What is the capital of the UK?') as response:\n        async for text in response.stream_text():\n            print(text)\n```\n\n----------------------------------------\n\nTITLE: Navigate to AG-UI TypeScript SDK Directory\nDESCRIPTION: Changes the current working directory to `ag-ui/typescript-sdk`. This directory contains the TypeScript-based AG-UI Dojo example application, which serves as the frontend component for demonstrating Pydantic AI agent interactions. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/examples/ag-ui.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ncd ag-ui/typescript-sdk\n```\n\n----------------------------------------\n\nTITLE: Install pydantic-ai-slim with Cohere Support\nDESCRIPTION: This command installs the `pydantic-ai-slim` package along with its `cohere` optional dependencies.",
    "chunk_length": 2154
  },
  {
    "chunk_id": 11,
    "source": "pydantic_ai_llms_data",
    "content": "This ensures that all necessary components for interacting with Cohere models via `pydantic-ai` are available in your environment. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/cohere.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip/uv-add \"pydantic-ai-slim[cohere]\"\n```\n\n----------------------------------------\n\nTITLE: Integrate single ACI.dev tool with Pydantic AI using tool_from_aci\nDESCRIPTION: This Python example demonstrates how to use the `tool_from_aci` convenience method to integrate a specific ACI.dev tool, like `TAVILY__SEARCH`, into a Pydantic AI `Agent`. It shows the setup for initializing the tool with a linked account owner ID and then using the agent to run a query. Users need to install `aci-sdk` and set the `ACI_API_KEY` environment variable. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/tools.md#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.ext.aci import tool_from_aci\n\n\ntavily_search = tool_from_aci(\n    'TAVILY__SEARCH',\n    linked_account_owner_id=os.getenv('LINKED_ACCOUNT_OWNER_ID'),\n)\n\nagent = Agent(\n    'google-gla:gemini-2.0-flash',\n    tools=[tavily_search],\n)\n\nresult = agent.run_sync('What is the release date of Elden Ring Nightreign?')\nprint(result.output)\n#> Elden Ring Nightreign is planned to be released on May 30, 2025. ```\n\n----------------------------------------\n\nTITLE: Install pydantic-ai with OpenAI support\nDESCRIPTION: Instructions to install the `pydantic-ai-slim` package with the `openai` optional group using pip or uv-add. This enables the necessary dependencies for OpenAI model integration. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/openai.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip/uv-add \"pydantic-ai-slim[openai]\"\n```\n\n----------------------------------------\n\nTITLE: Install pydantic-ai-slim with Mistral support\nDESCRIPTION: Instructions to install the `pydantic-ai-slim` package with the `mistral` optional group using `pip` or `uv-add`, which provides the necessary dependencies for Mistral integration.",
    "chunk_length": 2093
  },
  {
    "chunk_id": 12,
    "source": "pydantic_ai_llms_data",
    "content": "SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/mistral.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip/uv-add \"pydantic-ai-slim[mistral]\"\n```\n\n----------------------------------------\n\nTITLE: Clone AG-UI Protocol Repository\nDESCRIPTION: Clones the official AG-UI protocol repository from GitHub. This step is necessary to obtain the source code for the AG-UI Dojo example frontend application, which complements the Pydantic AI backend. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/examples/ag-ui.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/ag-ui-protocol/ag-ui.git\n```\n\n----------------------------------------\n\nTITLE: Install Pydantic AI with MCP Support\nDESCRIPTION: Provides the `pip` or `uv-add` command to install `pydantic-ai-slim` along with its `mcp` optional dependencies, which are necessary for MCP client functionality. This installation requires Python 3.10 or higher. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/mcp/client.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip/uv-add \"pydantic-ai-slim[mcp]\"\n```\n\n----------------------------------------\n\nTITLE: Run Pydantic AI Example\nDESCRIPTION: Execute the Pydantic AI example using `python/uv-run`. This command runs the `pydantic_model` module from `pydantic_ai_examples` with default settings or allows specifying an alternative model like Gemini via the `PYDANTIC_AI_MODEL` environment variable. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/examples/pydantic-model.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython/uv-run -m pydantic_ai_examples.pydantic_model\n```\n\nLANGUAGE: bash\nCODE:\n```\nPYDANTIC_AI_MODEL=gemini-1.5-pro python/uv-run -m pydantic_ai_examples.pydantic_model\n```\n\n----------------------------------------\n\nTITLE: Run PostgreSQL with pgvector using Docker\nDESCRIPTION: This command starts a PostgreSQL container with the pgvector extension, mapping port 54320 and mounting a local volume for data persistence. It's used as the search database for the RAG example, avoiding port conflicts with other PostgreSQL instances.",
    "chunk_length": 2097
  },
  {
    "chunk_id": 13,
    "source": "pydantic_ai_llms_data",
    "content": "SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/examples/rag.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmkdir postgres-data\ndocker run --rm \\\n  -e POSTGRES_PASSWORD=postgres \\\n  -p 54320:5432 \\\n  -v `pwd`/postgres-data:/var/lib/postgresql/data \\\n  pgvector/pgvector:pg17\n```\n\n----------------------------------------\n\nTITLE: Install Pydantic Evals Package\nDESCRIPTION: This snippet demonstrates how to install the Pydantic Evals library using `pip` or `uv`. The first command installs the base package, while the second command includes an optional `logfire` dependency for OpenTelemetry tracing and sending evaluation results to Logfire. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/evals.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip/uv-add pydantic-evals\n```\n\nLANGUAGE: bash\nCODE:\n```\npip/uv-add 'pydantic-evals[logfire]'\n```\n\n----------------------------------------\n\nTITLE: Install Deno Runtime via Curl\nDESCRIPTION: Command to install the Deno runtime using a `curl` script. Deno is a secure runtime for JavaScript and TypeScript, often used for web development and scripting. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/contributing.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl -fsSL https://deno.land/install.sh | sh\n```\n\n----------------------------------------\n\nTITLE: Install FastA2A Library\nDESCRIPTION: This command installs the `fasta2a` library from PyPI, which provides a framework-agnostic implementation of the A2A protocol in Python. It's the foundational package for building A2A-compliant services. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/a2a.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip/uv-add fasta2a\n```\n\n----------------------------------------\n\nTITLE: Install Pydantic AI with A2A Extra\nDESCRIPTION: This command installs the `pydantic-ai-slim` package along with its `a2a` extra, which automatically includes the `FastA2A` library as a dependency. This is the recommended installation method for users who want to leverage both Pydantic AI and FastA2A together.",
    "chunk_length": 2057
  },
  {
    "chunk_id": 14,
    "source": "pydantic_ai_llms_data",
    "content": "SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/a2a.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip/uv-add 'pydantic-ai-slim[a2a]'\n```\n\n----------------------------------------\n\nTITLE: Run clai with uvx\nDESCRIPTION: This command executes the `clai` command-line interface using `uvx`, a tool for running Python applications without global installation. It starts an interactive session where you can chat with an AI model. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/clai/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nuvx clai\n```\n\n----------------------------------------\n\nTITLE: Pydantic AI Agent with Tools, Dependency Injection, and Structured Output\nDESCRIPTION: This comprehensive example illustrates building a sophisticated Pydantic AI agent for a bank support system. It showcases key features such as defining agent dependencies (`SupportDependencies`), enforcing structured output with Pydantic models (`SupportOutput`), creating dynamic system prompts, and registering custom Python functions as LLM-callable tools (`customer_balance`). The example demonstrates how to run the agent asynchronously with injected dependencies and process its validated, structured output. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dataclasses import dataclass\n\nfrom pydantic import BaseModel, Field\nfrom pydantic_ai import Agent, RunContext\n\nfrom bank_database import DatabaseConn\n\n\n# SupportDependencies is used to pass data, connections, and logic into the model that will be needed when running\n# system prompt and tool functions. Dependency injection provides a type-safe way to customise the behavior of your agents. @dataclass\nclass SupportDependencies:\n    customer_id: int\n    db: DatabaseConn\n\n\n# This pydantic model defines the structure of the output returned by the agent. class SupportOutput(BaseModel):\n    support_advice: str = Field(description='Advice returned to the customer')\n    block_card: bool = Field(description=\"Whether to block the customer's card\")\n    risk: int = Field(description='Risk level of query', ge=0, le=10)\n\n\n# This agent will act as first-tier support in a bank.",
    "chunk_length": 2193
  },
  {
    "chunk_id": 15,
    "source": "pydantic_ai_llms_data",
    "content": "# Agents are generic in the type of dependencies they accept and the type of output they return. # In this case, the support agent has type `Agent[SupportDependencies, SupportOutput]`. support_agent = Agent(\n    'openai:gpt-4o',\n    deps_type=SupportDependencies,\n    # The response from the agent will, be guaranteed to be a SupportOutput,\n    # if validation fails the agent is prompted to try again. output_type=SupportOutput,\n    system_prompt=(\n        'You are a support agent in our bank, give the '\n        'customer support and judge the risk level of their query.'\n    ),\n)\n\n\n# Dynamic system prompts can make use of dependency injection. # Dependencies are carried via the `RunContext` argument, which is parameterized with the `deps_type` from above. # If the type annotation here is wrong, static type checkers will catch it. @support_agent.system_prompt\nasync def add_customer_name(ctx: RunContext[SupportDependencies]) -> str:\n    customer_name = await ctx.deps.db.customer_name(id=ctx.deps.customer_id)\n    return f\"The customer's name is {customer_name!r}\"\n\n\n# `tool` let you register functions which the LLM may call while responding to a user. # Again, dependencies are carried via `RunContext`, any other arguments become the tool schema passed to the LLM. # Pydantic is used to validate these arguments, and errors are passed back to the LLM so it can retry. @support_agent.tool\nasync def customer_balance(\n        ctx: RunContext[SupportDependencies], include_pending: bool\n) -> float:\n    \"\"\"Returns the customer's current account balance.\"\"\"\n    # The docstring of a tool is also passed to the LLM as the description of the tool. # Parameter descriptions are extracted from the docstring and added to the parameter schema sent to the LLM. balance = await ctx.deps.db.customer_balance(\n        id=ctx.deps.customer_id,\n        include_pending=include_pending,\n    )\n    return balance\n\n\n# ... # In a real use case, you'd add more tools and a longer system prompt\n\n\nasync def main():\n    deps = SupportDependencies(customer_id=123, db=DatabaseConn())\n    # Run the agent asynchronously, conducting a conversation with the LLM until a final response is reached.",
    "chunk_length": 2182
  },
  {
    "chunk_id": 16,
    "source": "pydantic_ai_llms_data",
    "content": "# Even in this fairly simple case, the agent will exchange multiple messages with the LLM as tools are called to retrieve an output. result = await support_agent.run('What is my balance?', deps=deps)\n    # The `result.output` will be validated with Pydantic to guarantee it is a `SupportOutput`. Since the agent is generic,\n    # it'll also be typed as a `SupportOutput` to aid with static type checking. print(result.output)\n    # \"\"\"\n    # support_advice='Hello John, your current account balance, including pending transactions, is $123.45.' block_card=False risk=1\n    # \"\"\"\n\n    result = await support_agent.run('I just lost my card!', deps=deps)\n    print(result.output)\n    # \"\"\"\n    # support_advice=\"I'm sorry to hear that, John. We are temporarily blocking your card to prevent unauthorized transactions.\" block_card=True risk=8\n    # \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Integrate MCP Run Python with Pydantic AI Agent\nDESCRIPTION: This Python example demonstrates how to set up and use the MCP Run Python server as a toolset for a Pydantic AI Agent. It shows the initialization of the MCPServerStdio, configuring logging, creating an Agent instance, and executing an asynchronous task that leverages the Python sandbox. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/mcp-run-python/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerStdio\n\nimport logfire\n\nlogfire.configure()\nlogfire.instrument_mcp()\nlogfire.instrument_pydantic_ai()\n\nserver = MCPServerStdio('deno',\n    args=[\n        'run',\n        '-N',\n        '-R=node_modules',\n        '-W=node_modules',\n        '--node-modules-dir=auto',\n        'jsr:@pydantic/mcp-run-python',\n        'stdio',\n    ])\nagent = Agent('claude-3-5-haiku-latest', toolsets=[server])\n\n\nasync def main():\n    async with agent:\n        result = await agent.run('How many days between 2000-01-01 and 2025-03-18?')\n    print(result.output)\n    # There are 9,208 days between January 1, 2000, and March 18, 2025.w\n\nif __name__ == '__main__':\n    import asyncio\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Install and Run Pydantic AI CLI with pip\nDESCRIPTION: Install the `clai` CLI using pip, Python's package installer.",
    "chunk_length": 2294
  },
  {
    "chunk_id": 17,
    "source": "pydantic_ai_llms_data",
    "content": "Once installed, execute `clai` to initiate an interactive chat session with the AI model. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/cli.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install clai\n... clai\n```\n\n----------------------------------------\n\nTITLE: Set LLM API Key Environment Variables\nDESCRIPTION: Commands to set environment variables for authenticating with Large Language Models (LLMs) such as OpenAI or Google Gemini. These API keys are crucial for the Pydantic AI examples to interact with the respective model providers. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/examples/index.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=your-api-key\n```\n\nLANGUAGE: bash\nCODE:\n```\nexport GEMINI_API_KEY=your-api-key\n```\n\n----------------------------------------\n\nTITLE: Create a Streamable HTTP MCP Server in Python\nDESCRIPTION: This Python example demonstrates how to set up a basic Model Context Protocol (MCP) server using `FastMCP`. It defines an `add` tool and configures the server to run using the `streamable-http` transport, which is a prerequisite for the client example. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/mcp/client.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom mcp.server.fastmcp import FastMCP\n\napp = FastMCP()\n\n@app.tool()\ndef add(a: int, b: int) -> int:\n    return a + b\n\nif __name__ == '__main__':\n    app.run(transport='streamable-http')\n```\n\n----------------------------------------\n\nTITLE: Install Pydantic-Graph Library\nDESCRIPTION: This snippet provides the command to install the `pydantic-graph` library using `pip` or `uv-add`. It is a required dependency for `pydantic-ai` and an optional one for `pydantic-ai-slim`, enabling the use of graph-based state machines. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/graph.md#_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\npip/uv-add pydantic-graph\n```\n\n----------------------------------------\n\nTITLE: Install and Run Ollama Locally\nDESCRIPTION: Instructions to download and run the Ollama server with a specific model, preparing it for local `pydantic-ai` integration.",
    "chunk_length": 2135
  },
  {
    "chunk_id": 18,
    "source": "pydantic_ai_llms_data",
    "content": "This command will pull the specified model if it's not already downloaded. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/openai.md#_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nollama run llama3.2\n```\n\n----------------------------------------\n\nTITLE: Run Pydantic AI Example with Gemini Model\nDESCRIPTION: Command to execute the Pydantic AI example using the Gemini 1.5 Pro model by setting the PYDANTIC_AI_MODEL environment variable. This allows overriding the default model. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/examples/pydantic-model.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nPYDANTIC_AI_MODEL=gemini-1.5-pro python/uv-run -m pydantic_ai_examples.pydantic_model\n```\n\n----------------------------------------\n\nTITLE: Query Pydantic AI Agent with RAG Search\nDESCRIPTION: This Python command allows users to ask questions to the Pydantic AI agent, leveraging the previously built RAG search database. The example demonstrates how to query the agent with a specific question about Logfire configuration. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/examples/rag.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npython/uv-run -m pydantic_ai_examples.rag search \"How do I configure logfire to work with FastAPI?\"\n```\n\n----------------------------------------\n\nTITLE: Configure Pydantic-AI Models with Fallback\nDESCRIPTION: This example shows how to initialize `OpenAIModel` and `AnthropicModel` with specific `ModelSettings` (e.g., temperature, max_tokens) and then combine them into a `FallbackModel`. An `Agent` is then created with the `FallbackModel` to execute a prompt, demonstrating automatic model failover. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/index.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nopenai_model = OpenAIModel(\n    'gpt-4o',\n    settings=ModelSettings(temperature=0.7, max_tokens=1000)  # Higher creativity for OpenAI\n)\nanthropic_model = AnthropicModel(\n    'claude-3-5-sonnet-latest',\n    settings=ModelSettings(temperature=0.2, max_tokens=1000)  # Lower temperature for consistency\n)\n\nfallback_model = FallbackModel(openai_model, anthropic_model)\nagent = Agent(fallback_model)\n\nresult = agent.run_sync('Write a creative story about space exploration')\nprint(result.output)\n```\n\n----------------------------------------\n\nTITLE: Install and Run MCP Run Python Server with Deno\nDESCRIPTION: This command installs and runs the MCP Run Python server using Deno.",
    "chunk_length": 2458
  },
  {
    "chunk_id": 19,
    "source": "pydantic_ai_llms_data",
    "content": "It specifies necessary permissions for network access and node_modules, and allows choosing a transport method (stdio, streamable_http, sse, or warmup) for server operation. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/mcp/run-python.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndeno run \\\n  -N -R=node_modules -W=node_modules --node-modules-dir=auto \\\n  jsr:@pydantic/mcp-run-python [stdio|streamable_http|sse|warmup]\n```\n\n----------------------------------------\n\nTITLE: Install Anthropic dependency for pydantic-ai-slim\nDESCRIPTION: This command installs the `anthropic` optional group for `pydantic-ai-slim`, enabling the use of Anthropic models. It ensures necessary dependencies are available for integration with Anthropic's API. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/anthropic.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip/uv-add \"pydantic-ai-slim[anthropic]\"\n```\n\n----------------------------------------\n\nTITLE: Run MCP Python Server with Deno\nDESCRIPTION: This snippet provides the Deno command to start the MCP Run Python server. It includes necessary flags for network access, read/write permissions to node_modules (required for Pyodide), and specifies different transport options like stdio, sse, or warmup for various deployment scenarios. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/mcp-run-python/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndeno run \\\n  -N -R=node_modules -W=node_modules --node-modules-dir=auto \\\n  jsr:@pydantic/mcp-run-python [stdio|sse|warmup]\n```\n\n----------------------------------------\n\nTITLE: Install Tavily Search Tool for Pydantic AI\nDESCRIPTION: Provides the `pip` or `uv` command to install the `tavily` optional group for `pydantic-ai-slim`, which is required to use the Tavily search tool with Pydantic AI agents. Users need to sign up for a Tavily account and obtain an API key. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/common-tools.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip/uv-add \"pydantic-ai-slim[tavily]\"\n```\n\n----------------------------------------\n\nTITLE: Serve Pydantic AI Documentation Locally\nDESCRIPTION: Command to run the Pydantic AI documentation site locally using `uv` and `mkdocs serve`.",
    "chunk_length": 2239
  },
  {
    "chunk_id": 20,
    "source": "pydantic_ai_llms_data",
    "content": "This allows contributors to preview documentation changes before committing. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/contributing.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nuv run mkdocs serve\n```\n\n----------------------------------------\n\nTITLE: Authenticate Local Environment with Logfire\nDESCRIPTION: Authenticates your local development environment with Pydantic Logfire. This command typically guides you through a process to link your local setup to your Logfire account, ensuring that data can be sent securely. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/logfire.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npy-cli logfire auth\n```\n\n----------------------------------------\n\nTITLE: Pydantic AI Native Output Mode Example\nDESCRIPTION: Demonstrates how to use Pydantic AI's `NativeOutput` mode to force a language model to return structured data matching a specified JSON schema. This mode leverages the model's native structured output capabilities, which are not supported by all models. The example shows an `Agent` configured to output either a `Fruit` or `Vehicle` object, and then runs a query to get a `Vehicle`. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/output.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom tool_output import Fruit, Vehicle\n\nfrom pydantic_ai import Agent, NativeOutput\n\nagent = Agent(\n    'openai:gpt-4o',\n    output_type=NativeOutput(\n        [Fruit, Vehicle], # (1)! name='Fruit_or_vehicle',\n        description='Return a fruit or vehicle.'\n    ),\n)\nresult = agent.run_sync('What is a Ford Explorer?')\nprint(repr(result.output))\n#> Vehicle(name='Ford Explorer', wheels=4)\n```\n\n----------------------------------------\n\nTITLE: Configure Pydantic AI with Together AI\nDESCRIPTION: Outlines the setup for using Together AI with Pydantic AI via the `TogetherProvider`. This configuration requires an API key from Together.ai and allows access to their model library, exemplified by 'meta-llama/Llama-3.3-70B-Instruct-Turbo-Free'. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/openai.md#_snippet_29\n\nLANGUAGE: Python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.together import TogetherProvider\n\nmodel = OpenAIModel(\n    'meta-llama/Llama-3.3-70B-Instruct-Turbo-Free',  # model library available at https://www.together.ai/models\n    provider=TogetherProvider(api_key='your-together-api-key'),\n)\nagent = Agent(model)\n...",
    "chunk_length": 2499
  },
  {
    "chunk_id": 21,
    "source": "pydantic_ai_llms_data",
    "content": "```\n\n----------------------------------------\n\nTITLE: Mermaid Diagram Definition for Fives Graph\nDESCRIPTION: This Mermaid syntax defines the visual flow of the `fives_graph` example. It shows the state transitions between `DivisibleBy5` and `Increment` nodes, including the start and end points of the graph execution. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/graph.md#_snippet_6\n\nLANGUAGE: Mermaid\nCODE:\n```\n--- \ntitle: fives_graph\n--- \nstateDiagram-v2\n  [*] --> DivisibleBy5\n  DivisibleBy5 --> Increment\n  DivisibleBy5 --> [*]\n  Increment --> DivisibleBy5\n```\n\n----------------------------------------\n\nTITLE: Implementing Static and Dynamic System Prompts in Pydantic AI\nDESCRIPTION: This example demonstrates how to define both static and dynamic system prompts for a Pydantic AI agent. It shows how static prompts are set during agent initialization and dynamic prompts are created using decorated functions, optionally leveraging `RunContext` for runtime information to tailor responses. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/agents.md#_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\nfrom datetime import date\n\nfrom pydantic_ai import Agent, RunContext\n\nagent = Agent(\n    'openai:gpt-4o',\n    deps_type=str,  # (1)! system_prompt=\"Use the customer's name while replying to them.\",  # (2)! )\n\n\n@agent.system_prompt  # (3)! def add_the_users_name(ctx: RunContext[str]) -> str:\n    return f\"The user's name is {ctx.deps}.\"\n\n\n@agent.system_prompt\ndef add_the_date() -> str:  # (4)! return f'The date is {date.today()}.'\n\n\nresult = agent.run_sync('What is the date?', deps='Frank')\nprint(result.output)\n#> Hello Frank, the date today is 2032-01-02. ```\n\n----------------------------------------\n\nTITLE: Dynamically Customize Pydantic-AI Tool Parameter Description\nDESCRIPTION: This Python example demonstrates using the `prepare` method to dynamically modify a tool's definition before it's passed to the model. The `prepare_greet` function updates the `description` of the `name` parameter for the `greet` tool based on the `deps` value from the `RunContext`, showcasing how tool metadata can be adapted at runtime.",
    "chunk_length": 2158
  },
  {
    "chunk_id": 22,
    "source": "pydantic_ai_llms_data",
    "content": "SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/tools.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\n\nfrom typing import Literal\n\nfrom pydantic_ai import Agent, RunContext\nfrom pydantic_ai.models.test import TestModel\nfrom pydantic_ai.tools import Tool, ToolDefinition\n\n\ndef greet(name: str) -> str:\n    return f'hello {name}'\n\n\nasync def prepare_greet(\n    ctx: RunContext[Literal['human', 'machine']], tool_def: ToolDefinition\n) -> ToolDefinition | None:\n    d = f'Name of the {ctx.deps} to greet.'\n    tool_def.parameters_json_schema['properties']['name']['description'] = d\n    return tool_def\n\n\ngreet_tool = Tool(greet, prepare=prepare_greet)\ntest_model = TestModel()\nagent = Agent(test_model, tools=[greet_tool], deps_type=Literal['human', 'machine'])\n\nresult = agent.run_sync('testing...', deps='human')\nprint(result.output)\n# {\"greet\":\"hello a\"}\nprint(test_model.last_model_request_parameters.function_tools)\n\"\"\"\n[\n    ToolDefinition(\n        name='greet',\n        parameters_json_schema={\n            'additionalProperties': False,\n            'properties': {\n                'name': {'type': 'string', 'description': 'Name of the human to greet.'}\n            },\n            'required': ['name'],\n            'type': 'object',\n        },\n    )\n]\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Python Example: Agent Delegation with Shared Dependencies\nDESCRIPTION: This Python example demonstrates how to set up agent delegation where a 'joke selection' agent delegates joke generation to a 'joke generation' agent. It highlights the use of `deps_type` to define shared dependencies (an HTTP client and API key) and how these dependencies are passed and utilized across agents to make external API calls efficiently. The example also shows how to track combined usage across delegated agents. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/multi-agent-applications.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dataclasses import dataclass\n\nimport httpx\n\nfrom pydantic_ai import Agent, RunContext\n\n\n@dataclass\nclass ClientAndKey:\n    http_client: httpx.AsyncClient\n    api_key: str\n\n\njoke_selection_agent = Agent(\n    'openai:gpt-4o',\n    deps_type=ClientAndKey,\n    system_prompt=(\n        'Use the `joke_factory` tool to generate some jokes on the given subject, '\n        'then choose the best.",
    "chunk_length": 2380
  },
  {
    "chunk_id": 23,
    "source": "pydantic_ai_llms_data",
    "content": "You must return just a single joke.'\n    ),\n)\njoke_generation_agent = Agent(\n    'gemini-1.5-flash',\n    deps_type=ClientAndKey,\n    output_type=list[str],\n    system_prompt=(\n        'Use the \"get_jokes\" tool to get some jokes on the given subject, '\n        'then extract each joke into a list.'\n    ),\n)\n\n\n@joke_selection_agent.tool\nasync def joke_factory(ctx: RunContext[ClientAndKey], count: int) -> list[str]:\n    r = await joke_generation_agent.run(\n        f'Please generate {count} jokes.',\n        deps=ctx.deps,\n        usage=ctx.usage,\n    )\n    return r.output\n\n\n@joke_generation_agent.tool\nasync def get_jokes(ctx: RunContext[ClientAndKey], count: int) -> str:\n    response = await ctx.deps.http_client.get(\n        'https://example.com',\n        params={'count': count},\n        headers={'Authorization': f'Bearer {ctx.deps.api_key}'},\n    )\n    response.raise_for_status()\n    return response.text\n\n\nasync def main():\n    async with httpx.AsyncClient() as client:\n        deps = ClientAndKey(client, 'foobar')\n        result = await joke_selection_agent.run('Tell me a joke.', deps=deps)\n        print(result.output)\n        # > Did you hear about the toothpaste scandal? They called it Colgate. print(result.usage())\n        # > Usage(requests=4, request_tokens=309, response_tokens=32, total_tokens=341)\n```\n\n----------------------------------------\n\nTITLE: Build RAG Search Database with OpenAI Embeddings\nDESCRIPTION: This Python command initiates the process of building the search database for the RAG example. It requires the `OPENAI_API_KEY` environment variable and will make approximately 300 calls to the OpenAI embedding API to generate embeddings for documentation sections. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/examples/rag.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npython/uv-run -m pydantic_ai_examples.rag build\n```\n\n----------------------------------------\n\nTITLE: Run Gradio UI for Weather Agent\nDESCRIPTION: Instructions to install Gradio and launch the web-based user interface for the Pydantic AI weather agent.",
    "chunk_length": 2074
  },
  {
    "chunk_id": 24,
    "source": "pydantic_ai_llms_data",
    "content": "This UI provides a multi-turn chat application built entirely in Python. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/examples/weather-agent.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install gradio>=5.9.0\npython/uv-run -m pydantic_ai_examples.weather_agent_gradio\n```\n\n----------------------------------------\n\nTITLE: Initialize GoogleModel with Vertex AI using Service Account\nDESCRIPTION: This Python example demonstrates how to authenticate `GoogleModel` with Vertex AI using a service account JSON file. It loads credentials from the specified file and passes them to the `GoogleProvider` for secure access. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/google.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom google.oauth2 import service_account\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.google import GoogleModel\nfrom pydantic_ai.providers.google import GoogleProvider\n\ncredentials = service_account.Credentials.from_service_account_file(\n    'path/to/service-account.json',\n    scopes=['https://www.googleapis.com/auth/cloud-platform'],\n)\nprovider = GoogleProvider(credentials=credentials)\nmodel = GoogleModel('gemini-1.5-flash', provider=provider)\nagent = Agent(model)\n... ```\n\n----------------------------------------\n\nTITLE: Use DuckDuckGo Search Tool with Pydantic AI Agent\nDESCRIPTION: Demonstrates how to initialize a Pydantic AI `Agent` with the `duckduckgo_search_tool` and execute a synchronous web search query. The example shows importing the necessary components, configuring the agent with a system prompt, and processing the search results. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/common-tools.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.common_tools.duckduckgo import duckduckgo_search_tool\n\nagent = Agent(\n    'openai:o3-mini',\n    tools=[duckduckgo_search_tool()],\n    system_prompt='Search DuckDuckGo for the given query and return the results.',\n)\n\nresult = agent.run_sync(\n    'Can you list the top five highest-grossing animated films of 2025?'\n)\nprint(result.output)\n```\n\n----------------------------------------\n\nTITLE: Initialize GoogleModel with Custom GoogleProvider and Base URL\nDESCRIPTION: This Python example shows how to provide a custom `google.genai.Client` instance to `GoogleProvider` to configure advanced options like a custom `base_url`.",
    "chunk_length": 2405
  },
  {
    "chunk_id": 25,
    "source": "pydantic_ai_llms_data",
    "content": "This is useful for connecting to custom-compatible endpoints with the Google Generative Language API. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/google.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom google.genai import Client\nfrom google.genai.types import HttpOptions\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.google import GoogleModel\nfrom pydantic_ai.providers.google import GoogleProvider\n\nclient = Client(\n    api_key='gemini-custom-api-key',\n    http_options=HttpOptions(base_url='gemini-custom-base-url'),\n)\nprovider = GoogleProvider(client=client)\nmodel = GoogleModel('gemini-1.5-flash', provider=provider)\nagent = Agent(model)\n... ```\n\n----------------------------------------\n\nTITLE: Install DuckDuckGo Search Tool for Pydantic AI\nDESCRIPTION: Provides the `pip` or `uv` command to install the `duckduckgo` optional group for `pydantic-ai-slim`, which is required to use the DuckDuckGo search tool with Pydantic AI agents. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/common-tools.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip/uv-add \"pydantic-ai-slim[duckduckgo]\"\n```\n\n----------------------------------------\n\nTITLE: Initialize Pydantic-AI Agent with GroqModel Object\nDESCRIPTION: This Python example shows how to explicitly create a `GroqModel` instance with a specific model name and then pass this model object to the `pydantic-ai` Agent. This approach offers more control over the model configuration compared to using a simple string name. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/groq.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.groq import GroqModel\n\nmodel = GroqModel('llama-3.3-70b-versatile')\nagent = Agent(model)\n... ```\n\n----------------------------------------\n\nTITLE: Logging Tool Execution with WrapperToolset\nDESCRIPTION: This Python code defines a `LoggingToolset` that inherits from `WrapperToolset`. It overrides the `call_tool` method to log the start and end of each tool call, including its name, arguments, and result.",
    "chunk_length": 2090
  },
  {
    "chunk_id": 26,
    "source": "pydantic_ai_llms_data",
    "content": "An `asyncio.sleep` is included to simulate asynchronous operations and ensure consistent logging order for testing. The example then demonstrates how to use this custom toolset with an `Agent` to observe tool execution. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/toolsets.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom typing_extensions import Any\n\nfrom prepared_toolset import prepared_toolset\n\nfrom pydantic_ai.agent import Agent\nfrom pydantic_ai.models.test import TestModel\nfrom pydantic_ai.tools import RunContext\nfrom pydantic_ai.toolsets import WrapperToolset, ToolsetTool\n\nLOG = []\n\nclass LoggingToolset(WrapperToolset):\n    async def call_tool(self, name: str, tool_args: dict[str, Any], ctx: RunContext, tool: ToolsetTool) -> Any:\n        LOG.append(f'Calling tool {name!r} with args: {tool_args!r}')\n        try:\n            await asyncio.sleep(0.1 * len(LOG)) # (1)! result = await super().call_tool(name, tool_args, ctx, tool)\n            LOG.append(f'Finished calling tool {name!r} with result: {result!r}')\n        except Exception as e:\n            LOG.append(f'Error calling tool {name!r}: {e}')\n            raise e\n        else:\n            return result\n\n\nlogging_toolset = LoggingToolset(prepared_toolset)\n\nagent = Agent(TestModel(), toolsets=[logging_toolset]) # (2)! result = agent.run_sync('Call all the tools')\nprint(LOG)\n\"\"\"\n[\n    \"Calling tool 'temperature_celsius' with args: {'city': 'a'}\",\n    \"Calling tool 'temperature_fahrenheit' with args: {'city': 'a'}\",\n    \"Calling tool 'weather_conditions' with args: {'city': 'a'}\",\n    \"Calling tool 'current_time' with args: {}\",\n    \"Finished calling tool 'temperature_celsius' with result: 21.0\",\n    \"Finished calling tool 'temperature_fahrenheit' with result: 69.8\",\n    'Finished calling tool \\'weather_conditions\\' with result: \"It\\'s raining\"',\n    \"Finished calling tool 'current_time' with result: datetime.datetime(...)\",\n]\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Run Pydantic AI Project Commands with Make\nDESCRIPTION: Commands to interact with the Pydantic AI project using `make`.",
    "chunk_length": 2120
  },
  {
    "chunk_id": 27,
    "source": "pydantic_ai_llms_data",
    "content": "`make help` displays available commands, while `make` (without arguments) runs formatting, linting, static type checks, and tests with coverage. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/contributing.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmake help\n```\n\nLANGUAGE: bash\nCODE:\n```\nmake\n```\n\n----------------------------------------\n\nTITLE: Run FastAPI Chat Application\nDESCRIPTION: Command to start the FastAPI chat application using `python/uv-run`. This command launches the application, making it accessible via a web browser, typically at `localhost:8000`. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/examples/chat-app.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython/uv-run -m pydantic_ai_examples.chat_app\n```\n\n----------------------------------------\n\nTITLE: FastAPI Server Example for AG-UI Agent\nDESCRIPTION: Demonstrates how to implement a FastAPI endpoint that accepts AG-UI run input, validates it, and streams AG-UI events back to the client using `run_ag_ui`, setting up a complete AG-UI server. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/ag-ui.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ag_ui.core import RunAgentInput\nfrom fastapi import FastAPI\nfrom http import HTTPStatus\nfrom fastapi.requests import Request\nfrom fastapi.responses import Response, StreamingResponse\nfrom pydantic import ValidationError\nimport json\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.ag_ui import run_ag_ui, SSE_CONTENT_TYPE\n\n\nagent = Agent('openai:gpt-4.1', instructions='Be fun!')\n\napp = FastAPI()\n\n\n@app.post(\"/\")\nasync def run_agent(request: Request) -> Response:\n    accept = request.headers.get('accept', SSE_CONTENT_TYPE)\n    try:\n        run_input = RunAgentInput.model_validate(await request.json())\n    except ValidationError as e:  # pragma: no cover\n        return Response(\n            content=json.dumps(e.json()),\n            media_type='application/json',\n            status_code=HTTPStatus.UNPROCESSABLE_ENTITY,\n        )\n\n    event_stream = run_ag_ui(agent, run_input, accept=accept)\n\n    return StreamingResponse(event_stream, media_type=accept)\n```\n\n----------------------------------------\n\nTITLE: Set OpenAI API Key\nDESCRIPTION: Sets the `OPENAI_API_KEY` environment variable, which is essential for authenticating and interacting with the OpenAI API services used by the Pydantic AI agents.",
    "chunk_length": 2372
  },
  {
    "chunk_id": 28,
    "source": "pydantic_ai_llms_data",
    "content": "This key allows the application to make requests to OpenAI's models. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/examples/ag-ui.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=<your api key>\n```\n\n----------------------------------------\n\nTITLE: OpenTelemetry Integration Example in Python\nDESCRIPTION: This Python example demonstrates how to integrate OpenTelemetry tracing with Pydantic Evals. It defines a `SpanTracingEvaluator` that analyzes the `SpanTree` from the `EvaluatorContext` to extract information like total processing time and error occurrences. The example also includes a traced asynchronous function (`process_text`) and sets up a `Dataset` to evaluate its performance and tracing behavior using Logfire for OpenTelemetry configuration. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/evals.md#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nimport asyncio\nfrom typing import Any\n\nimport logfire\n\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import Evaluator\nfrom pydantic_evals.evaluators.context import EvaluatorContext\nfrom pydantic_evals.otel.span_tree import SpanQuery\n\nlogfire.configure(  # ensure that an OpenTelemetry tracer is configured\n    send_to_logfire='if-token-present'\n)\n\n\nclass SpanTracingEvaluator(Evaluator[str, str]):\n    \"\"\"Evaluator that analyzes the span tree generated during function execution.\"\"\"\n\n    def evaluate(self, ctx: EvaluatorContext[str, str]) -> dict[str, Any]:\n        # Get the span tree from the context\n        span_tree = ctx.span_tree\n        if span_tree is None:\n            return {'has_spans': False, 'performance_score': 0.0}\n\n        # Find all spans with \"processing\" in the name\n        processing_spans = span_tree.find(lambda node: 'processing' in node.name)\n\n        # Calculate total processing time\n        total_processing_time = sum(\n            (span.duration.total_seconds() for span in processing_spans), 0.0\n        )\n\n        # Check for error spans\n        error_query: SpanQuery = {'name_contains': 'error'}\n        has_errors = span_tree.any(error_query)\n\n        # Calculate a performance score (lower is better)\n        performance_score = 1.0 if total_processing_time < 1.0 else 0.5\n\n        return {\n            'has_spans': True,\n            'has_errors': has_errors,\n            'performance_score': 0 if has_errors else performance_score,\n        }\n\n\nasync def process_text(text: str) -> str:\n    \"\"\"Function that processes text with OpenTelemetry instrumentation.\"\"\"\n    with logfire.span('process_text'):\n        # Simulate initial processing\n        with logfire.span('text_processing'):\n            await asyncio.sleep(0.1)\n            processed = text.strip().lower()\n\n        # Simulate additional processing\n        with logfire.span('additional_processing'):\n            if 'error' in processed:\n                with logfire.span('error_handling'):\n                    logfire.error(f'Error detected in text: {text}')\n                    return f'Error processing: {text}'\n            await asyncio.sleep(0.2)\n            processed = processed.replace(' ', '_')\n\n        return f'Processed: {processed}'\n\n\n# Create test cases\ndataset = Dataset(\n    cases=[\n        Case(\n            name='normal_text',\n            inputs='Hello World',\n            expected_output='Processed: hello_world',\n        ),\n        Case(\n            name='text_with_error',\n            inputs='Contains error marker',\n            expected_output='Error processing: Contains error marker',\n        ),\n    ],\n    evaluators=[SpanTracingEvaluator()],\n)\n\n# Run evaluation - spans are automatically captured since logfire is configured\nreport = dataset.evaluate_sync(process_text)\n```\n\n----------------------------------------\n\nTITLE: Connect to MCP Run Python Server using Python Client\nDESCRIPTION: This Python example demonstrates how to connect to the MCP Run Python server using the `mcp` client library.",
    "chunk_length": 3948
  },
  {
    "chunk_id": 29,
    "source": "pydantic_ai_llms_data",
    "content": "It initializes a `StdioServerParameters` object to run the Deno server as a subprocess, then uses an asynchronous client session to initialize, list available tools (e.g., `run_python_code`), and execute Python code, capturing the results including output and return values. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/mcp/run-python.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\n\ncode = \"\"\"\nimport numpy\na = numpy.array([1, 2, 3])\nprint(a)\na\n\"\"\"\nserver_params = StdioServerParameters(\n    command='deno',\n    args=[\n        'run',\n        '-N',\n        '-R=node_modules',\n        '-W=node_modules',\n        '--node-modules-dir=auto',\n        'jsr:@pydantic/mcp-run-python',\n        'stdio',\n    ],\n)\n\n\nasync def main():\n    async with stdio_client(server_params) as (read, write):\n        async with ClientSession(read, write) as session:\n            await session.initialize()\n            tools = await session.list_tools()\n            print(len(tools.tools))\n            # > 1\n            print(repr(tools.tools[0].name))\n            # > 'run_python_code'\n            print(repr(tools.tools[0].inputSchema))\n            \"\"\"\n            {'type': 'object', 'properties': {'python_code': {'type': 'string', 'description': 'Python code to run'}}, 'required': ['python_code'], 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}\n            \"\"\"\n            result = await session.call_tool('run_python_code', {'python_code': code})\n            print(result.content[0].text)\n            \"\"\"\n            <status>success</status>\n            <dependencies>[\"numpy\"]</dependencies>\n            <output>\n            [1 2 3]\n            </output>\n            <return_value>\n            [\n              1,\n              2,\n              3\n            ]\n            </return_value>\n            \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Install Pydantic AI with Retries Dependency\nDESCRIPTION: Instructions to install `pydantic-ai-slim` with the `retries` dependency group, which includes `tenacity`, using `pip` or `uv-add`.",
    "chunk_length": 2173
  },
  {
    "chunk_id": 30,
    "source": "pydantic_ai_llms_data",
    "content": "This step is necessary to enable the retry functionality. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/retries.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip/uv-add 'pydantic-ai-slim[retries]'\n```\n\n----------------------------------------\n\nTITLE: Run PostgreSQL for SQL Validation\nDESCRIPTION: Launches a PostgreSQL Docker container on port 54320, configured for use with the Pydantic AI SQL generation example. This instance is used to validate generated SQL queries via `EXPLAIN`. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/examples/sql-gen.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -e POSTGRES_PASSWORD=postgres -p 54320:5432 postgres\n```\n\n----------------------------------------\n\nTITLE: Registering Pydantic-AI Agent Tools via Constructor\nDESCRIPTION: This example demonstrates two methods for registering tools with a Pydantic-AI `Agent` during its construction. It shows how to pass a list of functions directly, allowing the agent to infer `RunContext` usage, or use `Tool` objects for explicit control over tool definitions, names, and descriptions. The snippet includes a simple dice game scenario to illustrate tool interaction and agent execution. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/tools.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport random\n\nfrom pydantic_ai import Agent, RunContext, Tool\n\nsystem_prompt = \"\"\"\nYou're a dice game, you should roll the die and see if the number\nyou get back matches the user's guess. If so, tell them they're a winner. Use the player's name in the response. \"\"\"\n\ndef roll_dice() -> str:\n    \"\"\"Roll a six-sided die and return the result.\"\"\"\n    return str(random.randint(1, 1, 6))\n\n\ndef get_player_name(ctx: RunContext[str]) -> str:\n    \"\"\"Get the player's name.\"\"\"\n    return ctx.deps\n\n\nagent_a = Agent(\n    'google-gla:gemini-1.5-flash',\n    deps_type=str,\n    tools=[roll_dice, get_player_name],\n    system_prompt=system_prompt,\n)\nagent_b = Agent(\n    'google-gla:gemini-1.5-flash',\n    deps_type=str,\n    tools=[\n        Tool(roll_dice, takes_ctx=False),\n        Tool(get_player_name, takes_ctx=True),\n    ],\n    system_prompt=system_prompt,\n)\n\ndice_result = {}\ndice_result['a'] = agent_a.run_sync('My guess is 6', deps='Yashar')\ndice_result['b'] = agent_b.run_sync('My guess is 4', deps='Anne')\nprint(dice_result['a'].output)\nprint(dice_result['b'].output)\n```\n\n----------------------------------------\n\nTITLE: Install Pydantic Logfire SDK\nDESCRIPTION: Installs the Pydantic Logfire Python SDK, including necessary dependencies for Pydantic AI integration, using either pip or uv package managers.",
    "chunk_length": 2627
  },
  {
    "chunk_id": 31,
    "source": "pydantic_ai_llms_data",
    "content": "This step is required to enable Logfire's observability features. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/logfire.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip/uv-add \"pydantic-ai[logfire]\"\n```\n\n----------------------------------------\n\nTITLE: Create Stand-alone ASGI Application from Pydantic AI Agent\nDESCRIPTION: This example illustrates how to convert a Pydantic AI `Agent` directly into a stand-alone ASGI application using the `Agent.to_ag_ui()` method. This simplifies deployment by allowing the agent to be served directly by any ASGI server. The shell command shows how to run this application with Uvicorn. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/ag-ui.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4.1', instructions='Be fun!')\napp = agent.to_ag_ui()\n```\n\nLANGUAGE: shell\nCODE:\n```\nuvicorn agent_to_ag_ui:app\n```\n\n----------------------------------------\n\nTITLE: Define Pydantic-AI Agent Instructions\nDESCRIPTION: This example demonstrates how to configure a Pydantic-AI agent with both static and dynamic instructions. Static instructions are set during agent initialization, while dynamic instructions are defined via decorated functions that can access runtime context, allowing for flexible prompt generation based on current dependencies or other run-time information. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/agents.md#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import date\n\nfrom pydantic_ai import Agent, RunContext\n\nagent = Agent(\n    'openai:gpt-4o',\n    deps_type=str,  # (1)! instructions=\"Use the customer's name while replying to them.\",  # (2)! )\n\n\n@agent.instructions  # (3)! def add_the_users_name(ctx: RunContext[str]) -> str:\n    return f\"The user's name is {ctx.deps}.\"\n\n\n@agent.instructions\ndef add_the_date() -> str:  # (4)! return f'The date is {date.today()}.'\n\n\nresult = agent.run_sync('What is the date?', deps='Frank')\nprint(result.output)\n#> Hello Frank, the date today is 2032-01-02.",
    "chunk_length": 2052
  },
  {
    "chunk_id": 32,
    "source": "pydantic_ai_llms_data",
    "content": "```\n\n----------------------------------------\n\nTITLE: Perform asynchronous model request with tool calling using pydantic-ai direct API\nDESCRIPTION: This advanced example illustrates how to integrate tool calling with the `pydantic_ai.direct` API. It defines a Pydantic model (`Divide`) to represent a tool, generates its JSON schema, and passes it to `model_request` via `ModelRequestParameters`. The example shows how the model can then suggest a tool call based on the prompt. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/direct.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom pydantic import BaseModel\nfrom typing_extensions import Literal\n\nfrom pydantic_ai.direct import model_request\nfrom pydantic_ai.messages import ModelRequest\nfrom pydantic_ai.models import ModelRequestParameters\nfrom pydantic_ai.tools import ToolDefinition\n\n\nclass Divide(BaseModel):\n    \"\"\"Divide two numbers.\"\"\"\n\n    numerator: float\n    denominator: float\n    on_inf: Literal['error', 'infinity'] = 'infinity'\n\n\nasync def main():\n    # Make a request to the model with tool access\n    model_response = await model_request(\n        'openai:gpt-4.1-nano',\n        [ModelRequest.user_text_prompt('What is 123 / 456?')],\n        model_request_parameters=ModelRequestParameters(\n            function_tools=[\n                ToolDefinition(\n                    name=Divide.__name__.lower(),\n                    description=Divide.__doc__,\n                    parameters_json_schema=Divide.model_json_schema(),\n                )\n            ],\n            allow_text_output=True,  # Allow model to either use tools or respond directly\n        ),\n    )\n    print(model_response)\n    \"\"\"\n    ModelResponse(\n        parts=[\n            ToolCallPart(\n                tool_name='divide',\n                args={'numerator': '123', 'denominator': '456'},\n                tool_call_id='pyd_ai_2e0e396768a14fe482df90a29a78dc7b',\n            )\n        ],\n        usage=Usage(requests=1, request_tokens=55, response_tokens=7, total_tokens=62),\n        model_name='gpt-4.1-nano',\n        timestamp=datetime.datetime(...),\n    )\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Pydantic AI Prompted Output Mode Example\nDESCRIPTION: Illustrates the use of Pydantic AI's `PromptedOutput` mode, where the language model is prompted to generate structured data based on a provided JSON schema.",
    "chunk_length": 2380
  },
  {
    "chunk_id": 33,
    "source": "pydantic_ai_llms_data",
    "content": "This mode is usable with all models but is generally less reliable as it depends on the model's interpretation of instructions. The example includes configuring an `Agent` to return a `Vehicle` or `Device` object, and also demonstrates using a custom prompt template. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/output.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\nfrom tool_output import Vehicle\n\nfrom pydantic_ai import Agent, PromptedOutput\n\n\nclass Device(BaseModel):\n    name: str\n    kind: str\n\n\nagent = Agent(\n    'openai:gpt-4o',\n    output_type=PromptedOutput(\n        [Vehicle, Device], # (1)! name='Vehicle or device',\n        description='Return a vehicle or device.'\n    ),\n)\nresult = agent.run_sync('What is a MacBook?')\nprint(repr(result.output))\n#> Device(name='MacBook', kind='laptop')\n\nagent = Agent(\n    'openai:gpt-4o',\n    output_type=PromptedOutput(\n        [Vehicle, Device],\n        template='Gimme some JSON: {schema}'\n    ),\n)\nresult = agent.run_sync('What is a Ford Explorer?')\nprint(repr(result.output))\n#> Vehicle(name='Ford Explorer', wheels=4)\n```\n\n----------------------------------------\n\nTITLE: Initialize Pydantic AI Agent with System Prompt\nDESCRIPTION: This snippet demonstrates how to create a basic Agent instance using the pydantic_ai library. It shows how to specify the LLM model to use and how to set a static system prompt, which defines the agent's initial behavior or instructions. This is a foundational step for building conversational AI applications with Pydantic AI. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/README.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom pydantic_ai import Agent\n\n# Define a very simple agent including the model to use, you can also set the model when running the agent. agent = Agent(\n    'google-gla:gemini-1.5-flash',\n    # Register a static system prompt using a keyword argument to the agent. # For more complex dynamically-generated system prompts, see the example below. system_prompt='Be concise, reply with one sentence.',\n)\n\n# Run the agent synchronously, conducting a conversation with the LLM.",
    "chunk_length": 2143
  },
  {
    "chunk_id": 34,
    "source": "pydantic_ai_llms_data",
    "content": "```\n\n----------------------------------------\n\nTITLE: Interact with MCP Server using Python Client\nDESCRIPTION: This Python client example demonstrates how to connect to an MCP server (like the one defined in `mcp_server.py`) using the `mcp.client.stdio` SDK. It initializes a client session, calls the 'poet' tool with a specific theme ('socks'), and then prints the generated poem, showcasing basic client-server communication. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/mcp/server.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport os\n\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\n\n\nasync def client():\n    server_params = StdioServerParameters(\n        command='python', args=['mcp_server.py'], env=os.environ\n    )\n    async with stdio_client(server_params) as (read, write):\n        async with ClientSession(read, write) as session:\n            await session.initialize()\n            result = await session.call_tool('poet', {'theme': 'socks'})\n            print(result.content[0].text)\n            \"\"\"\n            Oh, socks, those garments soft and sweet,\n            That nestle softly 'round our feet,\n            From cotton, wool, or blended thread,\n            They keep our toes from feeling dread. \"\"\"\n\n\nif __name__ == '__main__':\n    asyncio.run(client())\n```\n\n----------------------------------------\n\nTITLE: Initialize GroqModel with Custom Provider and Async HTTP Client\nDESCRIPTION: This advanced Python example demonstrates how to configure a `GroqModel` with a custom `GroqProvider` that uses a specific `httpx.AsyncClient`. This is useful for fine-tuning HTTP request behavior, such as setting custom timeouts or proxies, for interactions with the Groq API. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/groq.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom httpx import AsyncClient\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.groq import GroqModel\nfrom pydantic_ai.providers.groq import GroqProvider\n\ncustom_http_client = AsyncClient(timeout=30)\nmodel = GroqModel(\n    'llama-3.3-70b-versatile',\n    provider=GroqProvider(api_key='your-api-key', http_client=custom_http_client),\n)\nagent = Agent(model)\n...",
    "chunk_length": 2244
  },
  {
    "chunk_id": 35,
    "source": "pydantic_ai_llms_data",
    "content": "```\n\n----------------------------------------\n\nTITLE: Deploy FastAPI Web Endpoint on Modal with ASGI\nDESCRIPTION: This snippet demonstrates deploying an ASGI application, such as FastAPI, as a web endpoint on Modal. It includes configuring `min_containers=1` to meet Slack's 3-second response time requirement and integrates Logfire setup and application imports within the Modal function. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/examples/slack-lead-qualifier.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# This function defines an ASGI web application for deployment on Modal. # It ensures the web app is always running with min_containers=1 for fast responses. import modal\n# Assuming app.py contains the Pydantic AI app\n# from .app import app as pydantic_ai_app\n\napp = modal.App()\n\n@app.function(min_containers=1)\n@modal.asgi_app()\ndef web_app(): # type: ignore\n    # Call setup_logfire here as logfire package is available in Modal context\n    # setup_logfire() # Assuming setup_logfire is defined elsewhere\n    # return pydantic_ai_app\n    pass\n```\n\n----------------------------------------\n\nTITLE: Configure Pydantic AI with Fireworks AI\nDESCRIPTION: Provides an example of integrating Pydantic AI with Fireworks AI using the `FireworksProvider`. An API key obtained from Fireworks.AI account settings is required, and the snippet demonstrates the typical model naming convention for Fireworks models. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/openai.md#_snippet_28\n\nLANGUAGE: Python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.fireworks import FireworksProvider\n\nmodel = OpenAIModel(\n    'accounts/fireworks/models/qwq-32b',  # model library available at https://fireworks.ai/models\n    provider=FireworksProvider(api_key='your-fireworks-api-key'),\n)\nagent = Agent(model)\n... ```\n\n----------------------------------------\n\nTITLE: Define and Run a Basic Pydantic Graph\nDESCRIPTION: This example demonstrates how to define a simple graph using `pydantic-graph`.",
    "chunk_length": 2087
  },
  {
    "chunk_id": 36,
    "source": "pydantic_ai_llms_data",
    "content": "It showcases the creation of nodes by subclassing `BaseNode`, defining their `run` methods for state transitions, and executing the graph synchronously. The graph increments a number until it becomes divisible by 5, illustrating a basic state machine flow. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/pydantic_graph/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\n\nfrom pydantic_graph import BaseNode, End, Graph, GraphRunContext\n\n\n@dataclass\nclass DivisibleBy5(BaseNode[None, None, int]):\n    foo: int\n\n    async def run(\n        self,\n        ctx: GraphRunContext,\n    ) -> Increment | End[int]:\n        if self.foo % 5 == 0:\n            return End(self.foo)\n        else:\n            return Increment(self.foo)\n\n\n@dataclass\nclass Increment(BaseNode):\n    foo: int\n\n    async def run(self, ctx: GraphRunContext) -> DivisibleBy5:\n        return DivisibleBy5(self.foo + 1)\n\n\nfives_graph = Graph(nodes=[DivisibleBy5, Increment])\nresult = fives_graph.run_sync(DivisibleBy5(4))\nprint(result.output)\n#> 5\n```\n\n----------------------------------------\n\nTITLE: Configure Pydantic AI with OpenRouter\nDESCRIPTION: Details the process of integrating `pydantic-ai` with OpenRouter. This setup requires an API key, which can be obtained from the OpenRouter platform, to authenticate and access various models. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/openai.md#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.openrouter import OpenRouterProvider\n\nmodel = OpenAIModel(\n    'anthropic/claude-3.5-sonnet',\n    provider=OpenRouterProvider(api_key='your-openrouter-api-key'),\n)\nagent = Agent(model)\n... ```\n\n----------------------------------------\n\nTITLE: Pydantic-AI Tool with Advanced ToolReturn for Multi-modal Output\nDESCRIPTION: This Python example demonstrates how to define a `pydantic-ai` tool using `@agent.tool_plain` that returns a `ToolReturn` object.",
    "chunk_length": 2061
  },
  {
    "chunk_id": 37,
    "source": "pydantic_ai_llms_data",
    "content": "It showcases sending multi-modal content (text and binary images via `BinaryContent`) to the LLM as context, while providing a separate `return_value` for programmatic use and including `metadata` not visible to the LLM. The `click_and_capture` function simulates a UI interaction, capturing before/after screenshots and sending them to the model for analysis. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/tools.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport time\nfrom pydantic_ai import Agent\nfrom pydantic_ai.messages import ToolReturn, BinaryContent\n\nagent = Agent('openai:gpt-4o')\n\n@agent.tool_plain\ndef click_and_capture(x: int, y: int) -> ToolReturn:\n    \"\"\"Click at coordinates and show before/after screenshots.\"\"\"\n    # Take screenshot before action\n    before_screenshot = capture_screen()\n\n    # Perform click operation\n    perform_click(x, y)\n    time.sleep(0.5)  # Wait for UI to update\n\n    # Take screenshot after action\n    after_screenshot = capture_screen()\n\n    return ToolReturn(\n        return_value=f\"Successfully clicked at ({x}, {y})\",\n        content=[\n            f\"Clicked at coordinates ({x}, {y}). Here's the comparison:\",\n            \"Before:\",\n            BinaryContent(data=before_screenshot, media_type=\"image/png\"),\n            \"After:\",\n            BinaryContent(data=after_screenshot, media_type=\"image/png\"),\n            \"Please analyze the changes and suggest next steps.\"\n        ],\n        metadata={\n            \"coordinates\": {\"x\": x, \"y\": y},\n            \"action_type\": \"click_and_capture\",\n            \"timestamp\": time.time()\n        }\n    )\n\n# The model receives the rich visual content for analysis\n# while your application can access the structured return_value and metadata\nresult = agent.run_sync(\"Click on the submit button and tell me what happened\")\nprint(result.output)\n# The model can analyze the screenshots and provide detailed feedback\n```\n\n----------------------------------------\n\nTITLE: Filter Out Tools Conditionally with prepare_tools\nDESCRIPTION: This example illustrates how to use `prepare_tools` to conditionally filter out specific tools based on the agent's context or dependencies.",
    "chunk_length": 2171
  },
  {
    "chunk_id": 38,
    "source": "pydantic_ai_llms_data",
    "content": "It demonstrates disabling a tool ('launch_potato') if a boolean dependency (`ctx.deps`) is true, showing how to control tool availability dynamically. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/tools.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Union\n\nfrom pydantic_ai import Agent, RunContext\nfrom pydantic_ai.tools import Tool, ToolDefinition\n\n\ndef launch_potato(target: str) -> str:\n    return f'Potato launched at {target}!'\n\n\nasync def filter_out_tools_by_name(\n    ctx: RunContext[bool], tool_defs: list[ToolDefinition]\n) -> Union[list[ToolDefinition], None]:\n    if ctx.deps:\n        return [tool_def for tool_def in tool_defs if tool_def.name != 'launch_potato']\n    return tool_defs\n\n\nagent = Agent(\n    'test',\n    tools=[Tool(launch_potato)],\n    prepare_tools=filter_out_tools_by_name,\n    deps_type=bool,\n)\n\nresult = agent.run_sync('testing...', deps=False)\nprint(result.output)\n# > {\"launch_potato\":\"Potato launched at a!\"}\nresult = agent.run_sync('testing...', deps=True)\nprint(result.output)\n# > success (no tool calls)\n```\n\n----------------------------------------\n\nTITLE: Initialize AnthropicModel with custom AnthropicProvider\nDESCRIPTION: This Python example illustrates how to provide a custom `AnthropicProvider` instance when initializing `AnthropicModel`. This allows for explicit configuration of the provider, such as passing the API key directly instead of relying on environment variables, or customizing other provider-specific settings for enhanced control. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/anthropic.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.anthropic import AnthropicModel\nfrom pydantic_ai.providers.anthropic import AnthropicProvider\n\nmodel = AnthropicModel(\n    'claude-3-5-sonnet-latest', provider=AnthropicProvider(api_key='your-api-key')\n)\nagent = Agent(model)\n... ```\n\n----------------------------------------\n\nTITLE: Define Pydantic AI Agent with Function Toolset\nDESCRIPTION: This Python code demonstrates how to initialize a `pydantic-ai` agent using `FunctionToolset`.",
    "chunk_length": 2138
  },
  {
    "chunk_id": 39,
    "source": "pydantic_ai_llms_data",
    "content": "It defines two simple tools, `get_default_language` and `get_user_name`, and configures the agent to produce a `PersonalizedGreeting` BaseModel as output. The example shows a synchronous run of the agent and prints the structured result. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/toolsets.md#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.toolsets.function import FunctionToolset\n\ntoolset = FunctionToolset()\n\n\n@toolset.tool\ndef get_default_language():\n    return 'en-US'\n\n\n@toolset.tool\ndef get_user_name():\n    return 'David'\n\n\nclass PersonalizedGreeting(BaseModel):\n    greeting: str\n    language_code: str\n\n\nagent = Agent('openai:gpt-4o', toolsets=[toolset], output_type=PersonalizedGreeting)\n\nresult = agent.run_sync('Greet the user in a personalized way')\nprint(repr(result.output))\n#> PersonalizedGreeting(greeting='Hello, David!', language_code='en-US')\n```\n\n----------------------------------------\n\nTITLE: Integrate multiple ACI.dev tools with Pydantic AI Agent using Toolset\nDESCRIPTION: This Python example illustrates how to integrate multiple ACI.dev tools, such as `OPEN_WEATHER_MAP__CURRENT_WEATHER` and `OPEN_WEATHER_MAP__FORECAST`, into a Pydantic AI `Agent` using the `ACIToolset`. The `ACIToolset` takes a list of ACI tool names and the `linked_account_owner_id` to enable the agent to access a collection of tools. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/tools.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.ext.aci import ACIToolset\n\n\ntoolset = ACIToolset(\n    [\n        'OPEN_WEATHER_MAP__CURRENT_WEATHER',\n        'OPEN_WEATHER_MAP__FORECAST',\n    ],\n    linked_account_owner_id=os.getenv('LINKED_ACCOUNT_OWNER_ID'),\n)\n\nagent = Agent('openai:gpt-4o', toolsets=[toolset])\n```\n\n----------------------------------------\n\nTITLE: Agent Delegation Control Flow Diagram\nDESCRIPTION: This Mermaid diagram visualizes the execution flow of the agent delegation example.",
    "chunk_length": 2053
  },
  {
    "chunk_id": 40,
    "source": "pydantic_ai_llms_data",
    "content": "It illustrates how the 'joke_selection_agent' initiates a call to its 'joke_factory' tool, which then delegates to the 'joke_generation_agent', with control returning through the tool back to the parent agent. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/multi-agent-applications.md#_snippet_1\n\nLANGUAGE: mermaid\nCODE:\n```\ngraph TD\n  START --> joke_selection_agent\n  joke_selection_agent --> joke_factory[\"joke_factory (tool)\"]\n  joke_factory --> joke_generation_agent\n  joke_generation_agent --> joke_factory\n  joke_factory --> joke_selection_agent\n  joke_selection_agent --> END\n```\n\n----------------------------------------\n\nTITLE: Configure Pydantic AI with Grok (xAI)\nDESCRIPTION: Explains how to connect `pydantic-ai` to Grok (xAI) models. This setup requires an API key obtained from the xAI API Console to authenticate and access Grok's language models. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/openai.md#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.grok import GrokProvider\n\nmodel = OpenAIModel(\n    'grok-2-1212',\n    provider=GrokProvider(api_key='your-xai-api-key'),\n)\nagent = Agent(model)\n... ```\n\n----------------------------------------\n\nTITLE: Configure Pydantic AI with Heroku AI\nDESCRIPTION: Demonstrates how to initialize an `OpenAIModel` with `HerokuProvider` for Heroku AI. This setup requires a Heroku inference key to authenticate with the Heroku AI service. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/openai.md#_snippet_30\n\nLANGUAGE: Python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.heroku import HerokuProvider\n\nmodel = OpenAIModel(\n    'claude-3-7-sonnet',\n    provider=HerokuProvider(api_key='your-heroku-inference-key'),\n)\nagent = Agent(model)\n... ```\n\n----------------------------------------\n\nTITLE: Run MCP SSE Server\nDESCRIPTION: Command to start the Model Context Protocol (MCP) server using Deno, configured for Server-Sent Events (SSE) transport.",
    "chunk_length": 2132
  },
  {
    "chunk_id": 41,
    "source": "pydantic_ai_llms_data",
    "content": "This server must be running and accepting HTTP connections before the Pydantic AI agent can successfully connect to it. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/mcp/client.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndeno run \\\n  -N -R=node_modules -W=node_modules --node-modules-dir=auto \\\n  jsr:@pydantic/mcp-run-python sse\n```\n\n----------------------------------------\n\nTITLE: Configure OpenAI Responses API with built-in tools\nDESCRIPTION: Provides an example of configuring `OpenAIResponsesModel` with `OpenAIResponsesModelSettings` to enable built-in tools like web search. It demonstrates how to pass tool parameters and execute a query, showcasing the model's ability to perform external actions. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/openai.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom openai.types.responses import WebSearchToolParam  # (1)! from pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIResponsesModel, OpenAIResponsesModelSettings\n\nmodel_settings = OpenAIResponsesModelSettings(\n    openai_builtin_tools=[WebSearchToolParam(type='web_search_preview')],\n)\nmodel = OpenAIResponsesModel('gpt-4o')\nagent = Agent(model=model, model_settings=model_settings)\n\nresult = agent.run_sync('What is the weather in Tokyo?')\nprint(result.output)\n\"\"\"\nAs of 7:48 AM on Wednesday, April 2, 2025, in Tokyo, Japan, the weather is cloudy with a temperature of 53F (12C). \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Pydantic-AI Programmatic Agent Hand-off Example\nDESCRIPTION: This Python code demonstrates a programmatic agent hand-off using Pydantic-AI. It defines a `flight_search_agent` that uses an `openai:gpt-4o` model to find flight details via a `flight_search` tool. The example includes setting up usage limits and an asynchronous function `find_flight` that iteratively prompts the user and runs the agent until a flight is found or attempts are exhausted. It also defines a `SeatPreference` model, implying a subsequent agent for seat preference extraction.",
    "chunk_length": 2055
  },
  {
    "chunk_id": 42,
    "source": "pydantic_ai_llms_data",
    "content": "SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/multi-agent-applications.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Literal, Union\n\nfrom pydantic import BaseModel, Field\nfrom rich.prompt import Prompt\n\nfrom pydantic_ai import Agent, RunContext\nfrom pydantic_ai.messages import ModelMessage\nfrom pydantic_ai.usage import Usage, UsageLimits\n\n\nclass FlightDetails(BaseModel):\n    flight_number: str\n\n\nclass Failed(BaseModel):\n    \"\"\"Unable to find a satisfactory choice.\"\"\"\n\n\nflight_search_agent = Agent[None, Union[FlightDetails, Failed]](  # (1)! 'openai:gpt-4o',\n    output_type=Union[FlightDetails, Failed],  # type: ignore\n    system_prompt=(\n        'Use the \"flight_search\" tool to find a flight '\n        'from the given origin to the given destination.'\n    ),\n)\n\n\n@flight_search_agent.tool  # (2)! async def flight_search(\n    ctx: RunContext[None], origin: str, destination: str\n) -> Union[FlightDetails, None]:\n    # in reality, this would call a flight search API or\n    # use a browser to scrape a flight search website\n    return FlightDetails(flight_number='AK456')\n\n\nusage_limits = UsageLimits(request_limit=15)  # (3)! async def find_flight(usage: Usage) -> Union[FlightDetails, None]:  # (4)! message_history: Union[list[ModelMessage], None] = None\n    for _ in range(3):\n        prompt = Prompt.ask(\n            'Where would you like to fly from and to?',\n        )\n        result = await flight_search_agent.run(\n            prompt,\n            message_history=message_history,\n            usage=usage,\n            usage_limits=usage_limits,\n        )\n        if isinstance(result.output, FlightDetails):\n            return result.output\n        else:\n            message_history = result.all_messages(\n                output_tool_return_content='Please try again.'\n            )\n\n\nclass SeatPreference(BaseModel):\n    row: int = Field(ge=1, le=30)\n    seat: Literal['A', 'B', 'C', 'D', 'E', 'F']\n```\n\n----------------------------------------\n\nTITLE: Conditionally Register Pydantic-AI Tool with `prepare` Method\nDESCRIPTION: This example illustrates how to use the `prepare` method with a Pydantic-AI tool to control its registration dynamically.",
    "chunk_length": 2198
  },
  {
    "chunk_id": 43,
    "source": "pydantic_ai_llms_data",
    "content": "The `only_if_42` function, registered as the `prepare` method, checks the `RunContext`'s `deps` value and returns `None` if it's not 42, effectively preventing the `hitchhiker` tool from being available to the agent in that step. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/tools.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Union\n\nfrom pydantic_ai import Agent, RunContext\nfrom pydantic_ai.tools import ToolDefinition\n\nagent = Agent('test')\n\n\nasync def only_if_42(\n    ctx: RunContext[int], tool_def: ToolDefinition\n) -> Union[ToolDefinition, None]:\n    if ctx.deps == 42:\n        return tool_def\n\n\n@agent.tool(prepare=only_if_42)\ndef hitchhiker(ctx: RunContext[int], answer: str) -> str:\n    return f'{ctx.deps} {answer}'\n\n\nresult = agent.run_sync('testing...', deps=41)\nprint(result.output)\n# success (no tool calls)\nresult = agent.run_sync('testing...', deps=42)\nprint(result.output)\n# {\"hitchhiker\":\"42 a\"}\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Toolset Registration and Override in Pydantic AI Agent\nDESCRIPTION: This Python example illustrates the various ways to register and manage toolsets with a Pydantic AI `Agent`. It shows how toolsets can be provided at agent construction, during a specific `run_sync` call, and how they can be overridden using an `agent.override()` context manager. The `TestModel` is used to inspect the tools available to the agent at different stages, demonstrating the additive and overriding behaviors of toolset registration. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/toolsets.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.test import TestModel\nfrom pydantic_ai.toolsets import FunctionToolset\n\n\ndef agent_tool():\n    return \"I'm registered directly on the agent\"\n\n\ndef extra_tool():\n    return \"I'm passed as an extra tool for a specific run\"\n\n\ndef override_tool():\n    return \"I override all other tools\"\n\n\nagent_toolset = FunctionToolset(tools=[agent_tool])\nextra_toolset = FunctionToolset(tools=[extra_tool])\noverride_toolset = FunctionToolset(tools=[override_tool])\n\ntest_model = TestModel()\nagent = Agent(test_model, toolsets=[agent_toolset])\n\nresult = agent.run_sync('What tools are available?')\nprint([t.name for t in test_model.last_model_request_parameters.function_tools])\n#> ['agent_tool']\n\nresult = agent.run_sync('What tools are available?', toolsets=[extra_toolset])\nprint([t.name for t in test_model.last_model_request_parameters.function_tools])\n#> ['agent_tool', 'extra_tool']\n\nwith agent.override(toolsets=[override_toolset]):\n    result = agent.run_sync('What tools are available?', toolsets=[extra_toolset])\n    print([t.name for t in test_model.last_model_request_parameters.function_tools])\n    #> ['override_tool']\n```\n\n----------------------------------------\n\nTITLE: Generate Pydantic Evals Dataset to YAML\nDESCRIPTION: This Python example demonstrates how to generate a test dataset using `pydantic_evals.generation.generate_dataset`.",
    "chunk_length": 3036
  },
  {
    "chunk_id": 44,
    "source": "pydantic_ai_llms_data",
    "content": "It defines Pydantic `BaseModel` classes for `QuestionInputs`, `AnswerOutput`, and `MetadataType` to structure the data. The `generate_dataset` function is called with these schemas, a specified number of examples, and extra instructions for the LLM. The resulting dataset is then saved to a YAML file (`questions_cases.yaml`), automatically generating a corresponding JSON schema file for type checking and auto-completion. An example of the generated YAML output is provided in the original documentation. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/evals.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\n\nfrom pathlib import Path\n\nfrom pydantic import BaseModel, Field\n\nfrom pydantic_evals import Dataset\nfrom pydantic_evals.generation import generate_dataset\n\n\nclass QuestionInputs(BaseModel, use_attribute_docstrings=True):\n    \"\"\"Model for question inputs.\"\"\"\n\n    question: str\n    \"\"\"A question to answer\"\"\"\n    context: str | None = None\n    \"\"\"Optional context for the question\"\"\"\n\n\nclass AnswerOutput(BaseModel, use_attribute_docstrings=True):\n    \"\"\"Model for expected answer outputs.\"\"\"\n\n    answer: str\n    \"\"\"The answer to the question\"\"\"\n    confidence: float = Field(ge=0, le=1)\n    \"\"\"Confidence level (0-1)\"\"\"\n\n\nclass MetadataType(BaseModel, use_attribute_docstrings=True):\n    \"\"\"Metadata model for test cases.\"\"\"\n\n    difficulty: str\n    \"\"\"Difficulty level (easy, medium, hard)\"\"\"\n    category: str\n    \"\"\"Question category\"\"\"\n\n\nasync def main():\n    dataset = await generate_dataset(\n        dataset_type=Dataset[QuestionInputs, AnswerOutput, MetadataType],\n        n_examples=2,\n        extra_instructions=\"\"\"\n        Generate question-answer pairs about world capitals and landmarks. Make sure to include both easy and challenging questions. \"\"\",\n    )\n    output_file = Path('questions_cases.yaml')\n    dataset.to_file(output_file)\n    print(output_file.read_text())\n```\n\n----------------------------------------\n\nTITLE: Prepare Tool Definitions with PreparedToolset in Pydantic-AI\nDESCRIPTION: This Python code demonstrates how to use `PreparedToolset` in `pydantic-ai` to dynamically modify `ToolDefinition`s. It defines an asynchronous function `add_descriptions` that takes `RunContext` and a list of `ToolDefinition`s, then uses `dataclasses.replace` to add descriptions to existing tools based on a predefined dictionary.",
    "chunk_length": 2400
  },
  {
    "chunk_id": 45,
    "source": "pydantic_ai_llms_data",
    "content": "The example shows how to apply this prepared toolset to an `Agent` and inspect the resulting tool definitions available to the model. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/toolsets.md#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nfrom dataclasses import replace\nfrom typing import Union\n\nfrom renamed_toolset import renamed_toolset\n\nfrom pydantic_ai import Agent, RunContext\nfrom pydantic_ai.models.test import TestModel\nfrom pydantic_ai.tools import ToolDefinition\n\ndescriptions = {\n    'temperature_celsius': 'Get the temperature in degrees Celsius',\n    'temperature_fahrenheit': 'Get the temperature in degrees Fahrenheit',\n    'weather_conditions': 'Get the current weather conditions',\n    'current_time': 'Get the current time',\n}\n\nasync def add_descriptions(ctx: RunContext, tool_defs: list[ToolDefinition]) -> Union[list[ToolDefinition], None]:\n    return [\n        replace(tool_def, description=description)\n        if (description := descriptions.get(tool_def.name, None))\n        else tool_def\n        for tool_def\n        in tool_defs\n    ]\n\nprepared_toolset = renamed_toolset.prepared(add_descriptions)\n\ntest_model = TestModel() # (1)! agent = Agent(test_model, toolsets=[prepared_toolset])\nresult = agent.run_sync('What tools are available?')\nprint(test_model.last_model_request_parameters.function_tools)\n\"\"\"\n[\n    ToolDefinition(\n        name='temperature_celsius',\n        parameters_json_schema={\n            'additionalProperties': False,\n            'properties': {'city': {'type': 'string'}},\n            'required': ['city'],\n            'type': 'object',\n        },\n        description='Get the temperature in degrees Celsius',\n    ),\n    ToolDefinition(\n        name='temperature_fahrenheit',\n        parameters_json_schema={\n            'additionalProperties': False,\n            'properties': {'city': {'type': 'string'}},\n            'required': ['city'],\n            'type': 'object',\n        },\n        description='Get the temperature in degrees Fahrenheit',\n    ),\n    ToolDefinition(\n        name='weather_conditions',\n        parameters_json_schema={\n            'additionalProperties': False,\n            'properties': {'city': {'type': 'string'}},\n            'required': ['city'],\n            'type': 'object',\n        },\n        description='Get the current weather conditions',\n    ),\n    ToolDefinition(\n        name='current_time',\n        parameters_json_schema={\n            'additionalProperties': False,\n            'properties': {},\n            'type': 'object',\n        },\n        description='Get the current time',\n    ),\n]\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Pydantic-AI Agent with Synchronous Dependencies\nDESCRIPTION: Demonstrates how to configure a Pydantic-AI agent to use synchronous dependencies.",
    "chunk_length": 2792
  },
  {
    "chunk_id": 46,
    "source": "pydantic_ai_llms_data",
    "content": "This example shows a `httpx.Client` and a regular Python function for the system prompt, which `pydantic-ai` automatically runs in a thread pool using `run_in_executor`. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/dependencies.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dataclasses import dataclass\n\nimport httpx\n\nfrom pydantic_ai import Agent, RunContext\n\n\n@dataclass\nclass MyDeps:\n    api_key: str\n    http_client: httpx.Client  # (1)! agent = Agent(\n    'openai:gpt-4o',\n    deps_type=MyDeps,\n)\n\n\n@agent.system_prompt\ndef get_system_prompt(ctx: RunContext[MyDeps]) -> str:  # (2)! response = ctx.deps.http_client.get(\n        'https://example.com', headers={'Authorization': f'Bearer {ctx.deps.api_key}'}\n    )\n    response.raise_for_status()\n    return f'Prompt: {response.text}'\n\n\nasync def main():\n    deps = MyDeps('foobar', httpx.Client())\n    result = await agent.run(\n        'Tell me a joke.',\n        deps=deps,\n    )\n    print(result.output)\n    #> Did you hear about the toothpaste scandal? They called it Colgate. ```\n\n----------------------------------------\n\nTITLE: Forcing Structured Output with Pydantic Model\nDESCRIPTION: This example demonstrates how to configure a `pydantic-ai` agent to return structured data by specifying a Pydantic `BaseModel` as the `output_type`. It shows how to define the expected output structure, initialize the agent, run a query, and then access both the structured output and the usage statistics from the `AgentRunResult`. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/output.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent\n\n\nclass CityLocation(BaseModel):\n    city: str\n    country: str\n\n\nagent = Agent('google-gla:gemini-1.5-flash', output_type=CityLocation)\nresult = agent.run_sync('Where were the olympics held in 2012?')\nprint(result.output)\n#> city='London' country='United Kingdom'\nprint(result.usage())\n#> Usage(requests=1, request_tokens=57, response_tokens=8, total_tokens=65)\n```\n\n----------------------------------------\n\nTITLE: Make a basic synchronous model request with pydantic-ai direct API\nDESCRIPTION: This example demonstrates how to use `model_request_sync` from the `pydantic_ai.direct` module to send a simple text prompt to an LLM and retrieve its response.",
    "chunk_length": 2330
  },
  {
    "chunk_id": 47,
    "source": "pydantic_ai_llms_data",
    "content": "It shows how to construct a `ModelRequest` and access the content and usage statistics from the `ModelResponse`. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/direct.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom pydantic_ai.direct import model_request_sync\nfrom pydantic_ai.messages import ModelRequest\n\n# Make a synchronous request to the model\nmodel_response = model_request_sync(\n    'anthropic:claude-3-5-haiku-latest',\n    [ModelRequest.user_text_prompt('What is the capital of France?')]\n)\n\nprint(model_response.parts[0].content)\n# The capital of France is Paris. print(model_response.usage)\n# Usage(requests=1, request_tokens=56, response_tokens=7, total_tokens=63)\n```\n\n----------------------------------------\n\nTITLE: Configure Hugging Face model with custom provider parameters\nDESCRIPTION: This Python example illustrates how to programmatically instantiate `HuggingFaceProvider` with custom parameters like `api_key` and `provider_name`, and then pass this configured provider to the `HuggingFaceModel`. This allows fine-grained control over the inference provider settings, such as selecting a specific backend. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/huggingface.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.huggingface import HuggingFaceModel\nfrom pydantic_ai.providers.huggingface import HuggingFaceProvider\n\nmodel = HuggingFaceModel('Qwen/Qwen3-235B-A22B', provider=HuggingFaceProvider(api_key='hf_token', provider_name='nebius'))\nagent = Agent(model)\n... ```\n\n----------------------------------------\n\nTITLE: Execute Python Code with Inline Script Dependencies via MCP Client\nDESCRIPTION: This Python example demonstrates how to use the `mcp` client to execute a Python script that includes PEP 723 inline metadata for defining its dependencies (`pydantic`, `email-validator`). It shows connecting to a server via `stdio_client`, initializing a session, and calling the `run_python_code` tool. The output illustrates the successful execution and dependency recognition.",
    "chunk_length": 2086
  },
  {
    "chunk_id": 48,
    "source": "pydantic_ai_llms_data",
    "content": "SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/mcp/run-python.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom mcp import ClientSession\nfrom mcp.client.stdio import stdio_client\n\n# using `server_params` from the above example. from mcp_run_python import server_params\n\ncode = \"\"\"\\\n# /// script\n# dependencies = [\"pydantic\", \"email-validator\"]\n# ///\nimport pydantic\n\nclass Model(pydantic.BaseModel):\n    email: pydantic.EmailStr\n\nprint(Model(email='hello@pydantic.dev'))\n\"\"\"\n\n\nasync def main():\n    async with stdio_client(server_params) as (read, write):\n        async with ClientSession(read, write) as session:\n            await session.initialize()\n            result = await session.call_tool('run_python_code', {'python_code': code})\n            print(result.content[0].text)\n            \"\"\"\n            <status>success</status>\n            <dependencies>[\"pydantic\",\"email-validator\"]</dependencies>\n            <output>\n            email='hello@pydantic.dev'\n            </output>\n            \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Implement AG-UI State Management with Pydantic AI Agent\nDESCRIPTION: This snippet demonstrates how to leverage AG-UI's state management capabilities with a Pydantic AI agent. It uses `StateDeps` with a Pydantic `BaseModel` (`DocumentState`) to define and automatically validate shared state between the UI and the server. The example shows how to initialize the agent with a `deps_type` and how to run the application using Uvicorn. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/ag-ui.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.ag_ui import StateDeps\n\n\nclass DocumentState(BaseModel):\n    \"\"\"State for the document being written.\"\"\"\n\n    document: str = ''\n\n\nagent = Agent(\n    'openai:gpt-4.1',\n    instructions='Be fun!',\n    deps_type=StateDeps[DocumentState],\n)\napp = agent.to_ag_ui(deps=StateDeps(DocumentState()))\n```\n\nLANGUAGE: shell\nCODE:\n```\nuvicorn ag_ui_state:app --host 0.0.0.0 --port 9000\n```\n\n----------------------------------------\n\nTITLE: Configure custom HTTP client for DeepSeekProvider\nDESCRIPTION: This example illustrates how to provide a custom httpx.AsyncClient instance to a provider, such as DeepSeekProvider.",
    "chunk_length": 2302
  },
  {
    "chunk_id": 49,
    "source": "pydantic_ai_llms_data",
    "content": "This allows for advanced control over HTTP request behavior, including setting custom timeouts or other client-specific configurations. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/openai.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom httpx import AsyncClient\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.deepseek import DeepSeekProvider\n\ncustom_http_client = AsyncClient(timeout=30)\nmodel = OpenAIModel(\n    'deepseek-chat',\n    provider=DeepSeekProvider(\n        api_key='your-deepseek-api-key', http_client=custom_http_client\n    ),\n)\nagent = Agent(model)\n... ```\n\n----------------------------------------\n\nTITLE: pydantic-ai Anthropic Integration Classes and Configuration\nDESCRIPTION: This section details the core classes for integrating Anthropic models with `pydantic-ai`, including `AnthropicModel` for model instantiation and `AnthropicProvider` for configuring API access and HTTP client settings. It outlines their constructors and key parameters for flexible setup and customization. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/anthropic.md#_snippet_6\n\nLANGUAGE: APIDOC\nCODE:\n```\nAnthropicModel:\n  __init__(model_name: str, provider: AnthropicProvider | None = None)\n    model_name: The specific Anthropic model identifier (e.g., 'claude-3-5-sonnet-latest'). provider: An optional custom AnthropicProvider instance for advanced configuration. AnthropicProvider:\n  __init__(api_key: str | None = None, http_client: httpx.AsyncClient | None = None)\n    api_key: Your Anthropic API key. If not provided, it defaults to the ANTHROPIC_API_KEY environment variable. http_client: An optional custom httpx.AsyncClient instance to control HTTP request behavior (e.g., timeouts). ```\n\n----------------------------------------\n\nTITLE: Generate Pydantic Evals Dataset to JSON\nDESCRIPTION: This Python example illustrates how to save a generated `pydantic_evals` dataset to a JSON file. It reuses the Pydantic models defined previously and calls `generate_dataset` with similar parameters.",
    "chunk_length": 2104
  },
  {
    "chunk_id": 50,
    "source": "pydantic_ai_llms_data",
    "content": "The key difference is saving the output to `questions_cases.json`, which also generates a corresponding JSON schema file, enabling structured editing and validation. An example of the generated JSON output is provided in the original documentation. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/evals.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom pathlib import Path\n\nfrom generate_dataset_example import AnswerOutput, MetadataType, QuestionInputs\n\nfrom pydantic_evals import Dataset\nfrom pydantic_evals.generation import generate_dataset\n\n\nasync def main():\n    dataset = await generate_dataset(\n        dataset_type=Dataset[QuestionInputs, AnswerOutput, MetadataType],\n        n_examples=2,\n        extra_instructions=\"\"\"\n        Generate question-answer pairs about world capitals and landmarks. Make sure to include both easy and challenging questions. \"\"\",\n    )\n    output_file = Path('questions_cases.json')\n    dataset.to_file(output_file)\n    print(output_file.read_text())\n```\n\n----------------------------------------\n\nTITLE: Initialize Agent with Explicit CohereModel Object\nDESCRIPTION: This Python example illustrates how to explicitly create a `CohereModel` object and then pass it to the `Agent` constructor. This method provides more granular control over the model instance before it's used by the agent, allowing for potential pre-configuration. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/cohere.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.cohere import CohereModel\n\nmodel = CohereModel('command')\nagent = Agent(model)\n... ```\n\n----------------------------------------\n\nTITLE: Customize OpenAIModel behavior with ModelProfile\nDESCRIPTION: This example illustrates how to fine-tune the behavior of OpenAIModel requests using ModelProfile or OpenAIModelProfile. This allows for handling provider-specific requirements, such as different JSON schema transformations for tool definitions or support for strict tool definitions. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/openai.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.profiles import InlineDefsJsonSchemaTransformer\nfrom pydantic_ai.profiles.openai import OpenAIModelProfile\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nmodel = OpenAIModel(\n    'model_name',\n    provider=OpenAIProvider(\n        base_url='https://<openai-compatible-api-endpoint>.com', api_key='your-api-key'\n    ),\n    profile=OpenAIModelProfile(\n        json_schema_transformer=InlineDefsJsonSchemaTransformer,  # Supported by any model class on a plain ModelProfile\n        openai_supports_strict_tool_definition=False  # Supported by OpenAIModel only, requires OpenAIModelProfile\n    )\n)\nagent = Agent(model)\n```\n\n----------------------------------------\n\nTITLE: Pydantic-AI Development and Testing Commands\nDESCRIPTION: Common `make` commands for setting up the development environment, running checks (format, lint, typecheck, test), building documentation, and executing specific tests within the Pydantic-AI project.",
    "chunk_length": 3193
  },
  {
    "chunk_id": 51,
    "source": "pydantic_ai_llms_data",
    "content": "SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/CLAUDE.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nmake install\nmake\nmake format\nmake lint\nmake typecheck\nmake typecheck-both\nmake test\nmake docs\nmake docs-serve\nuv run pytest tests/test_agent.py::test_function_name -v\nuv run pytest tests/test_agent.py -v\nuv run pytest tests/test_agent.py -v -s\nmake install-all-python\nmake test-all-python\n```\n\n----------------------------------------\n\nTITLE: Define Pydantic Evals Dataset and Case\nDESCRIPTION: This Python example illustrates the creation of a `Case` and a `Dataset` in Pydantic Evals. A `Case` represents a single test scenario with inputs, expected outputs, and metadata. A `Dataset` is a collection of such cases, forming the basis for an evaluation. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/evals.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_evals import Case, Dataset\n\ncase1 = Case(\n    name='simple_case',\n    inputs='What is the capital of France?',\n    expected_output='Paris',\n    metadata={'difficulty': 'easy'},\n)\n\ndataset = Dataset(cases=[case1])\n```\n\n----------------------------------------\n\nTITLE: Pydantic AI OpenTelemetry SDK Configuration Example\nDESCRIPTION: Python code demonstrating how to manually configure OpenTelemetry for Pydantic AI without Logfire. It sets up an `OTLPSpanExporter` to send traces via HTTP to a local endpoint, initializes a `TracerProvider`, and instruments the `Agent` class for automatic tracing of AI operations. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/logfire.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.trace import set_tracer_provider\n\nfrom pydantic_ai.agent import Agent\n\nos.environ['OTEL_EXPORTER_OTLP_ENDPOINT'] = 'http://localhost:4318'\nexporter = OTLPSpanExporter()\nspan_processor = BatchSpanProcessor(exporter)\ntracer_provider = TracerProvider()\ntracer_provider.add_span_processor(span_processor)\n\nset_tracer_provider(tracer_provider)\n\nAgent.instrument_all()\nagent = Agent('openai:gpt-4o')\nresult = agent.run_sync('What is the capital of France?')\nprint(result.output)\n#> Paris\n```\n\n----------------------------------------\n\nTITLE: Pydantic-AI Agent with Output Functions and Chained Agents Example\nDESCRIPTION: This comprehensive Python example demonstrates the use of Pydantic-AI's output functions to define the expected output of an agent.",
    "chunk_length": 2588
  },
  {
    "chunk_id": 52,
    "source": "pydantic_ai_llms_data",
    "content": "It showcases a multi-agent architecture where a `router_agent` delegates natural language queries to a `sql_agent`. The `sql_agent` uses a `run_sql_query` output function to simulate database interaction, handling valid queries and raising `ModelRetry` for unsupported or invalid ones. Custom Pydantic models (`SQLFailure`, `RouterFailure`) are used for structured error reporting, illustrating robust error handling and inter-agent communication within the Pydantic-AI framework. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/output.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimport re\nfrom typing import Union\n\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent, ModelRetry, RunContext\nfrom pydantic_ai.exceptions import UnexpectedModelBehavior\n\n\nclass Row(BaseModel):\n    name: str\n    country: str\n\n\ntables = {\n    'capital_cities': [\n        Row(name='Amsterdam', country='Netherlands'),\n        Row(name='Mexico City', country='Mexico'),\n    ]\n}\n\n\nclass SQLFailure(BaseModel):\n    \"\"\"An unrecoverable failure. Only use this when you can't change the query to make it work.\"\"\"\n\n    explanation: str\n\n\ndef run_sql_query(query: str) -> list[Row]:\n    \"\"\"Run a SQL query on the database.\"\"\"\n\n    select_table = re.match(r'SELECT (.+) FROM (\\w+)', query)\n    if select_table:\n        column_names = select_table.group(1)\n        if column_names != '*':\n            raise ModelRetry(\"Only 'SELECT *' is supported, you'll have to do column filtering manually.\")\n\n        table_name = select_table.group(2)\n        if table_name not in tables:\n            raise ModelRetry(\n                f\"Unknown table '{table_name}' in query '{query}'. Available tables: {', '.join(tables.keys())}.\"\n            )\n\n        return tables[table_name]\n\n    raise ModelRetry(f\"Unsupported query: '{query}'.\")\n\n\nsql_agent = Agent[None, Union[list[Row], SQLFailure]](\n    'openai:gpt-4o',\n    output_type=[run_sql_query, SQLFailure],\n    instructions='You are a SQL agent that can run SQL queries on a database.',\n)\n\n\nasync def hand_off_to_sql_agent(ctx: RunContext, query: str) -> list[Row]:\n    \"\"\"I take natural language queries, turn them into SQL, and run them on a database.\"\"\"\n\n    # Drop the final message with the output tool call, as it shouldn't be passed on to the SQL agent\n    messages = ctx.messages[:-1]\n    try:\n        result = await sql_agent.run(query, message_history=messages)\n        output = result.output\n        if isinstance(output, SQLFailure):\n            raise ModelRetry(f'SQL agent failed: {output.explanation}')\n        return output\n    except UnexpectedModelBehavior as e:\n        # Bubble up potentially retryable errors to the router agent\n        if (cause := e.__cause__) and isinstance(cause, ModelRetry):\n            raise ModelRetry(f'SQL agent failed: {cause.message}') from e\n        else:\n            raise\n\n\nclass RouterFailure(BaseModel):\n    \"\"\"Use me when no appropriate agent is found or the used agent failed.\"\"\"\n\n    explanation: str\n\n\nrouter_agent = Agent[None, Union[list[Row], RouterFailure]](\n    'openai:gpt-4o',\n    output_type=[hand_off_to_sql_agent, RouterFailure],\n    instructions='You are a router to other agents.",
    "chunk_length": 3184
  },
  {
    "chunk_id": 53,
    "source": "pydantic_ai_llms_data",
    "content": "Never try to solve a problem yourself, just pass it on.',\n)\n\nresult = router_agent.run_sync('Select the names and countries of all capitals')\nprint(result.output)\n\"\"\"\n[\n    Row(name='Amsterdam', country='Netherlands'),\n    Row(name='Mexico City', country='Mexico'),\n]\n\"\"\"\n\nresult = router_agent.run_sync('Select all pets')\nprint(repr(result.output))\n\"\"\"\nRouterFailure(explanation=\"The requested table 'pets' does not exist in the database. The only available table is 'capital_cities', which does not contain data about pets.\")\n\"\"\"\n\nresult = router_agent.run_sync('How do I fly from Amsterdam to Mexico City?')\nprint(repr(result.output))\n\"\"\"\nRouterFailure(explanation='I am not equipped to provide travel information, such as flights from Amsterdam to Mexico City.')\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Customize Tools with prepare_tools based on Model Type\nDESCRIPTION: This example demonstrates how to use the `prepare_tools` function to dynamically modify tool definitions. Specifically, it shows how to set all tools to 'strict' mode if the agent's model is identified as an OpenAI model, showcasing global modification of tool properties. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/tools.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom dataclasses import replace\nfrom typing import Union\n\nfrom pydantic_ai import Agent, RunContext\nfrom pydantic_ai.tools import ToolDefinition\nfrom pydantic_ai.models.test import TestModel\n\n\nasync def turn_on_strict_if_openai(\n    ctx: RunContext[None], tool_defs: list[ToolDefinition]\n) -> Union[list[ToolDefinition], None]:\n    if ctx.model.system == 'openai':\n        return [replace(tool_def, strict=True) for tool_def in tool_defs]\n    return tool_defs\n\n\ntest_model = TestModel()\nagent = Agent(test_model, prepare_tools=turn_on_strict_if_openai)\n\n\n@agent.tool_plain\ndef echo(message: str) -> str:\n    return message\n\n\nagent.run_sync('testing...')\nassert test_model.last_model_request_parameters.function_tools[0].strict is None\n\n# Set the system attribute of the test_model to 'openai'\ntest_model._system = 'openai'\n\nagent.run_sync('testing with openai...')\nassert test_model.last_model_request_parameters.function_tools[0].strict\n```\n\n----------------------------------------\n\nTITLE: Run Pydantic AI Slack Qualifier as Ephemeral Modal App\nDESCRIPTION: Executes the Slack lead qualifier application as a temporary Modal app.",
    "chunk_length": 2409
  },
  {
    "chunk_id": 54,
    "source": "pydantic_ai_llms_data",
    "content": "This command starts a web function that processes incoming events and runs until manually terminated, useful for local development and testing. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/examples/slack-lead-qualifier.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython/uv-run -m modal serve -m pydantic_ai_examples.slack_lead_qualifier.modal\n```\n\n----------------------------------------\n\nTITLE: Integrate multiple ACI.dev tools with Pydantic AI using ACIToolset\nDESCRIPTION: This Python example illustrates how to integrate multiple ACI.dev tools into a Pydantic AI `Agent` using the `ACIToolset`. It demonstrates initializing the `ACIToolset` with a list of tool names and a linked account owner ID, then passing the toolset to the `Agent` constructor. This approach is suitable for applications requiring access to several ACI functionalities. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/tools.md#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.ext.aci import ACIToolset\n\n\ntoolset = ACIToolset(\n    [\n        'OPEN_WEATHER_MAP__CURRENT_WEATHER',\n        'OPEN_WEATHER_MAP__FORECAST',\n    ],\n    linked_account_owner_id=os.getenv('LINKED_ACCOUNT_OWNER_ID'),\n)\n\nagent = Agent('openai:gpt-4o', toolsets=[toolset])\n```\n\n----------------------------------------\n\nTITLE: Generate Tool Schema with Docstring Descriptions in Pydantic AI\nDESCRIPTION: This Python example demonstrates how Pydantic AI extracts detailed JSON schemas for tools directly from function signatures and docstrings. It showcases the use of `docstring_format='google'` and `require_parameter_descriptions=True` to ensure parameter descriptions are included in the generated schema, which is then printed using `FunctionModel`. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/tools.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.messages import ModelMessage, ModelResponse, TextPart\nfrom pydantic_ai.models.function import AgentInfo, FunctionModel\n\nagent = Agent()\n\n\n@agent.tool_plain(docstring_format='google', require_parameter_descriptions=True)\ndef foobar(a: int, b: str, c: dict[str, list[float]]) -> str:\n    \"\"\"Get me foobar.",
    "chunk_length": 2240
  },
  {
    "chunk_id": 55,
    "source": "pydantic_ai_llms_data",
    "content": "Args:\n        a: apple pie\n        b: banana cake\n        c: carrot smoothie\n    \"\"\"\n    return f'{a} {b} {c}'\n\n\ndef print_schema(messages: list[ModelMessage], info: AgentInfo) -> ModelResponse:\n    tool = info.function_tools[0]\n    print(tool.description)\n    #> Get me foobar. print(tool.parameters_json_schema)\n    \"\"\"\n    {\n        'additionalProperties': False,\n        'properties': {\n            'a': {'description': 'apple pie', 'type': 'integer'},\n            'b': {'description': 'banana cake', 'type': 'string'},\n            'c': {\n                'additionalProperties': {'items': {'type': 'number'}, 'type': 'array'},\n                'description': 'carrot smoothie',\n                'type': 'object',\n            },\n        },\n        'required': ['a', 'b', 'c'],\n        'type': 'object',\n    }\n    \"\"\"\n    return ModelResponse(parts=[TextPart('foobar')])\n\n\nagent.run_sync('hello', model=FunctionModel(print_schema))\n```\n\n----------------------------------------\n\nTITLE: Pydantic-AI Type-Safe Agent Output Definition\nDESCRIPTION: Demonstrates how to define a type-safe agent in Pydantic-AI by specifying an `output_type` using a Pydantic `BaseModel`. This ensures that the agent's output conforms to a predefined schema, enabling robust data validation. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/CLAUDE.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nclass OutputModel(BaseModel):\n    result: str\n    confidence: float\n\nagent: Agent[MyDeps, OutputModel] = Agent(\n    'openai:gpt-4o',\n    deps_type=MyDeps,\n    output_type=OutputModel\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a custom TypeAdapter for Pydantic AI ModelMessage list\nDESCRIPTION: This example shows how to manually create a `TypeAdapter` for a list of `ModelMessage` objects from `pydantic_ai.messages`. This is an alternative to using the pre-exported `ModelMessagesTypeAdapter` and can be useful for custom serialization/deserialization needs. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/message-history.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import TypeAdapter\nfrom pydantic_ai.messages import ModelMessage\nModelMessagesTypeAdapter = TypeAdapter(list[ModelMessage])\n```\n\n----------------------------------------\n\nTITLE: Integrate Single LangChain Tool with Pydantic AI Agent in Python\nDESCRIPTION: This example shows how to integrate a single LangChain tool, such as `DuckDuckGoSearchRun`, into a Pydantic AI agent using the `tool_from_langchain` convenience method.",
    "chunk_length": 2517
  },
  {
    "chunk_id": 56,
    "source": "pydantic_ai_llms_data",
    "content": "It highlights the necessary `langchain-community` dependency and notes that Pydantic AI delegates argument validation to the LangChain tool in this integration scenario. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/tools.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.tools import DuckDuckGoSearchRun\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.ext.langchain import tool_from_langchain\n\n\nsearch = DuckDuckGoSearchRun()\nsearch_tool = tool_from_langchain(search)\n\nagent = Agent(\n    'google-gla:gemini-2.0-flash',\n    tools=[search_tool],\n)\n\nresult = agent.run_sync('What is the release date of Elden Ring Nightreign?')\nprint(result.output)\n```\n\n----------------------------------------\n\nTITLE: Define and Register Agent Tools (Python)\nDESCRIPTION: This Python code demonstrates how to initialize a `pydantic-ai` agent and register custom functions as tools using `@agent.tool_plain` (for context-independent tools) and `@agent.tool` (for tools requiring `RunContext`). The example implements a dice game where the agent uses `roll_dice` and `get_player_name` tools to determine a winner based on a user's guess and name. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/tools.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport random\n\nfrom pydantic_ai import Agent, RunContext\n\nagent = Agent(\n    'google-gla:gemini-1.5-flash',  # (1)! deps_type=str,  # (2)! system_prompt=(\n        \"You're a dice game, you should roll the die and see if the number \"\n        \"you get back matches the user's guess. If so, tell them they're a winner. \"\n        \"Use the player's name in the response.\"\n    ),\n)\n\n\n@agent.tool_plain  # (3)! def roll_dice() -> str:\n    \"\"\"Roll a six-sided die and return the result.\"\"\"\n    return str(random.randint(1, 6))\n\n\n@agent.tool  # (4)! def get_player_name(ctx: RunContext[str]) -> str:\n    \"\"\"Get the player's name.\"\"\"\n    return ctx.deps\n\n\ndice_result = agent.run_sync('My guess is 4', deps='Anne')  # (5)! print(dice_result.output)\n```\n\n----------------------------------------\n\nTITLE: Evaluate Python Function with Pydantic Evals\nDESCRIPTION: This example demonstrates the core workflow of Pydantic Evals.",
    "chunk_length": 2186
  },
  {
    "chunk_id": 57,
    "source": "pydantic_ai_llms_data",
    "content": "It shows how to define a test case with inputs and expected output, create a custom evaluator (`MatchAnswer`) to assess the function's output, and combine these with a built-in evaluator (`IsInstance`) into a `Dataset`. Finally, it illustrates running an asynchronous Python function (`answer_question`) against the defined dataset and printing the evaluation report, which summarizes scores and assertions, similar to the provided output:\n\n```\n                                    Evaluation Summary: answer_question\n\n Case ID           Inputs                          Outputs  Scores             Assertions  Duration \n\n capital_question  What is the capital of France?  Paris    MatchAnswer: 1.00                 10ms \n\n Averages                                                   MatchAnswer: 1.00  100.0%         10ms \n\n```\n\nSOURCE: https://github.com/pydantic/pydantic-ai/blob/main/pydantic_evals/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext, IsInstance\n\n# Define a test case with inputs and expected output\ncase = Case(\n    name='capital_question',\n    inputs='What is the capital of France?',\n    expected_output='Paris',\n)\n\n# Define a custom evaluator\nclass MatchAnswer(Evaluator[str, str]):\n    def evaluate(self, ctx: EvaluatorContext[str, str]) -> float:\n        if ctx.output == ctx.expected_output:\n            return 1.0\n        elif isinstance(ctx.output, str) and ctx.expected_output.lower() in ctx.output.lower():\n            return 0.8\n        return 0.0\n\n# Create a dataset with the test case and evaluators\ndataset = Dataset(\n    cases=[case],\n    evaluators=[IsInstance(type_name='str'), MatchAnswer()],\n)\n\n# Define the function to evaluate\nasync def answer_question(question: str) -> str:\n    return 'Paris'\n\n# Run the evaluation\nreport = dataset.evaluate_sync(answer_question)\nreport.print(include_input=True, include_output=True)\n```\n\n----------------------------------------\n\nTITLE: Stream User Profile with Fine-Grained Validation (pydantic-ai)\nDESCRIPTION: This example illustrates advanced streaming with `pydantic-ai`, providing fine-grained control over validation using `stream_structured` and `validate_structured_output`.",
    "chunk_length": 2714
  },
  {
    "chunk_id": 58,
    "source": "pydantic_ai_llms_data",
    "content": "It demonstrates how to handle `ValidationError` during streaming and leverage `allow_partial` for robust partial data validation. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/output.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import date\n\nfrom pydantic import ValidationError\nfrom typing_extensions import TypedDict\n\nfrom pydantic_ai import Agent\n\n\nclass UserProfile(TypedDict, total=False):\n    name: str\n    dob: date\n    bio: str\n\n\nagent = Agent('openai:gpt-4o', output_type=UserProfile)\n\n\nasync def main():\n    user_input = 'My name is Ben, I was born on January 28th 1990, I like the chain the dog and the pyramid.'\n    async with agent.run_stream(user_input) as result:\n        async for message, last in result.stream_structured(debounce_by=0.01):\n            try:\n                profile = await result.validate_structured_output(\n                    message,\n                    allow_partial=not last,\n                )\n            except ValidationError:\n                continue\n            print(profile)\n```\n\n----------------------------------------\n\nTITLE: Demonstrate Pydantic AI FallbackModel usage with OpenAI and Anthropic\nDESCRIPTION: This Python example demonstrates how to use `FallbackModel` from `pydantic_ai` to chain multiple language models. It shows how the agent automatically falls back from an initial failing OpenAI model to a working Anthropic model, and how to inspect the `model_name` in the `ModelResponse` to identify the successful model. Dependencies include `pydantic_ai`, `OpenAIModel`, `AnthropicModel`, and `FallbackModel`. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/index.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.anthropic import AnthropicModel\nfrom pydantic_ai.models.fallback import FallbackModel\nfrom pydantic_ai.models.openai import OpenAIModel\n\nopenai_model = OpenAIModel('gpt-4o')\nanthropic_model = AnthropicModel('claude-3-5-sonnet-latest')\nfallback_model = FallbackModel(openai_model, anthropic_model)\n\nagent = Agent(fallback_model)\nresponse = agent.run_sync('What is the capital of France?')\nprint(response.data)\n#> Paris\n\nprint(response.all_messages())\n\"\"\"\n[\n    ModelRequest(\n        parts=[\n            UserPromptPart(\n                content='What is the capital of France?',\n                timestamp=datetime.datetime(...),\n                part_kind='user-prompt',\n            )\n        ],\n        kind='request',\n    ),\n    ModelResponse(\n        parts=[TextPart(content='Paris', part_kind='text')],\n        model_name='claude-3-5-sonnet-latest',\n        timestamp=datetime.datetime(...),\n        kind='response',\n        vendor_id=None,\n    ),\n]\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Provide local document input to LLM using Pydantic-AI BinaryContent\nDESCRIPTION: This example shows how to send the binary content of a local document to an LLM using `pydantic-ai`'s `BinaryContent` class.",
    "chunk_length": 2983
  },
  {
    "chunk_id": 59,
    "source": "pydantic_ai_llms_data",
    "content": "It reads the bytes of a local PDF file and passes them to the `Agent` along with the appropriate `media_type`. This method is useful for processing documents stored locally or generated dynamically. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/input.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pathlib import Path\nfrom pydantic_ai import Agent, BinaryContent\n\npdf_path = Path('document.pdf')\nagent = Agent(model='anthropic:claude-3-sonnet')\nresult = agent.run_sync(\n    [\n        'What is the main content of this document?',\n        BinaryContent(data=pdf_path.read_bytes(), media_type='application/pdf'),\n    ]\n)\nprint(result.output)\n```\n\n----------------------------------------\n\nTITLE: Accessing Agent Run Output in Pydantic AI\nDESCRIPTION: This Python example demonstrates how to initialize a Pydantic AI agent, run a synchronous command, and then access the agent's output from the `result` object. It showcases a basic interaction where the agent responds to a prompt and its output is printed. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/message-history.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o', system_prompt='Be a helpful assistant.')\n\nresult = agent.run_sync('Tell me a joke.')\nprint(result.output)\n#> Did you hear about the toothpaste scandal? They called it Colgate. ```\n\n----------------------------------------\n\nTITLE: Limit Pydantic AI Agent Response Tokens\nDESCRIPTION: This example demonstrates how to apply `UsageLimits` to a Pydantic AI agent to restrict the number of response tokens. It shows a successful run within the limit and a `UsageLimitExceeded` exception when the response exceeds the configured `response_tokens_limit`. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/agents.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.exceptions import UsageLimitExceeded\nfrom pydantic_ai.usage import UsageLimits\n\nagent = Agent('anthropic:claude-3-5-sonnet-latest')\n\nresult_sync = agent.run_sync(\n    'What is the capital of Italy?",
    "chunk_length": 2111
  },
  {
    "chunk_id": 60,
    "source": "pydantic_ai_llms_data",
    "content": "Answer with just the city.',\n    usage_limits=UsageLimits(response_tokens_limit=10),\n)\nprint(result_sync.output)\n# Rome\nprint(result_sync.usage())\n# Usage(requests=1, request_tokens=62, response_tokens=1, total_tokens=63)\n\ntry:\n    result_sync = agent.run_sync(\n        'What is the capital of Italy? Answer with a paragraph.',\n        usage_limits=UsageLimits(response_tokens_limit=10),\n    )\nexcept UsageLimitExceeded as e:\n    print(e)\n    # Exceeded the response_tokens_limit of 10 (response_tokens=32)\n```\n\n----------------------------------------\n\nTITLE: Integrate LangChain Tools with Pydantic AI Agent\nDESCRIPTION: This snippet shows how to integrate tools from LangChain's community library into a Pydantic AI agent using `LangChainToolset`. It highlights the necessary `langchain-community` package installation and notes that Pydantic AI does not validate arguments for LangChain tools, leaving it to the model and the LangChain tool itself. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/toolsets.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.agent_toolkits import SlackToolkit\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.ext.langchain import LangChainToolset\n\n\ntoolkit = SlackToolkit()\ntoolset = LangChainToolset(toolkit.get_tools())\n\nagent = Agent('openai:gpt-4o', toolsets=[toolset])\n# ... ```\n\n----------------------------------------\n\nTITLE: Define Custom Tool from Schema for Pydantic-AI Agent\nDESCRIPTION: This Python example demonstrates how to create a `Tool` for a Pydantic-AI agent from a function (`foobar`) that lacks proper documentation. It uses `Tool.from_schema` to explicitly define the tool's name, description, and a JSON schema for its arguments, allowing the agent to correctly interpret and use the function. Note that argument validation is not performed, and all arguments are passed as keyword arguments. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/tools.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent, Tool\nfrom pydantic_ai.models.test import TestModel\n\n\ndef foobar(**kwargs) -> str:\n    return kwargs['a'] + kwargs['b']\n\ntool = Tool.from_schema(\n    function=foobar,\n    name='sum',\n    description='Sum two numbers.',\n    json_schema={\n        'additionalProperties': False,\n        'properties': {\n            'a': {'description': 'the first number', 'type': 'integer'},\n            'b': {'description': 'the second number', 'type': 'integer'},\n        },\n        'required': ['a', 'b'],\n        'type': 'object',\n    }\n)\n\ntest_model = TestModel()\nagent = Agent(test_model, tools=[tool])\n\nresult = agent.run_sync('testing...')\nprint(result.output)\n# {\"sum\":0}\n```\n\n----------------------------------------\n\nTITLE: Pydantic AI Text Output with TextOutput\nDESCRIPTION: Demonstrates how to configure a Pydantic AI agent to produce plain text output using the `TextOutput` marker class.",
    "chunk_length": 2910
  },
  {
    "chunk_id": 61,
    "source": "pydantic_ai_llms_data",
    "content": "The example shows an agent processing a query and returning a list of words from the model's text response, illustrating how to transform raw model output. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/output.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent, TextOutput\n\ndef split_into_words(text: str) -> list[str]:\n    return text.split()\n\n\nagent = Agent(\n    'openai:gpt-4o',\n    output_type=TextOutput(split_into_words),\n)\nresult = agent.run_sync('Who was Albert Einstein?')\nprint(result.output)\n#> ['Albert', 'Einstein', 'was', 'a', 'German-born', 'theoretical', 'physicist.']\n```\n\n----------------------------------------\n\nTITLE: Agent Delegation Example with Pydantic AI\nDESCRIPTION: This Python code demonstrates agent delegation in Pydantic AI. A 'joke_selection_agent' utilizes a 'joke_factory' tool, which in turn calls a 'joke_generation_agent' to produce jokes. It illustrates how to pass 'ctx.usage' to the delegate agent to ensure combined usage tracking and defines two agents with distinct models. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/multi-agent-applications.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent, RunContext\nfrom pydantic_ai.usage import UsageLimits\n\njoke_selection_agent = Agent(  # (1)! 'openai:gpt-4o',\n    system_prompt=(\n        'Use the `joke_factory` to generate some jokes, then choose the best. '\n        'You must return just a single joke.'\n    ),\n)\njoke_generation_agent = Agent(  # (2)! 'google-gla:gemini-1.5-flash', output_type=list[str]\n)\n\n\n@joke_selection_agent.tool\nasync def joke_factory(ctx: RunContext[None], count: int) -> list[str]:\n    r = await joke_generation_agent.run(  # (3)! f'Please generate {count} jokes.',\n        usage=ctx.usage,  # (4)! )\n    return r.output  # (5)! result = joke_selection_agent.run_sync(\n    'Tell me a joke.',\n    usage_limits=UsageLimits(request_limit=5, total_tokens_limit=300),\n)\nprint(result.output)\n#> Did you hear about the toothpaste scandal? They called it Colgate.",
    "chunk_length": 2048
  },
  {
    "chunk_id": 62,
    "source": "pydantic_ai_llms_data",
    "content": "print(result.usage())\n#> Usage(requests=3, request_tokens=204, response_tokens=24, total_tokens=228)\n```\n\n----------------------------------------\n\nTITLE: Provide local image input to LLM using Pydantic-AI BinaryContent\nDESCRIPTION: This example illustrates how to send local image data to an LLM using `pydantic-ai`'s `BinaryContent` class. It first fetches image bytes (simulating a local file read) and then wraps them in `BinaryContent`, specifying the correct `media_type`. This allows the LLM to process image content that is not accessible via a public URL. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/input.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport httpx\n\nfrom pydantic_ai import Agent, BinaryContent\n\nimage_response = httpx.get('https://iili.io/3Hs4FMg.png')  # Pydantic logo\n\nagent = Agent(model='openai:gpt-4o')\nresult = agent.run_sync(\n    [\n        'What company is this logo from?',\n        BinaryContent(data=image_response.content, media_type='image/png'),\n    ]\n)\nprint(result.output)\n```\n\n----------------------------------------\n\nTITLE: Implement MCP Server with Pydantic AI Sampling\nDESCRIPTION: This Python code extends the MCP server example to incorporate sampling, allowing the Pydantic AI agent to make LLM calls back through the MCP client. By using `MCPSamplingModel`, the agent's `run` method directs LLM requests to the client session, enabling more flexible and controlled LLM interactions within the MCP ecosystem. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/mcp/server.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom mcp.server.fastmcp import Context, FastMCP\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.mcp_sampling import MCPSamplingModel\n\nserver = FastMCP('Pydantic AI Server with sampling')\nserver_agent = Agent(system_prompt='always reply in rhyme')\n\n\n@server.tool()\nasync def poet(ctx: Context, theme: str) -> str:\n    \"\"\"Poem generator\"\"\"\n    r = await server_agent.run(f'write a poem about {theme}', model=MCPSamplingModel(session=ctx.session))\n    return r.output\n\n\nif __name__ == '__main__':\n    server.run()\n```\n\n----------------------------------------\n\nTITLE: Make Synchronous AI Model Request (Python)\nDESCRIPTION: This snippet demonstrates how to make a synchronous request to an AI model, such as 'anthropic:claude-3-5-haiku-latest', using the `model_request_sync` function.",
    "chunk_length": 2377
  },
  {
    "chunk_id": 63,
    "source": "pydantic_ai_llms_data",
    "content": "It shows how to pass a user text prompt and enable instrumentation for observability. The example then accesses and prints the content of the model's response. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/direct.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nmodel_response = model_request_sync(\n    'anthropic:claude-3-5-haiku-latest',\n    [ModelRequest.user_text_prompt('What is the capital of France?')],\n    instrument=True\n)\n\nprint(model_response.parts[0].content)\n```\n\n----------------------------------------\n\nTITLE: Initialize AnthropicModel directly and pass to Agent\nDESCRIPTION: This Python code shows how to explicitly create an `AnthropicModel` instance with a specific model name and then pass this model object to the `Agent` constructor. This approach provides more direct control over the model configuration before it's used by the agent, allowing for more complex setups. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/anthropic.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.anthropic import AnthropicModel\n\nmodel = AnthropicModel('claude-3-5-sonnet-latest')\nagent = Agent(model)\n... ```\n\n----------------------------------------\n\nTITLE: Support MCP Sampling in a Python Client\nDESCRIPTION: This Python client example demonstrates how to support MCP sampling by providing a `sampling_callback` function to the `ClientSession`. This callback intercepts LLM calls originating from the server, allowing the client to handle them (e.g., by routing them to an actual LLM or providing a mock response), thus enabling a full sampling workflow. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/mcp/server.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom typing import Any\n\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\nfrom mcp.shared.context import RequestContext\nfrom mcp.types import CreateMessageRequestParams, CreateMessageResult, ErrorData, TextContent\n\n\nasync def sampling_callback(\n    context: RequestContext[ClientSession, Any], params: CreateMessageRequestParams\n) -> CreateMessageResult | ErrorData:\n    print('sampling system prompt:', params.systemPrompt)\n    #> sampling system prompt: always reply in rhyme\n    print('sampling messages:', params.messages)\n    \"\"\"\n    sampling messages:\n    [\n        SamplingMessage(\n            role='user',\n            content=TextContent(\n                type='text',\n                text='write a poem about socks',\n                annotations=None,\n                meta=None,\n            ),\n        )\n    ]\n    \"\"\"\n\n    # TODO get the response content by calling an LLM...",
    "chunk_length": 2700
  },
  {
    "chunk_id": 64,
    "source": "pydantic_ai_llms_data",
    "content": "response_content = 'Socks for a fox.'\n\n    return CreateMessageResult(\n        role='assistant',\n        content=TextContent(type='text', text=response_content),\n        model='fictional-llm',\n    )\n\n\nasync def client():\n    server_params = StdioServerParameters(command='python', args=['mcp_server_sampling.py'])\n    async with stdio_client(server_params) as (read, write):\n        async with ClientSession(read, write, sampling_callback=sampling_callback) as session:\n            await session.initialize()\n            result = await session.call_tool('poet', {'theme': 'socks'})\n            print(result.content[0].text)\n            #> Socks for a fox. if __name__ == '__main__':\n    asyncio.run(client())\n```\n\n----------------------------------------\n\nTITLE: Customize CohereProvider with Custom HTTP Client\nDESCRIPTION: This Python example shows how to configure the `CohereProvider` with a custom `httpx.AsyncClient`, enabling advanced HTTP client settings such as custom timeouts. This provides fine-grained control over network requests made to the Cohere API, which can be crucial for performance or specific network environments. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/cohere.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom httpx import AsyncClient\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.cohere import CohereModel\nfrom pydantic_ai.providers.cohere import CohereProvider\n\ncustom_http_client = AsyncClient(timeout=30)\nmodel = CohereModel(\n    'command',\n    provider=CohereProvider(api_key='your-api-key', http_client=custom_http_client),\n)\nagent = Agent(model)\n... ```\n\n----------------------------------------\n\nTITLE: Run Pydantic AI Agent with Event Streaming\nDESCRIPTION: This asynchronous Python example demonstrates how to run a Pydantic AI agent using `agent.run()` with an `event_stream_handler`. It captures and prints all events generated during the agent's execution, including tool calls and final text output, showcasing the detailed flow of an agent's response. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/agents.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom run_stream_events import weather_agent, event_stream_handler, output_messages\n\nimport asyncio\n\n\nasync def main():\n    user_prompt = 'What will the weather be like in Paris on Tuesday?'\n\n    run = await weather_agent.run(user_prompt, event_stream_handler=event_stream_handler)\n\n    output_messages.append(f'[Final Output] {run.output}')\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n\n    print(output_messages)\n    \"\"\"\n    [\n        \"[Request] Starting part 0: ToolCallPart(tool_name='weather_forecast', tool_call_id='0001')\",\n        '[Request] Part 0 args delta: {\"location\":\"Pa',\n        '[Request] Part 0 args delta: ris\",\"forecast_',\n        '[Request] Part 0 args delta: date\":\"2030-01-',\n        '[Request] Part 0 args delta: 01\"}',\n        '[Tools] The LLM calls tool=\\'weather_forecast\\' with args={\"location\":\"Paris\",\"forecast_date\":\"2030-01-01\"} (tool_call_id=\\'0001\\')',\n        \"[Tools] Tool call '0001' returned => The forecast in Paris on 2030-01-01 is 24C and sunny.\",\n        \"[Request] Starting part 0: TextPart(content='It will be ')\",\n        '[Result] The model starting producing a final result (tool_name=None)',\n        \"[Request] Part 0 text delta: 'warm and sunny '\",\n        \"[Request] Part 0 text delta: 'in Paris on '\",\n        \"[Request] Part 0 text delta: 'Tuesday.'\",\n        '[Final Output] It will be warm and sunny in Paris on Tuesday.',\n    ]\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Define and Run a Simple Pydantic Graph\nDESCRIPTION: This Python example demonstrates how to create a basic computational graph using `pydantic-graph`.",
    "chunk_length": 3745
  },
  {
    "chunk_id": 65,
    "source": "pydantic_ai_llms_data",
    "content": "It defines two nodes, `DivisibleBy5` and `Increment`, which interact to find the next multiple of 5. The snippet shows node parameterization, asynchronous `run` methods, graph instantiation, and synchronous execution using `run_sync`. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/graph.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\n\nfrom pydantic_graph import BaseNode, End, Graph, GraphRunContext\n\n\n@dataclass\nclass DivisibleBy5(BaseNode[None, None, int]):  # (1)! foo: int\n\n    async def run(\n        self,\n        ctx: GraphRunContext,\n    ) -> Increment | End[int]:\n        if self.foo % 5 == 0:\n            return End(self.foo)\n        else:\n            return Increment(self.foo)\n\n\n@dataclass\nclass Increment(BaseNode):  # (2)! foo: int\n\n    async def run(self, ctx: GraphRunContext) -> DivisibleBy5:\n        return DivisibleBy5(self.foo + 1)\n\n\nfives_graph = Graph(nodes=[DivisibleBy5, Increment])  # (3)! result = fives_graph.run_sync(DivisibleBy5(4))  # (4)! print(result.output)\n```\n\n----------------------------------------\n\nTITLE: Integrate Tavily Search with Pydantic-AI Agent\nDESCRIPTION: This Python code snippet demonstrates how to set up and use a Pydantic-AI agent with the Tavily search tool. It requires a `TAVILY_API_KEY` environment variable for authentication. The agent is configured to search Tavily for a given query, and the example shows it retrieving top news in the GenAI world, including links and summaries, then printing the output. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/common-tools.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimport os\n\nfrom pydantic_ai.agent import Agent\nfrom pydantic_ai.common_tools.tavily import tavily_search_tool\n\napi_key = os.getenv('TAVILY_API_KEY')\nassert api_key is not None\n\nagent = Agent(\n    'openai:o3-mini',\n    tools=[tavily_search_tool(api_key)],\n    system_prompt='Search Tavily for the given query and return the results.',\n)\n\nresult = agent.run_sync('Tell me the top news in the GenAI world, give me links.')\nprint(result.output)\n\"\"\"\nHere are some of the top recent news articles related to GenAI:\n\n1.",
    "chunk_length": 2179
  },
  {
    "chunk_id": 66,
    "source": "pydantic_ai_llms_data",
    "content": "How CLEAR users can improve risk analysis with GenAI  Thomson Reuters\n   Read more: https://legal.thomsonreuters.com/blog/how-clear-users-can-improve-risk-analysis-with-genai/\n   (This article discusses how CLEAR's new GenAI-powered tool streamlines risk analysis by quickly summarizing key information from various public data sources.)\n\n2. TELUS Digital Survey Reveals Enterprise Employees Are Entering Sensitive Data Into AI Assistants More Than You Think  FT.com\n   Read more: https://markets.ft.com/data/announce/detail?dockey=600-202502260645BIZWIRE_USPRX____20250226_BW490609-1\n   (This news piece highlights findings from a TELUS Digital survey showing that many enterprise employees use public GenAI tools and sometimes even enter sensitive data.)\n\n3. The Essential Guide to Generative AI  Virtualization Review\n   Read more: https://virtualizationreview.com/Whitepapers/2025/02/SNOWFLAKE-The-Essential-Guide-to-Generative-AI.aspx\n   (This guide provides insights into how GenAI is revolutionizing enterprise strategies and productivity, with input from industry leaders.)\n\nFeel free to click on the links to dive deeper into each story! \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Multi-Agent Flight Booking Control Flow (Mermaid)\nDESCRIPTION: Illustrates the delegation and hand-off process between agents in the flight booking system, showing the interaction sequence from search to seat selection and purchase. It visualizes the flow from initial search to final purchase, including human confirmation and seat choice steps. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/examples/flight-booking.md#_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\ngraph TD\n  START --> search_agent(\"search agent\")\n  search_agent --> extraction_agent(\"extraction agent\")\n  extraction_agent --> search_agent\n  search_agent --> human_confirm(\"human confirm\")\n  human_confirm --> search_agent\n  search_agent --> FAILED\n  human_confirm --> find_seat_function(\"find seat function\")\n  find_seat_function --> human_seat_choice(\"human seat choice\")\n  human_seat_choice --> find_seat_agent(\"find seat agent\")\n  find_seat_agent --> find_seat_function\n  find_seat_function --> buy_flights(\"buy flights\")\n  buy_flights --> SUCCESS\n```\n\n----------------------------------------\n\nTITLE: Pydantic-AI error handling with retry mechanisms\nDESCRIPTION: This Python example illustrates the context for handling errors when using `pydantic-ai` with retry transports.",
    "chunk_length": 2466
  },
  {
    "chunk_id": 67,
    "source": "pydantic_ai_llms_data",
    "content": "It highlights that if all retry attempts fail, the last exception will be re-raised, necessitating appropriate error handling in the application code. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/retries.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nfrom smart_retry_example import create_retrying_client\n\nclient = create_retrying_client()\nmodel = OpenAIModel('gpt-4o', provider=OpenAIProvider(http_client=client))\nagent = Agent(model)\n```\n\n----------------------------------------\n\nTITLE: Set Custom OpenTelemetry SDK Providers in Pydantic AI\nDESCRIPTION: Python example demonstrating how to override the default global OpenTelemetry `TracerProvider` and `EventLoggerProvider` in Pydantic AI. This is achieved by passing custom instances of these providers via `InstrumentationSettings` when initializing an `Agent` or instrumenting all agents globally. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/logfire.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom opentelemetry.sdk._events import EventLoggerProvider\nfrom opentelemetry.sdk.trace import TracerProvider\n\nfrom pydantic_ai.agent import Agent, InstrumentationSettings\n\ninstrumentation_settings = InstrumentationSettings(\n    tracer_provider=TracerProvider(),\n    event_logger_provider=EventLoggerProvider(),\n)\n\nagent = Agent('gpt-4o', instrument=instrumentation_settings)\n# or to instrument all agents:\nAgent.instrument_all(instrumentation_settings)\n```\n\n----------------------------------------\n\nTITLE: Simplify Tool Schema for Single Pydantic Model Parameters\nDESCRIPTION: This Python example illustrates how Pydantic AI simplifies the JSON schema for a tool when its function accepts a single parameter that is a Pydantic `BaseModel`. Instead of nesting, the tool's schema directly reflects the schema of the Pydantic model, as demonstrated by inspecting `test_model.last_model_request_parameters.function_tools`. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/tools.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.test import TestModel\n\nagent = Agent()\n\n\nclass Foobar(BaseModel):\n    \"\"\"This is a Foobar\"\"\"\n\n    x: int\n    y: str\n    z: float = 3.14\n\n\n@agent.tool_plain\ndef foobar(f: Foobar) -> str:\n    return str(f)\n\n\ntest_model = TestModel()\nresult = agent.run_sync('hello', model=test_model)\nprint(result.output)\n#> {\"foobar\":\"x=0 y='a' z=3.14\"}\nprint(test_model.last_model_request_parameters.function_tools)\n\"\"\"\n[\n    ToolDefinition(\n        name='foobar',\n        parameters_json_schema={\n            'properties': {\n                'x': {'type': 'integer'},\n                'y': {'type': 'string'},\n                'z': {'default': 3.14, 'type': 'number'},\n            },\n            'required': ['x', 'y'],\n            'title': 'Foobar',\n            'type': 'object',\n        },\n        description='This is a Foobar',\n    )\n]\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Initialize GoogleModel with Generative Language API Key\nDESCRIPTION: This Python code demonstrates how to explicitly create a `GoogleProvider` instance using an API key and then use it to initialize a `GoogleModel` for interacting with Google's Gemini models via the Generative Language API.",
    "chunk_length": 3408
  },
  {
    "chunk_id": 68,
    "source": "pydantic_ai_llms_data",
    "content": "SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/google.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.google import GoogleModel\nfrom pydantic_ai.providers.google import GoogleProvider\n\nprovider = GoogleProvider(api_key='your-api-key')\nmodel = GoogleModel('gemini-1.5-flash', provider=provider)\nagent = Agent(model)\n... ```\n\n----------------------------------------\n\nTITLE: Stream AI Agent Response (Deltas)\nDESCRIPTION: This example shows how to stream text responses as deltas (incremental changes) rather than the entire text. By passing `delta=True` to `stream_text()`, each yielded item represents a new piece of the response, suitable for real-time UI updates. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/output.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('google-gla:gemini-1.5-flash')\n\n\nasync def main():\n    async with agent.run_stream('Where does \"hello world\" come from?') as result:\n        async for message in result.stream_text(delta=True):\n            print(message)\n            # The first known\n            # use of \"hello,\"\n            # world\" was in\n            # a 1974 textbook\n            # about the C\n            # programming language. ```\n\n----------------------------------------\n\nTITLE: Iterate Pydantic-AI Agent Nodes with async for\nDESCRIPTION: This example demonstrates using `async for` with `agent.iter()` to automatically record each node executed by a Pydantic-AI agent. The `AgentRun` object acts as an async-iterable, yielding `BaseNode` or `End` objects representing steps in the agent's execution graph. This method is suitable for observing the complete flow. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/agents.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\n\nasync def main():\n    nodes = []\n    # Begin an AgentRun, which is an async-iterable over the nodes of the agent's graph\n    async with agent.iter('What is the capital of France?') as agent_run:\n        async for node in agent_run:\n            # Each node represents a step in the agent's execution\n            nodes.append(node)\n    print(nodes)\n    \"\"\"\n    [\n        UserPromptNode(\n            user_prompt='What is the capital of France?',\n            instructions=None,\n            instructions_functions=[],\n            system_prompts=(),\n            system_prompt_functions=[],\n            system_prompt_dynamic_functions={},\n        ),\n        ModelRequestNode(\n            request=ModelRequest(\n                parts=[\n                    UserPromptPart(\n                        content='What is the capital of France?',\n                        timestamp=datetime.datetime(...),\n                    )\n                ]\n            )\n        ),\n        CallToolsNode(\n            model_response=ModelResponse(\n                parts=[TextPart(content='The capital of France is Paris.')],\n                usage=\nUsage(\n                    requests=1, request_tokens=56, response_tokens=7, total_tokens=63\n                ),\n                model_name='gpt-4o',\n                timestamp=datetime.datetime(...),\n            )\n        ),\n        End(data=FinalResult(output='The capital of France is Paris.')),\n    ]\n    \"\"\"\n    print(agent_run.result.output)\n    # > The capital of France is Paris.",
    "chunk_length": 3413
  },
  {
    "chunk_id": 69,
    "source": "pydantic_ai_llms_data",
    "content": "```\n\n----------------------------------------\n\nTITLE: Clone and Navigate Pydantic AI Repository\nDESCRIPTION: Instructions to clone the Pydantic AI repository from GitHub and change the current directory into the newly cloned repository. This is the first step for local development. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/contributing.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone git@github.com:<your username>/pydantic-ai.git\ncd pydantic-ai\n```\n\n----------------------------------------\n\nTITLE: Handle Deferred Tool Calls in Pydantic AI Frontend\nDESCRIPTION: This Python example demonstrates how a frontend application can manage deferred tool calls from a Pydantic AI agent. It defines local tool functions, processes agent outputs, and handles tool returns to continue the conversation loop. The `run_agent` function is assumed to be an API call to the backend. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/toolsets.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom deferred_toolset_api import run_agent\n\nfrom pydantic_ai.messages import ModelMessage, ModelRequest, RetryPromptPart, ToolReturnPart, UserPromptPart\nfrom pydantic_ai.tools import ToolDefinition\nfrom pydantic_ai.output import DeferredToolCalls\n\nfrontend_tool_definitions = [\n    ToolDefinition(\n        name='get_preferred_language',\n        parameters_json_schema={'type': 'object', 'properties': {'default_language': {'type': 'string'}}},\n        description=\"Get the user's preferred language from their browser\",\n    )\n]\n\ndef get_preferred_language(default_language: str) -> str:\n    return 'es-MX' # (1)! frontend_tool_functions = {'get_preferred_language': get_preferred_language}\n\nmessages: list[ModelMessage] = [\n    ModelRequest(\n        parts=[\n            UserPromptPart(content='Greet the user in a personalized way')\n        ]\n    )\n]\n\nfinal_output = None\nwhile True:\n    output, new_messages = run_agent(messages, frontend_tool_definitions)\n    messages += new_messages\n\n    if not isinstance(output, DeferredToolCalls):\n        final_output = output\n        break\n\n    print(output.tool_calls)\n    \"\"\"\n    [\n        ToolCallPart(\n            tool_name='get_preferred_language',\n            args={'default_language': 'en-US'},\n            tool_call_id='pyd_ai_tool_call_id',\n        )\n    ]\n    \"\"\"\n    for tool_call in output.tool_calls:\n        if function := frontend_tool_functions.get(tool_call.tool_name):\n            part = ToolReturnPart(\n                tool_name=tool_call.tool_name,\n                content=function(**tool_call.args_as_dict()),\n                tool_call_id=tool_call.tool_call_id,\n            )\n        else:\n            part = RetryPromptPart(\n                tool_name=tool_call.tool_name,\n                content=f'Unknown tool {tool_call.tool_name!r}',\n                tool_call_id=tool_call.tool_call_id,\n            )\n        messages.append(ModelRequest(parts=[part]))\n\nprint(repr(final_output))\n\"\"\"\nPersonalizedGreeting(greeting='Hola, David!",
    "chunk_length": 3004
  },
  {
    "chunk_id": 70,
    "source": "pydantic_ai_llms_data",
    "content": "Espero que tengas un gran da!', language_code='es-MX')\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Apply Multiple History Processors in Python\nDESCRIPTION: This example demonstrates how to chain multiple history processors together. When multiple processors are provided to an `Agent`, they are applied sequentially in the order they are listed. This allows for complex message manipulation pipelines, combining different filtering and summarization strategies. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/message-history.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.messages import ModelMessage, ModelRequest\n\n\ndef filter_responses(messages: list[ModelMessage]) -> list[ModelMessage]:\n    return [msg for msg in messages if isinstance(msg, ModelRequest)]\n\n\ndef summarize_old_messages(messages: list[ModelMessage]) -> list[ModelMessage]:\n    return messages[-5:]\n\n\nagent = Agent('openai:gpt-4o', history_processors=[filter_responses, summarize_old_messages])\n```\n\n----------------------------------------\n\nTITLE: MCP Server Example with Sampling for Image Generation\nDESCRIPTION: This Python script defines an MCP server that exposes an `image_generator` tool. It demonstrates how an MCP server can use `ctx.session.create_message` to perform an LLM call via the connected MCP client (sampling) to generate an SVG image based on user input. The script handles potential markdown wrapping of the SVG output and writes the result to a file. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/mcp/client.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport re\nfrom pathlib import Path\n\nfrom mcp import SamplingMessage\nfrom mcp.server.fastmcp import Context, FastMCP\nfrom mcp.types import TextContent\n\napp = FastMCP()\n\n\n@app.tool()\nasync def image_generator(ctx: Context, subject: str, style: str) -> str:\n    prompt = f'{subject=} {style=}'\n    # `ctx.session.create_message` is the sampling call\n    result = await ctx.session.create_message(\n        [SamplingMessage(role='user', content=TextContent(type='text', text=prompt))],\n        max_tokens=1_024,\n        system_prompt='Generate an SVG image as per the user input',\n    )\n    assert isinstance(result.content, TextContent)\n\n    path = Path(f'{subject}_{style}.svg')\n    # remove triple backticks if the svg was returned within markdown\n    if m := re.search(r'^```\\w*$(.+?)```$', result.content.text, re.S | re.M):\n        path.write_text(m.group(1))\n    else:\n        path.write_text(result.content.text)\n    return f'See {path}'\n\n\nif __name__ == '__main__':\n    # run the server via stdio\n    app.run()\n```\n\n----------------------------------------\n\nTITLE: Integrate Retrying HTTP Client with Anthropic API in Python\nDESCRIPTION: Demonstrates passing a custom `httpx` client with retry capabilities (e.g., from `smart_retry_example.py`) to the `AnthropicProvider` in `pydantic-ai`.",
    "chunk_length": 2928
  },
  {
    "chunk_id": 71,
    "source": "pydantic_ai_llms_data",
    "content": "This setup ensures robust communication with the Anthropic API, handling transient errors and rate limits automatically. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/retries.md#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.anthropic import AnthropicModel\nfrom pydantic_ai.providers.anthropic import AnthropicProvider\n\nfrom smart_retry_example import create_retrying_client\n\nclient = create_retrying_client()\nmodel = AnthropicModel('claude-3-5-sonnet-20241022', provider=AnthropicProvider(http_client=client))\nagent = Agent(model)\n```\n\n----------------------------------------\n\nTITLE: Integrate single ACI.dev tool with Pydantic AI Agent\nDESCRIPTION: This Python example demonstrates how to integrate a single ACI.dev tool, such as `TAVILY__SEARCH`, into a Pydantic AI `Agent` using the `tool_from_aci` convenience method. It requires the `aci-sdk` package and setting the `ACI_API_KEY` environment variable, along with providing the `linked_account_owner_id`. Note that Pydantic AI does not validate arguments for ACI tools. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/tools.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.ext.aci import tool_from_aci\n\n\ntavily_search = tool_from_aci(\n    'TAVILY__SEARCH',\n    linked_account_owner_id=os.getenv('LINKED_ACCOUNT_OWNER_ID'),\n)\n\nagent = Agent(\n    'google-gla:gemini-2.0-flash',\n    tools=[tavily_search],\n)\n\nresult = agent.run_sync('What is the release date of Elden Ring Nightreign?')\nprint(result.output)\n```\n\n----------------------------------------\n\nTITLE: Run Pydantic AI agent without explicit HTTPX instrumentation\nDESCRIPTION: This Python example shows the default execution of a `pydantic-ai` agent without explicit HTTPX instrumentation. While `pydantic-ai` is instrumented, this snippet highlights the absence of detailed HTTP request/response capture, serving as a comparison to the fully instrumented version. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/logfire.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport logfire\n\nfrom pydantic_ai import Agent\n\nlogfire.configure()\nlogfire.instrument_pydantic_ai()\n\nagent = Agent('openai:gpt-4o')\nresult = agent.run_sync('What is the capital of France?')\nprint(result.output)\n\n```\n\n----------------------------------------\n\nTITLE: Access Dependencies in Pydantic AI System Prompt\nDESCRIPTION: This example illustrates how to access defined dependencies within a Pydantic AI agent's system prompt function.",
    "chunk_length": 2562
  },
  {
    "chunk_id": 72,
    "source": "pydantic_ai_llms_data",
    "content": "It shows that the `RunContext` object, parameterized with the dependency type, is passed as the first argument to the system prompt function. Dependencies are then accessed via the `.deps` attribute of the `RunContext`, enabling the prompt to use external services like an HTTP client with an API key. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/dependencies.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom dataclasses import dataclass\n\nimport httpx\n\nfrom pydantic_ai import Agent, RunContext\n\n\n@dataclass\nclass MyDeps:\n    api_key: str\n    http_client: httpx.AsyncClient\n\n\nagent = Agent(\n    'openai:gpt-4o',\n    deps_type=MyDeps,\n)\n\n\n@agent.system_prompt  # (1)! async def get_system_prompt(ctx: RunContext[MyDeps]) -> str:  # (2)! response = await ctx.deps.http_client.get(  # (3)! 'https://example.com',\n        headers={'Authorization': f'Bearer {ctx.deps.api_key}'},  # (4)! )\n    response.raise_for_status()\n    return f'Prompt: {response.text}'\n\n\nasync def main():\n    async with httpx.AsyncClient() as client:\n        deps = MyDeps('foobar', client)\n        result = await agent.run('Tell me a joke.', deps=deps)\n        print(result.output)\n        #> Did you hear about the toothpaste scandal? They called it Colgate. ```\n\n----------------------------------------\n\nTITLE: Integrate ACI.dev Tools with Pydantic AI Agent\nDESCRIPTION: This example illustrates how to incorporate tools from the ACI.dev library into a Pydantic AI agent using `ACIToolset`. It specifies the requirement for the `aci-sdk` package, setting the `ACI_API_KEY` environment variable, and providing a `linked_account_owner_id`. Similar to LangChain, Pydantic AI does not validate arguments for ACI tools. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/toolsets.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.ext.aci import ACIToolset\n\n\ntoolset = ACIToolset(\n    [\n        'OPEN_WEATHER_MAP__CURRENT_WEATHER',\n        'OPEN_WEATHER_MAP__FORECAST',\n    ],\n    linked_account_owner_id=os.getenv('LINKED_ACCOUNT_OWNER_ID'),\n)\n\nagent = Agent('openai:gpt-4o', toolsets=[toolset])\n```\n\n----------------------------------------\n\nTITLE: Reusing messages in a conversation with Pydantic AI agents\nDESCRIPTION: This example demonstrates how to continue a conversation with a Pydantic AI agent by passing the message history from a previous run.",
    "chunk_length": 2403
  },
  {
    "chunk_id": 73,
    "source": "pydantic_ai_llms_data",
    "content": "By providing `message_history` to `Agent.run_sync`, the agent maintains context without generating a new system prompt, allowing for multi-turn interactions. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/message-history.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o', system_prompt='Be a helpful assistant.')\n\nresult1 = agent.run_sync('Tell me a joke.')\nprint(result1.output)\n# Did you hear about the toothpaste scandal? They called it Colgate. result2 = agent.run_sync('Explain?', message_history=result1.new_messages())\nprint(result2.output)\n# This is an excellent joke invented by Samuel Colvin, it needs no explanation. print(result2.all_messages())\n\"\"\"\n[\n    ModelRequest(\n        parts=[\n            SystemPromptPart(\n                content='Be a helpful assistant.',\n                timestamp=datetime.datetime(...),\n            ),\n            UserPromptPart(\n                content='Tell me a joke.',\n                timestamp=datetime.datetime(...),\n            ),\n        ]\n    ),\n    ModelResponse(\n        parts=[\n            TextPart(\n                content='Did you hear about the toothpaste scandal? They called it Colgate.'\n            )\n        ],\n        usage=Usage(requests=1, request_tokens=60, response_tokens=12, total_tokens=72),\n        model_name='gpt-4o',\n        timestamp=datetime.datetime(...),\n    ),\n    ModelRequest(\n        parts=[\n            UserPromptPart(\n                content='Explain?',\n                timestamp=datetime.datetime(...),\n            )\n        ]\n    ),\n    ModelResponse(\n        parts=[\n            TextPart(\n                content='This is an excellent joke invented by Samuel Colvin, it needs no explanation.'\n            )\n        ],\n        usage=Usage(requests=1, request_tokens=61, response_tokens=26, total_tokens=87),\n        model_name='gpt-4o',\n        timestamp=datetime.datetime(...),\n    ),\n]\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Handling Diverse Tool Output Types in Pydantic-AI\nDESCRIPTION: This example illustrates the flexibility of Pydantic-AI tools in returning various data types, including standard Python objects like `datetime`, Pydantic `BaseModel` instances, and specialized multi-modal types like `ImageUrl` and `DocumentUrl`.",
    "chunk_length": 2309
  },
  {
    "chunk_id": 74,
    "source": "pydantic_ai_llms_data",
    "content": "It demonstrates how the agent automatically handles serialization of these outputs to JSON or processes them according to the model's capabilities, enabling rich interactions and responses. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/tools.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime\n\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent, DocumentUrl, ImageUrl\nfrom pydantic_ai.models.openai import OpenAIResponsesModel\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nagent = Agent(model=OpenAIResponsesModel('gpt-4o'))\n\n\n@agent.tool_plain\ndef get_current_time() -> datetime:\n    return datetime.now()\n\n\n@agent.tool_plain\ndef get_user() -> User:\n    return User(name='John', age=30)\n\n\n@agent.tool_plain\ndef get_company_logo() -> ImageUrl:\n    return ImageUrl(url='https://iili.io/3Hs4FMg.png')\n\n\n@agent.tool_plain\ndef get_document() -> DocumentUrl:\n    return DocumentUrl(url='https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf')\n\n\nresult = agent.run_sync('What time is it?')\nprint(result.output)\n\nresult = agent.run_sync('What is the user name?')\nprint(result.output)\n\nresult = agent.run_sync('What is the company name in the logo?')\nprint(result.output)\n\nresult = agent.run_sync('What is the main content of the document?')\nprint(result.output)\n```\n\n----------------------------------------\n\nTITLE: Control Concurrency in Pydantic Evals Dataset Evaluation\nDESCRIPTION: This Python example demonstrates how to evaluate a dataset with and without concurrency limits using `pydantic-evals`. It showcases the use of `dataset.evaluate_sync` to run a function against multiple test cases, first with unlimited concurrency and then with `max_concurrency=1` to illustrate the performance difference when operations are serialized. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/evals.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport time\n\nfrom pydantic_evals import Case, Dataset\n\n# Create a dataset with multiple test cases\ndataset = Dataset(\n    cases=[\n        Case(\n            name=f'case_{i}',\n            inputs=i,\n            expected_output=i * 2,\n        )\n        for i in range(5)\n    ]\n)\n\n\nasync def double_number(input_value: int) -> int:\n    \"\"\"Function that simulates work by sleeping for a tenth of a second before returning double the input.\"\"\"\n    await asyncio.sleep(0.1)  # Simulate work\n    return input_value * 2\n\n\n# Run evaluation with unlimited concurrency\nt0 = time.time()\nreport_default = dataset.evaluate_sync(double_number)\nprint(f'Evaluation took less than 0.5s: {time.time() - t0 < 0.5}')\n# Evaluation took less than 0.5s: True\n\nreport_default.print(include_input=True, include_output=True, include_durations=False)\n#       Evaluation Summary:\n#          double_number\n# \n#  Case ID   Inputs  Outputs \n# \n#  case_0    0       0       \n# \n#  case_1    1       2       \n# \n#  case_2    2       4       \n# \n#  case_3    3       6       \n# \n#  case_4    4       8       \n# \n#  Averages                  \n# \n\n# Run evaluation with limited concurrency\nt0 = time.time()\nreport_limited = dataset.evaluate_sync(double_number, max_concurrency=1)\nprint(f'Evaluation took more than 0.5s: {time.time() - t0 > 0.5}')\n# Evaluation took more than 0.5s: True\n\nreport_limited.print(include_input=True, include_output=True, include_durations=False)\n#       Evaluation Summary:\n#          double_number\n# \n#  Case ID   Inputs  Outputs \n# \n#  case_0    0       0       \n# \n#  case_1    1       2       \n# \n#  case_2    2       4       \n# \n#  case_3    3       6       \n# \n#  case_4    4       8       \n# \n#  Averages                  \n# \n```\n\n----------------------------------------\n\nTITLE: Pydantic-Graph: Dependency Injection with ProcessPoolExecutor\nDESCRIPTION: This Python example demonstrates how to implement dependency injection within `pydantic-graph` nodes.",
    "chunk_length": 4438
  },
  {
    "chunk_id": 75,
    "source": "pydantic_ai_llms_data",
    "content": "It shows how to define a `GraphDeps` class to hold dependencies (e.g., a `ProcessPoolExecutor`), pass these dependencies to the graph run, and access them within `BaseNode` subclasses via `GraphRunContext.deps` to offload computation to a separate process. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/graph.md#_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\nfrom __future__ import annotations\n\nimport asyncio\nfrom concurrent.futures import ProcessPoolExecutor\nfrom dataclasses import dataclass\n\nfrom pydantic_graph import BaseNode, End, FullStatePersistence, Graph, GraphRunContext\n\n\n@dataclass\nclass GraphDeps:\n    executor: ProcessPoolExecutor\n\n\n@dataclass\nclass DivisibleBy5(BaseNode[None, GraphDeps, int]):\n    foo: int\n\n    async def run(\n        self,\n        ctx: GraphRunContext[None, GraphDeps],\n    ) -> Increment | End[int]:\n        if self.foo % 5 == 0:\n            return End(self.foo)\n        else:\n            return Increment(self.foo)\n\n\n@dataclass\nclass Increment(BaseNode[None, GraphDeps]):\n    foo: int\n\n    async def run(self, ctx: GraphRunContext[None, GraphDeps]) -> DivisibleBy5:\n        loop = asyncio.get_running_loop()\n        compute_result = await loop.run_in_executor(\n            ctx.deps.executor,\n            self.compute,\n        )\n        return DivisibleBy5(compute_result)\n\n    def compute(self) -> int:\n        return self.foo + 1\n\n\nfives_graph = Graph(nodes=[DivisibleBy5, Increment])\n\n\nasync def main():\n    with ProcessPoolExecutor() as executor:\n        deps = GraphDeps(executor)\n        result = await fives_graph.run(DivisibleBy5(3), deps=deps, persistence=FullStatePersistence())\n    print(result.output)\n    #> 5\n    # the full history is quite verbose (see below), so we'll just print the summary\n    print([item.node for item in result.persistence.history])\n    \"\"\"\n    [\n        DivisibleBy5(foo=3),\n        Increment(foo=3),\n        DivisibleBy5(foo=4),\n        Increment(foo=4),\n        DivisibleBy5(foo=5),\n        End(data=5),\n    ]\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Rename tools in Pydantic-AI toolset\nDESCRIPTION: Shows how to rename tools in a Pydantic-AI toolset using `RenamedToolset` or the `renamed()` method.",
    "chunk_length": 2208
  },
  {
    "chunk_id": 76,
    "source": "pydantic_ai_llms_data",
    "content": "This is useful for clarifying ambiguous names or resolving conflicts without lengthy prefixes. The example renames specific weather and datetime tools, and `TestModel` is used to confirm the new tool names. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/toolsets.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nfrom combined_toolset import combined_toolset\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.test import TestModel\n\n\nrenamed_toolset = combined_toolset.renamed(\n    {\n        'current_time': 'datetime_now',\n        'temperature_celsius': 'weather_temperature_celsius',\n        'temperature_fahrenheit': 'weather_temperature_fahrenheit'\n    }\n)\n\ntest_model = TestModel()\nagent = Agent(test_model, toolsets=[renamed_toolset])\nresult = agent.run_sync('What tools are available?')\nprint([t.name for t in test_model.last_model_request_parameters.function_tools])\n\"\"\"\n['temperature_celsius', 'temperature_fahrenheit', 'weather_conditions', 'current_time']\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Main Application Flow for Flight and Seat Booking\nDESCRIPTION: This `main` asynchronous function orchestrates the application's flow. It initializes `Usage` tracking, then calls `find_flight` (assumed to be defined elsewhere) to get flight details. If a flight is found, it proceeds to call `find_seat` to determine the user's seat preference, demonstrating the sequential interaction with the defined AI agents. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/multi-agent-applications.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nasync def main():  # (7)! usage: Usage = Usage()\n\n    opt_flight_details = await find_flight(usage)\n    if opt_flight_details is not None:\n        print(f'Flight found: {opt_flight_details.flight_number}')\n        #> Flight found: AK456\n        seat_preference = await find_seat(usage)\n        print(f'Seat preference: {seat_preference}')\n        #> Seat preference: row=1 seat='A'\n```\n\n----------------------------------------\n\nTITLE: Prefix tool names in Pydantic-AI toolset\nDESCRIPTION: Illustrates how to add prefixes to tool names within a Pydantic-AI toolset using `PrefixedToolset` or the `prefixed()` method.",
    "chunk_length": 2196
  },
  {
    "chunk_id": 77,
    "source": "pydantic_ai_llms_data",
    "content": "This technique helps prevent naming conflicts when combining multiple toolsets. The example demonstrates prefixing 'weather' and 'datetime' tools, then verifies the prefixed names via `TestModel`. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/toolsets.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nfrom function_toolset import weather_toolset, datetime_toolset\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.test import TestModel\nfrom pydantic_ai.toolsets import CombinedToolset\n\n\ncombined_toolset = CombinedToolset(\n    [\n        weather_toolset.prefixed('weather'),\n        datetime_toolset.prefixed('datetime')\n    ]\n)\n\ntest_model = TestModel()\nagent = Agent(test_model, toolsets=[combined_toolset])\nresult = agent.run_sync('What tools are available?')\nprint([t.name for t in test_model.last_model_request_parameters.function_tools])\n\"\"\"\n[\n    'weather_temperature_celsius',\n    'weather_temperature_fahrenheit',\n    'weather_conditions',\n    'datetime_now',\n]\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Save and Load Pydantic Evals Datasets\nDESCRIPTION: This example demonstrates how to persist and retrieve `pydantic-evals` datasets to and from file systems. It uses the `to_file()` method to save a `Dataset` object to a YAML file, showcasing the structured output. Subsequently, it uses the `from_file()` class method to load the dataset back into memory, verifying that the cases are correctly loaded. This functionality is crucial for managing and reusing evaluation configurations. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/evals.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom pathlib import Path\n\nfrom judge_recipes import CustomerOrder, Recipe, recipe_dataset\n\nfrom pydantic_evals import Dataset\n\nrecipe_transforms_file = Path('recipe_transform_tests.yaml')\nrecipe_dataset.to_file(recipe_transforms_file)\nprint(recipe_transforms_file.read_text())\n\n# Load dataset from file\nloaded_dataset = Dataset[CustomerOrder, Recipe, dict].from_file(recipe_transforms_file)\n\nprint(f'Loaded dataset with {len(loaded_dataset.cases)} cases')\n```\n\n----------------------------------------\n\nTITLE: Configure Logfire for Pydantic Evals Tracing\nDESCRIPTION: This Python snippet demonstrates the basic setup for integrating Pydantic Evals with Logfire.",
    "chunk_length": 2300
  },
  {
    "chunk_id": 78,
    "source": "pydantic_ai_llms_data",
    "content": "It configures the Logfire SDK to send traces, specifying the environment as 'development' and the service name as 'evals'. The `send_to_logfire` parameter ensures traces are sent only if a token is present, enabling detailed monitoring of evaluation runs. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/pydantic_evals/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport logfire\n\nlogfire.configure(\n    send_to_logfire='if-token-present',\n    environment='development',\n    service_name='evals',\n)\n\n... my_dataset.evaluate_sync(my_task)\n```\n\n----------------------------------------\n\nTITLE: Configure Logfire for Pydantic Evals Tracing\nDESCRIPTION: This Python code demonstrates how to configure Pydantic Logfire to send OpenTelemetry traces from Pydantic Evals. It shows setting `send_to_logfire` based on token presence, defining the `environment` for filtering, and specifying a `service_name` for identification in the Logfire UI. It also includes an example of evaluating a dataset synchronously. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/evals.md#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nimport logfire\nfrom judge_recipes import recipe_dataset, transform_recipe\n\nlogfire.configure(\n    send_to_logfire='if-token-present',  # (1)! environment='development',  # (2)! service_name='evals',  # (3)! )\n\nrecipe_dataset.evaluate_sync(transform_recipe)\n```\n\n----------------------------------------\n\nTITLE: FunctionModel Usage Example for Agent Testing\nDESCRIPTION: This Python code demonstrates how to use `FunctionModel` to override an `Agent`'s behavior for unit testing. It defines an asynchronous `model_function` that simulates a model's response, then uses `FunctionModel` within an `Agent`'s `override` context to test the agent's output, asserting the expected 'hello world' result. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/api/models/function.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.messages import ModelMessage, ModelResponse, TextPart\nfrom pydantic_ai.models.function import FunctionModel, AgentInfo\n\nmy_agent = Agent('openai:gpt-4o')\n\n\nasync def model_function(\n    messages: list[ModelMessage], info: AgentInfo\n) -> ModelResponse:\n    print(messages)\n    \"\"\"\n    [\n        ModelRequest(\n            parts=[\n                UserPromptPart(\n                    content='Testing my agent...',\n                    timestamp=datetime.datetime(...),\n                )\n            ]\n        )\n    ]\n    \"\"\"\n    print(info)\n    \"\"\"\n    AgentInfo(\n        function_tools=[], allow_text_output=True, output_tools=[], model_settings=None\n    )\n    \"\"\"\n    return ModelResponse(parts=[TextPart('hello world')])\n\n\nasync def test_my_agent():\n    \"\"\"Unit test for my_agent, to be run by pytest.\"\"\"\n    with my_agent.override(model=FunctionModel(model_function)):\n        result = await my_agent.run('Testing my agent...')\n        assert result.output == 'hello world'\n```\n\n----------------------------------------\n\nTITLE: Implement Network Error Retries for HTTP Clients in Python\nDESCRIPTION: Defines a client setup that automatically retries requests upon common network issues such as timeouts, connection errors, and read errors.",
    "chunk_length": 3235
  },
  {
    "chunk_id": 79,
    "source": "pydantic_ai_llms_data",
    "content": "It leverages `tenacity` to catch specific `httpx` exceptions and apply an exponential backoff strategy. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/retries.md#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nimport httpx\nfrom tenacity import AsyncRetrying, retry_if_exception_type, wait_exponential, stop_after_attempt\nfrom pydantic_ai.retries import AsyncTenacityTransport\n\ndef create_network_resilient_client():\n    \"\"\"Create a client that handles network errors with retries.\"\"\"\n    transport = AsyncTenacityTransport(\n        controller=AsyncRetrying(\n            retry=retry_if_exception_type((\n                httpx.TimeoutException,\n                httpx.ConnectError,\n                httpx.ReadError\n            )),\n            wait=wait_exponential(multiplier=1, max=10),\n            stop=stop_after_attempt(3),\n            reraise=True\n        )\n    )\n    return httpx.AsyncClient(transport=transport)\n\n# Example usage\nclient = create_network_resilient_client()\n# Client will now retry on timeout, connection, and read errors\n```\n\n----------------------------------------\n\nTITLE: Extracting Box Dimensions or Text with Pydantic AI\nDESCRIPTION: This example demonstrates configuring a `pydantic-ai.Agent` to extract structured data (a `Box` object) or return a plain string if complete information is not available. It showcases how the agent prompts for missing data and then successfully parses the structured output once all details are provided, highlighting the flexibility of `output_type` with multiple choices. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/output.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent\n\n\nclass Box(BaseModel):\n    width: int\n    height: int\n    depth: int\n    units: str\n\n\nagent = Agent(\n    'openai:gpt-4o-mini',\n    output_type=[Box, str], # (1)! system_prompt=(\n        \"Extract me the dimensions of a box, \"\n        \"if you can't extract all data, ask the user to try again.\"\n    ),\n)\n\nresult = agent.run_sync('The box is 10x20x30')\nprint(result.output)\n#> Please provide the units for the dimensions (e.g., cm, in, m).",
    "chunk_length": 2148
  },
  {
    "chunk_id": 80,
    "source": "pydantic_ai_llms_data",
    "content": "result = agent.run_sync('The box is 10x20x30 cm')\nprint(result.output)\n#> width=10 height=20 depth=30 units='cm'\n```\n\n----------------------------------------\n\nTITLE: Filter tools in Pydantic-AI toolset\nDESCRIPTION: Demonstrates how to filter available tools in a Pydantic-AI toolset using `FilteredToolset` or the `filtered()` method. This example shows how to exclude tools whose names contain 'fahrenheit' by providing a lambda function that evaluates the tool's context and definition. It uses `TestModel` to inspect the tools available to the agent. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/toolsets.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfrom combined_toolset import combined_toolset\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.test import TestModel\n\nfiltered_toolset = combined_toolset.filtered(lambda ctx, tool_def: 'fahrenheit' not in tool_def.name)\n\ntest_model = TestModel()\nagent = Agent(test_model, toolsets=[filtered_toolset])\nresult = agent.run_sync('What tools are available?')\nprint([t.name for t in test_model.last_model_request_parameters.function_tools])\n#> ['weather_temperature_celsius', 'weather_conditions', 'datetime_now']\n```\n\n----------------------------------------\n\nTITLE: Iterating Pydantic Graph nodes with `Graph.iter` and `async for`\nDESCRIPTION: This example demonstrates how to use `Graph.iter` to gain direct control over graph execution. It returns a context manager yielding a `GraphRun` object, which is an async-iterable over the graph's nodes. This allows recording or modifying nodes as they execute, with the final result available via `run.result`. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/graph.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations as _annotations\n\nfrom dataclasses import dataclass\nfrom pydantic_graph import Graph, BaseNode, End, GraphRunContext\n\n\n@dataclass\nclass CountDownState:\n    counter: int\n\n\n@dataclass\nclass CountDown(BaseNode[CountDownState, None, int]):\n    async def run(self, ctx: GraphRunContext[CountDownState]) -> CountDown | End[int]:\n        if ctx.state.counter <= 0:\n            return End(ctx.state.counter)\n        ctx.state.counter -= 1\n        return CountDown()\n\n\ncount_down_graph = Graph(nodes=[CountDown])\n\n\nasync def main():\n    state = CountDownState(counter=3)\n    async with count_down_graph.iter(CountDown(), state=state) as run:\n        async for node in run:\n            print('Node:', node)\n    print('Final output:', run.result.output)\n```\n\n----------------------------------------\n\nTITLE: Configure WebSearchTool with Advanced Options\nDESCRIPTION: Illustrates how to configure the `WebSearchTool` with various parameters such as `search_context_size`, `user_location`, `blocked_domains`, `allowed_domains`, and `max_uses` to customize web search behavior.",
    "chunk_length": 2837
  },
  {
    "chunk_id": 81,
    "source": "pydantic_ai_llms_data",
    "content": "Shows how to pass these configurations during agent initialization for more controlled search operations. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/builtin-tools.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent, WebSearchTool, WebSearchUserLocation\n\nagent = Agent(\n    'anthropic:claude-sonnet-4-0',\n    builtin_tools=[\n        WebSearchTool(\n            search_context_size='high',\n            user_location=WebSearchUserLocation(\n                city='San Francisco',\n                country='US',\n                region='CA',\n                timezone='America/Los_Angeles',\n            ),\n            blocked_domains=['example.com', 'spam-site.net'],\n            allowed_domains=None,  # Cannot use both blocked_domains and allowed_domains with Anthropic\n            max_uses=5,  # Anthropic only: limit tool usage\n        )\n    ],\n)\n\nresult = agent.run_sync('Use the web to get the current time.')\n# > In San Francisco, it's 8:21:41 pm PDT on Wednesday, August 6, 2025. ```\n\n----------------------------------------\n\nTITLE: Pydantic AI Agent Run Event Processing Example\nDESCRIPTION: This snippet illustrates how Pydantic AI agents process different event types during a run, such as `FunctionToolResultEvent` and reaching an `End` node. It shows how to capture and display output messages, including the final agent output, and demonstrates the execution flow using `asyncio.run(main())`. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/agents.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n                            )\n                        elif isinstance(event, FunctionToolResultEvent):\n                            output_messages.append(\n                                f'[Tools] Tool call {event.tool_call_id!r} returned => {event.result.content}'\n                            )\n            elif Agent.is_end_node(node):\n                # Once an End node is reached, the agent run is complete\n                assert run.result is not None\n                assert run.result.output == node.data.output\n                output_messages.append(f'=== Final Agent Output: {run.result.output} ===')\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n\n    print(output_messages)\n```\n\n----------------------------------------\n\nTITLE: Define and Register Functions as AI Agent Tools\nDESCRIPTION: This snippet demonstrates how to create a `FunctionToolset` in pydantic-ai to expose Python functions as tools for an AI agent.",
    "chunk_length": 2483
  },
  {
    "chunk_id": 82,
    "source": "pydantic_ai_llms_data",
    "content": "It illustrates three primary methods for registering functions: initializing the toolset with a list of functions, using the `@toolset.tool` decorator for class methods or standalone functions, and dynamically adding functions using `add_function()`. The example also shows how to verify which tools are made available to the agent. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/toolsets.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime\n\nfrom pydantic_ai import Agent, RunContext\nfrom pydantic_ai.models.test import TestModel\nfrom pydantic_ai.toolsets import FunctionToolset\n\n\ndef temperature_celsius(city: str) -> float:\n    return 21.0\n\n\ndef temperature_fahrenheit(city: str) -> float:\n    return 69.8\n\n\nweather_toolset = FunctionToolset(tools=[temperature_celsius, temperature_fahrenheit])\n\n\n@weather_toolset.tool\ndef conditions(ctx: RunContext, city: str) -> str:\n    if ctx.run_step % 2 == 0:\n        return \"It's sunny\"\n    else:\n        return \"It's raining\"\n\n\ndatetime_toolset = FunctionToolset()\ndatetime_toolset.add_function(lambda: datetime.now(), name='now')\n\ntest_model = TestModel()\nagent = Agent(test_model)\n\nresult = agent.run_sync('What tools are available?', toolsets=[weather_toolset])\nprint([t.name for t in test_model.last_model_request_parameters.function_tools])\n#> ['temperature_celsius', 'temperature_fahrenheit', 'conditions']\n\nresult = agent.run_sync('What tools are available?', toolsets=[datetime_toolset])\nprint([t.name for t in test_model.last_model_request_parameters.function_tools])\n#> ['now']\n```\n\n----------------------------------------\n\nTITLE: Example Model Exchange Messages for Pydantic AI Agent Testing\nDESCRIPTION: This snippet illustrates a typical message exchange between a Pydantic AI agent and its model during a test run. It shows the structure of `ModelResponse` (containing `ToolCallPart`), `ModelRequest` (containing `ToolReturnPart`), and a final `ModelResponse` with `TextPart`, demonstrating how tool calls and their results are represented. The `IsStr()` and `IsNow()` helpers are used for flexible assertion of dynamic values.",
    "chunk_length": 2122
  },
  {
    "chunk_id": 83,
    "source": "pydantic_ai_llms_data",
    "content": "SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/testing.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n[\n    ModelResponse(\n        parts=[\n            ToolCallPart(\n                tool_name='weather_forecast',\n                args={'location': 'London'},\n                tool_call_id=IsStr(),\n            )\n        ],\n        usage=Usage(\n            requests=1,\n            request_tokens=71,\n            response_tokens=7,\n            total_tokens=78,\n            details=None,\n        ),\n        model_name='test',\n        timestamp=IsNow(tz=timezone.utc),\n    ),\n    ModelRequest(\n        parts=[\n            ToolReturnPart(\n                tool_name='weather_forecast',\n                content='Sunny with a chance of rain',\n                tool_call_id=IsStr(),\n                timestamp=IsNow(tz=timezone.utc),\n            ),\n        ],\n    ),\n    ModelResponse(\n        parts=[\n            TextPart(\n                content='{\"weather_forecast\":\"Sunny with a chance of rain\"}',\n            )\n        ],\n        usage=Usage(\n            requests=1,\n            request_tokens=77,\n            response_tokens=16,\n            total_tokens=93,\n            details=None,\n        ),\n        model_name='test',\n        timestamp=IsNow(tz=timezone.utc),\n    ),\n]\n```\n\n----------------------------------------\n\nTITLE: Define Dependencies for Pydantic AI Agent\nDESCRIPTION: This code demonstrates how to define dependencies for a Pydantic AI agent. It shows creating a `dataclass` to hold dependency objects like an API key and an HTTP client, passing the `dataclass` type to the `Agent` constructor for type checking, and then providing an instance of the `dataclass` when running the agent. Although the dependencies are defined, they are not actively used within the agent's logic in this specific example. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/dependencies.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom dataclasses import dataclass\n\nimport httpx\n\nfrom pydantic_ai import Agent\n\n\n@dataclass\nclass MyDeps:  # (1)!",
    "chunk_length": 2051
  },
  {
    "chunk_id": 84,
    "source": "pydantic_ai_llms_data",
    "content": "api_key: str\n    http_client: httpx.AsyncClient\n\n\nagent = Agent(\n    'openai:gpt-4o',\n    deps_type=MyDeps,  # (2)! )\n\n\nasync def main():\n    async with httpx.AsyncClient() as client:\n        deps = MyDeps('foobar', client)\n        result = await agent.run(\n            'Tell me a joke.',\n            deps=deps,  # (3)! )\n        print(result.output)\n        #> Did you hear about the toothpaste scandal? They called it Colgate. ```\n\n----------------------------------------\n\nTITLE: Display Pydantic AI CLI Help\nDESCRIPTION: Use the `--help` flag with `uvx clai` to display the command-line interface's help message. This provides information on available commands, options, and usage. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/cli.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nuvx clai --help\n```\n\n----------------------------------------\n\nTITLE: Manually Drive Pydantic-AI Agent Iteration with next()\nDESCRIPTION: This example illustrates how to manually control the agent's execution flow using the `agent_run.next(...)` method. By passing the current node to `next()`, you can inspect or modify it before execution, or even skip nodes based on custom logic. The iteration continues until an `End` node is returned, signifying the completion of the agent run. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/agents.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_graph import End\n\nagent = Agent('openai:gpt-4o')\n\n\nasync def main():\n    async with agent.iter('What is the capital of France?') as agent_run:\n        node = agent_run.next_node  # (1)! all_nodes = [node]\n\n        # Drive the iteration manually:\n        while not isinstance(node, End):  # (2)! node = await agent_run.next(node)  # (3)! all_nodes.append(node)  # (4)! print(all_nodes)\n        \"\"\"\n        [\n            UserPromptNode(\n                user_prompt='What is the capital of France?',\n                instructions=None,\n                instructions_functions=[],\n                system_prompts=(),\n                system_prompt_functions=[],\n                system_prompt_dynamic_functions={},\n            ),\n            ModelRequestNode(\n                request=ModelRequest(\n                    parts=[\n                        UserPromptPart(\n                            content='What is the capital of France?',\n                            timestamp=datetime.datetime(...),\n                        )\n                    ]\n                )\n            ),\n            CallToolsNode(\n                model_response=ModelResponse(\n                    parts=[TextPart(content='The capital of France is Paris.')],\n                    usage=(\n                        Usage(\n                            requests=1,\n                            request_tokens=56,\n                            response_tokens=7,\n                            total_tokens=63,\n                        )\n                    ),\n                    model_name='gpt-4o',\n                    timestamp=datetime.datetime(...),\n                )\n            ),\n            End(data=FinalResult(output='The capital of France is Paris.')),\n        ]\n        \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Implement API Endpoint for Pydantic AI Agent with Deferred Tools\nDESCRIPTION: This Python function, `run_agent`, simulates an API endpoint designed to interact with a `pydantic-ai` agent, incorporating deferred tools.",
    "chunk_length": 3443
  },
  {
    "chunk_id": 85,
    "source": "pydantic_ai_llms_data",
    "content": "It demonstrates how to dynamically add a `DeferredToolset` at runtime, override the agent's `output_type` to include `DeferredToolCalls`, and manage message history for continued agent runs. This setup is crucial for handling scenarios where tool results are produced externally. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/toolsets.md#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nfrom deferred_toolset_agent import agent, PersonalizedGreeting\n\nfrom typing import Union\n\nfrom pydantic_ai.output import DeferredToolCalls\nfrom pydantic_ai.tools import ToolDefinition\nfrom pydantic_ai.toolsets import DeferredToolset\nfrom pydantic_ai.messages import ModelMessage\n\ndef run_agent(\n    messages: list[ModelMessage] = [], frontend_tools: list[ToolDefinition] = {}\n) -> tuple[Union[PersonalizedGreeting, DeferredToolCalls], list[ModelMessage]]:\n    deferred_toolset = DeferredToolset(frontend_tools)\n    result = agent.run_sync(\n        toolsets=[deferred_toolset], # (1)! output_type=[agent.output_type, DeferredToolCalls], # (2)! message_history=messages, # (3)! )\n    return result.output, result.new_messages()\n```\n\n----------------------------------------\n\nTITLE: Generate Mermaid Diagram for Pydantic-AI Graph\nDESCRIPTION: This Python snippet demonstrates how to generate a Mermaid diagram representation of a `pydantic-ai` graph. It imports the graph definition and a starting node from `vending_machine.py`, then calls the `mermaid_code` method on the graph object, passing the initial node. The output is a Mermaid syntax string that can be rendered into a visual state diagram. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/graph.md#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nfrom vending_machine import InsertCoin, vending_machine_graph\n\nvending_machine_graph.mermaid_code(start_node=InsertCoin)\n```\n\n----------------------------------------\n\nTITLE: Pydantic AI Bank Support Agent with Tools and Dependency Injection\nDESCRIPTION: This Python code demonstrates the implementation of a bank support agent using Pydantic AI.",
    "chunk_length": 2051
  },
  {
    "chunk_id": 86,
    "source": "pydantic_ai_llms_data",
    "content": "It defines `SupportDependencies` for injecting contextual data like `customer_id` and a `DatabaseConn`, and `SupportOutput` as a Pydantic `BaseModel` to enforce a structured response from the AI, including advice, card blocking status, and risk level. The `Agent` is configured with an OpenAI model, a static system prompt, and dynamic system prompts and tools (e.g., `customer_balance`) that leverage dependency injection via `RunContext`. The `main` function illustrates how to run the agent with specific dependencies and process its validated output. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/index.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dataclasses import dataclass\n\nfrom pydantic import BaseModel, Field\nfrom pydantic_ai import Agent, RunContext\n\nfrom bank_database import DatabaseConn\n\n\n@dataclass\nclass SupportDependencies:  # (3)! customer_id: int\n    db: DatabaseConn  # (12)! class SupportOutput(BaseModel):  # (13)! support_advice: str = Field(description='Advice returned to the customer')\n    block_card: bool = Field(description=\"Whether to block the customer's card\")\n    risk: int = Field(description='Risk level of query', ge=0, le=10)\n\n\nsupport_agent = Agent(  # (1)! 'openai:gpt-4o',  # (2)! deps_type=SupportDependencies,\n    output_type=SupportOutput,  # (9)! system_prompt=(  # (4)! 'You are a support agent in our bank, give the '\n        'customer support and judge the risk level of their query.'\n    ),\n)\n\n\n@support_agent.system_prompt  # (5)! async def add_customer_name(ctx: RunContext[SupportDependencies]) -> str:\n    customer_name = await ctx.deps.db.customer_name(id=ctx.deps.customer_id)\n    return f\"The customer's name is {customer_name!r}\"\n\n\n@support_agent.tool  # (6)! async def customer_balance(\n    ctx: RunContext[SupportDependencies], include_pending: bool\n) -> float:\n    \"\"\"Returns the customer's current account balance.\"\"\"  # (7)! return await ctx.deps.db.customer_balance(\n        id=ctx.deps.customer_id,\n        include_pending=include_pending,\n    )\n\n\n... # (11)! async def main():\n    deps = SupportDependencies(customer_id=123, db=DatabaseConn())\n    result = await support_agent.run('What is my balance?', deps=deps)  # (8)!",
    "chunk_length": 2206
  },
  {
    "chunk_id": 87,
    "source": "pydantic_ai_llms_data",
    "content": "print(result.output)  # (10)! \"\"\"\n    support_advice='Hello John, your current account balance, including pending transactions, is $123.45.' block_card=False risk=1\n    \"\"\"\n\n    result = await support_agent.run('I just lost my card!', deps=deps)\n    print(result.output)\n    \"\"\"\n    support_advice=\"I'm sorry to hear that, John. We are temporarily blocking your card to prevent unauthorized transactions.\" block_card=True risk=8\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Define and Run a Pydantic AI Agent with a Tool\nDESCRIPTION: This example demonstrates how to create a Pydantic AI Agent that simulates a roulette wheel. It defines an Agent with specific dependency and output types, registers a tool function using the `@roulette_agent.tool` decorator, and then runs the agent synchronously with different inputs to show its behavior. The agent expects an integer dependency and produces a boolean output, while the tool checks if a given square matches the dependency. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/agents.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent, RunContext\n\nroulette_agent = Agent(  # (1)! 'openai:gpt-4o',\n    deps_type=int,\n    output_type=bool,\n    system_prompt=(\n        'Use the `roulette_wheel` function to see if the '\n        'customer has won based on the number they provide.'\n    ),\n)\n\n\n@roulette_agent.tool\nasync def roulette_wheel(ctx: RunContext[int], square: int) -> str:  # (2)! \"\"\"check if the square is a winner\"\"\"\n    return 'winner' if square == ctx.deps else 'loser'\n\n\n# Run the agent\nsuccess_number = 18  # (3)! result = roulette_agent.run_sync('Put my money on square eighteen', deps=success_number)\nprint(result.output)  # (4)! # > True\n\nresult = roulette_agent.run_sync('I bet five is the winner', deps=success_number)\nprint(result.output)\n# > False\n```\n\n----------------------------------------\n\nTITLE: Pydantic AI Core Classes: Agent and OpenAIModel\nDESCRIPTION: Documentation for the core `Agent` and `OpenAIModel` classes in `pydantic-ai`, including their constructors and key methods for model interaction and structured output.",
    "chunk_length": 2150
  },
  {
    "chunk_id": 88,
    "source": "pydantic_ai_llms_data",
    "content": "It also details the `AgentResult` and `Usage` objects returned by the agent. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/openai.md#_snippet_23\n\nLANGUAGE: APIDOC\nCODE:\n```\nAgent:\n  __init__(model: OpenAIModel, output_type: BaseModel = None)\n    model: An instance of OpenAIModel configured for a specific LLM. output_type: (Optional) A Pydantic BaseModel subclass to enforce structured output. run_sync(prompt: str) -> AgentResult\n    prompt: The input prompt string for the LLM. Returns: An AgentResult object containing the LLM's output and usage statistics. AgentResult:\n    output: The parsed output, typically an instance of the specified output_type or a string. usage() -> Usage\n      Returns: A Usage object detailing request, response, and total token counts. Usage:\n      requests: Number of requests made. request_tokens: Tokens in the request. response_tokens: Tokens in the response. total_tokens: Total tokens used. OpenAIModel:\n  __init__(model_name: str, provider: Provider = OpenAIProvider())\n    model_name: The name of the LLM model (e.g., 'llama3.2', 'gpt-4o'). provider: An instance of a Provider class (e.g., OpenAIProvider, AzureProvider) to handle API communication. ```\n\n----------------------------------------\n\nTITLE: Limit Pydantic AI Agent Requests and Handle Tool Retries\nDESCRIPTION: This example illustrates how to prevent infinite loops or excessive tool calling in a Pydantic AI agent by setting a `request_limit` using `UsageLimits`. It also demonstrates how a tool can raise a `ModelRetry` exception to trigger retries, and how the `request_limit` can ultimately prevent an infinite retry loop. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/agents.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom typing_extensions import TypedDict\n\nfrom pydantic_ai import Agent, ModelRetry\nfrom pydantic_ai.exceptions import UsageLimitExceeded\nfrom pydantic_ai.usage import UsageLimits\n\n\nclass NeverOutputType(TypedDict):\n    \"\"\"\n    Never ever coerce data to this type. \"\"\"\n\n    never_use_this: str\n\n\nagent = Agent(\n    'anthropic:claude-3-5-sonnet-latest',\n    retries=3,\n    output_type=NeverOutputType,\n    system_prompt='Any time you get a response, call the `infinite_retry_tool` to produce another response.',\n)\n\n\n@agent.tool_plain(retries=5)  # (1)!",
    "chunk_length": 2319
  },
  {
    "chunk_id": 89,
    "source": "pydantic_ai_llms_data",
    "content": "def infinite_retry_tool() -> int:\n    raise ModelRetry('Please try again.')\n\n\ntry:\n    result_sync = agent.run_sync(\n        'Begin infinite retry loop!', usage_limits=UsageLimits(request_limit=3)  # (2)! )\nexcept UsageLimitExceeded as e:\n    print(e)\n    # The next request would exceed the request_limit of 3\n```\n\n----------------------------------------\n\nTITLE: Implement Asynchronous Output Validation with Pydantic AI Agent Decorator\nDESCRIPTION: Illustrates how to add custom asynchronous validation logic to an agent's output using the `@agent.output_validator` decorator. This is useful for validations requiring I/O or complex checks. The example shows validating a generated SQL query against a database and raising `ModelRetry` on failure. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/output.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Union\n\nfrom fake_database import DatabaseConn, QueryError\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent, RunContext, ModelRetry\n\n\nclass Success(BaseModel):\n    sql_query: str\n\n\nclass InvalidRequest(BaseModel):\n    error_message: str\n\n\nOutput = Union[Success, InvalidRequest]\nagent = Agent[DatabaseConn, Output](\n    'google-gla:gemini-1.5-flash',\n    output_type=Output,  # type: ignore\n    deps_type=DatabaseConn,\n    system_prompt='Generate PostgreSQL flavored SQL queries based on user input.',\n)\n\n\n@agent.output_validator\nasync def validate_sql(ctx: RunContext[DatabaseConn], output: Output) -> Output:\n    if isinstance(output, InvalidRequest):\n        return output\n    try:\n        await ctx.deps.execute(f'EXPLAIN {output.sql_query}')\n    except QueryError as e:\n        raise ModelRetry(f'Invalid query: {e}') from e\n    else:\n        return output\n\n\nresult = agent.run_sync(\n    'get me users who were last active yesterday.', deps=DatabaseConn()\n)\n# sql_query='SELECT * FROM users WHERE last_active::date = today() - interval 1 day'\n```\n\n----------------------------------------\n\nTITLE: Summarize Old Messages with LLM in Python\nDESCRIPTION: This example illustrates how to use a separate, potentially cheaper, language model to summarize older parts of the conversation history.",
    "chunk_length": 2189
  },
  {
    "chunk_id": 90,
    "source": "pydantic_ai_llms_data",
    "content": "This technique helps in reducing the total token count sent to the main agent while preserving essential context, making long conversations more efficient and cost-effective. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/message-history.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.messages import ModelMessage\n\n# Use a cheaper model to summarize old messages. summarize_agent = Agent(\n    'openai:gpt-4o-mini',\n    instructions=\"\"\"\nSummarize this conversation, omitting small talk and unrelated topics. Focus on the technical discussion and next steps. \"\"\",\n)\n\n\nasync def summarize_old_messages(messages: list[ModelMessage]) -> list[ModelMessage]:\n    # Summarize the oldest 10 messages\n    if len(messages) > 10:\n        oldest_messages = messages[:10]\n        summary = await summarize_agent.run(message_history=oldest_messages)\n        # Return the last message and the summary\n        return summary.new_messages() + messages[-1:]\n\n    return messages\n\n\nagent = Agent('openai:gpt-4o', history_processors=[summarize_old_messages])\n```\n\n----------------------------------------\n\nTITLE: Pydantic-AI Agent with Union Return Type\nDESCRIPTION: This Python example demonstrates configuring a `pydantic-ai` Agent to handle multiple possible output types using `typing.Union`. The agent is initialized with `Union[list[str], list[int]]` as its `output_type`, allowing it to extract either colors (strings) or sizes (integers) from text. It showcases how the agent dynamically adapts its output based on the input provided. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/output.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom typing import Union\n\nfrom pydantic_ai import Agent\n\nagent = Agent[None, Union[list[str], list[int]]](\n    'openai:gpt-4o-mini',\n    output_type=Union[list[str], list[int]],  # type: ignore # (1)! system_prompt='Extract either colors or sizes from the shapes provided.',\n)\n\nresult = agent.run_sync('red square, blue circle, green triangle')\nprint(result.output)\n#> ['red', 'blue', 'green']\n\nresult = agent.run_sync('square size 10, circle size 20, triangle size 30')\nprint(result.output)\n#> [10, 20, 30]\n```\n\n----------------------------------------\n\nTITLE: Vending Machine State Diagram (Mermaid)\nDESCRIPTION: This Mermaid code block defines a state diagram visualizing the `vending_machine_graph`'s flow.",
    "chunk_length": 2403
  },
  {
    "chunk_id": 91,
    "source": "pydantic_ai_llms_data",
    "content": "It illustrates the transitions between different states (nodes) like `InsertCoin`, `CoinsInserted`, `SelectProduct`, and `Purchase`, including the start `[*]` and end `[*]` states. This diagram helps in understanding the graph's execution path and node interactions. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/graph.md#_snippet_10\n\nLANGUAGE: Mermaid\nCODE:\n```\n--- \ntitle: vending_machine_graph\n---\nstateDiagram-v2\n  [*] --> InsertCoin\n  InsertCoin --> CoinsInserted\n  CoinsInserted --> SelectProduct\n  CoinsInserted --> Purchase\n  SelectProduct --> Purchase\n  Purchase --> InsertCoin\n  Purchase --> SelectProduct\n  Purchase --> [*]\n```\n\n----------------------------------------\n\nTITLE: Run Pydantic AI AG-UI Application with Uvicorn\nDESCRIPTION: This Bash command illustrates how to serve the Pydantic AI application, which is an ASGI application, using the Uvicorn server. It specifies the module and application object (`ag_ui_tool_events:app`) along with host and port settings, making the AG-UI integrated application accessible. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/ag-ui.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nuvicorn ag_ui_tool_events:app --host 0.0.0.0 --port 9000\n```\n\n----------------------------------------\n\nTITLE: Configure Google Model Settings\nDESCRIPTION: Demonstrates how to initialize `GoogleModelSettings` with various parameters like `temperature`, `max_tokens`, `google_thinking_config`, and `google_safety_settings` to customize the behavior of a `GoogleModel` when used with an `Agent`. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/google.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom google.genai.types import HarmBlockThreshold, HarmCategory\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.google import GoogleModel, GoogleModelSettings\n\nsettings = GoogleModelSettings(\n    temperature=0.2,\n    max_tokens=1024,\n    google_thinking_config={'thinking_budget': 2048},\n    google_safety_settings=[\n        {\n            'category': HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n            'threshold': HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n        }\n    ]\n)\nmodel = GoogleModel('gemini-1.5-flash')\nagent = Agent(model, model_settings=settings)\n...",
    "chunk_length": 2246
  },
  {
    "chunk_id": 92,
    "source": "pydantic_ai_llms_data",
    "content": "```\n\n----------------------------------------\n\nTITLE: Mermaid Diagram for Question Graph\nDESCRIPTION: This Mermaid `stateDiagram-v2` code visualizes the `question_graph` defined in the Python example. It illustrates the flow between the 'Ask', 'Answer', 'Evaluate', and 'Reprimand' nodes, including an explicit edge label 'Ask the question', a multi-line note attached to the 'Ask' node, and highlights the 'Answer' node using a custom CSS class definition. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/graph.md#_snippet_20\n\nLANGUAGE: mermaid\nCODE:\n```\n--- \ntitle: question_graph \n--- \nstateDiagram-v2 \n  Ask --> Answer: Ask the question \n  note right of Ask \n    Judge the answer. Decide on next step. end note \n  Answer --> Evaluate \n  Evaluate --> Reprimand \n  Evaluate --> [*]: success \n  Reprimand --> Ask \n\nclassDef highlighted fill:#fdff32 \nclass Answer highlighted\n```\n\n----------------------------------------\n\nTITLE: Implement Context-Aware History Processor in Python\nDESCRIPTION: This snippet demonstrates how to create a history processor that leverages the `RunContext` parameter to access real-time information about the current agent run, such as token usage. It shows how to dynamically filter messages, for example, by keeping only recent messages when token usage exceeds a threshold, to optimize costs or performance. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/message-history.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.messages import ModelMessage\nfrom pydantic_ai.tools import RunContext\n\n\ndef context_aware_processor(\n    ctx: RunContext[None],\n    messages: list[ModelMessage],\n) -> list[ModelMessage]:\n    # Access current usage\n    current_tokens = ctx.usage.total_tokens\n\n    # Filter messages based on context\n    if current_tokens > 1000:\n        return messages[-3:]  # Keep only recent messages when token usage is high\n    return messages\n\nagent = Agent('openai:gpt-4o', history_processors=[context_aware_processor])\n```\n\n----------------------------------------\n\nTITLE: Implement Pydantic-AI Agent Tool Retries with ModelRetry\nDESCRIPTION: This example illustrates how to implement self-correction in a Pydantic-AI agent by retrying tool calls.",
    "chunk_length": 2262
  },
  {
    "chunk_id": 93,
    "source": "pydantic_ai_llms_data",
    "content": "It demonstrates raising a `ModelRetry` exception within a tool function when a condition, such as a user not being found, is met. The agent is configured to retry the tool call a specified number of times, allowing the model to attempt a corrected response. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/agents.md#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent, RunContext, ModelRetry\n\nfrom fake_database import DatabaseConn\n\n\nclass ChatResult(BaseModel):\n    user_id: int\n    message: str\n\n\nagent = Agent(\n    'openai:gpt-4o',\n    deps_type=DatabaseConn,\n    output_type=ChatResult,\n)\n\n\n@agent.tool(retries=2)\ndef get_user_by_name(ctx: RunContext[DatabaseConn], name: str) -> int:\n    \"\"\"Get a user's ID from their full name.\"\"\"\n    print(name)\n    #> John\n    #> John Doe\n    user_id = ctx.deps.users.get(name=name)\n    if user_id is None:\n        raise ModelRetry(\n            f'No user found with name {name!r}, remember to provide their full name'\n        )\n    return user_id\n\n\nresult = agent.run_sync(\n    'Send a message to John Doe asking for coffee next week', deps=DatabaseConn()\n)\nprint(result.output)\n\"\"\"\nuser_id=123 message='Hello John, would you be free for coffee sometime next week? Let me know what works for you!'\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Python Vending Machine Stateful Graph Example\nDESCRIPTION: This Python code demonstrates a stateful graph implementation using `pydantic-graph` to simulate a vending machine. It defines a `MachineState` dataclass to track the user's balance and selected product. Various `BaseNode` classes (`InsertCoin`, `CoinsInserted`, `SelectProduct`, `Purchase`) represent the vending machine's operations, updating the shared `MachineState` as the graph progresses through user interactions like inserting coins and selecting products. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/graph.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\n\nfrom rich.prompt import Prompt\n\nfrom pydantic_graph import BaseNode, End, Graph, GraphRunContext\n\n\n@dataclass\nclass MachineState:\n    user_balance: float = 0.0\n    product: str | None = None\n\n\n@dataclass\nclass InsertCoin(BaseNode[MachineState]):\n    async def run(self, ctx: GraphRunContext[MachineState]) -> CoinsInserted:\n        return CoinsInserted(float(Prompt.ask('Insert coins')))\n\n\n@dataclass\nclass CoinsInserted(BaseNode[MachineState]):\n    amount: float\n\n    async def run(\n        self, ctx: GraphRunContext[MachineState]\n    ) -> SelectProduct | Purchase:\n        ctx.state.user_balance += self.amount\n        if ctx.state.product is not None:\n            return Purchase(ctx.state.product)\n        else:\n            return SelectProduct()\n\n\n@dataclass\nclass SelectProduct(BaseNode[MachineState]):\n    async def run(self, ctx: GraphRunContext[MachineState]) -> Purchase:\n        return Purchase(Prompt.ask('Select product'))\n\n\nPRODUCT_PRICES = {\n    'water': 1.25,\n    'soda': 1.50,\n    'crisps': 1.75,\n    'chocolate': 2.00,\n}\n\n\n@dataclass\nclass Purchase(BaseNode[MachineState, None, None]):\n    product: str\n\n    async def run(\n        self, ctx: GraphRunContext[MachineState]\n    ) -> End | InsertCoin | SelectProduct:\n        if price := PRODUCT_PRICES.get(self.product):\n            ctx.state.product = self.product\n            if ctx.state.user_balance >= price:\n                ctx.state.user_balance -= price\n                return End(None)\n            else:\n                diff = price - ctx.state.user_balance\n                print(f'Not enough money for {self.product}, need {diff:0.2f} more')\n                return InsertCoin()\n        else:\n            print(f'No such product: {self.product}, try again')\n            return SelectProduct()\n\n\nvending_machine_graph = Graph(\n    nodes=[InsertCoin, CoinsInserted, SelectProduct, Purchase]\n)\n\n\nasync def main():\n    state = MachineState()\n    await vending_machine_graph.run(InsertCoin(), state=state)\n    print(f'purchase successful item={state.product} change={state.user_balance:0.2f}')\n```\n\n----------------------------------------\n\nTITLE: Initialize Bedrock Model with Agent by Name\nDESCRIPTION: Shows a concise way to initialize an `Agent` instance in `pydantic-ai` by directly providing a Bedrock model name string, leveraging the library's internal model resolution.",
    "chunk_length": 4419
  },
  {
    "chunk_id": 94,
    "source": "pydantic_ai_llms_data",
    "content": "SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/bedrock.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('bedrock:anthropic.claude-3-sonnet-20240229-v1:0')\n... ```\n\n----------------------------------------\n\nTITLE: Unit Testing Pydantic AI Agent with TestModel\nDESCRIPTION: This Python example demonstrates how to use `TestModel` from `pydantic_ai.models.test` to facilitate unit testing of a `pydantic_ai.Agent`. By overriding the agent's model with `TestModel` using a context manager, developers can simulate responses and assert expected behavior without making actual external API calls. The snippet specifically checks for no tool calls and a 'success' output. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/api/models/test.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.test import TestModel\n\nmy_agent = Agent('openai:gpt-4o', system_prompt='...')\n\n\nasync def test_my_agent():\n    \"\"\"Unit test for my_agent, to be run by pytest.\"\"\"\n    m = TestModel()\n    with my_agent.override(model=m):\n        result = await my_agent.run('Testing my agent...')\n        assert result.output == 'success (no tool calls)'\n    assert m.last_model_request_parameters.function_tools == []\n```\n\n----------------------------------------\n\nTITLE: Run AG-UI FastAPI Server with Uvicorn\nDESCRIPTION: Command to launch the FastAPI application as an ASGI server using Uvicorn, making the Pydantic AI agent accessible via the AG-UI protocol. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/ag-ui.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nuvicorn run_ag_ui:app\n```\n\n----------------------------------------\n\nTITLE: Streaming results and messages with Pydantic-AI Agent\nDESCRIPTION: Illustrates how to use `StreamedRunResult` to process agent responses asynchronously. This example sets up an agent, initiates a streaming run with `agent.run_stream`, and then demonstrates accessing incomplete messages before the stream finishes, iterating over streamed text parts using `result.stream_text()`, and finally retrieving complete messages once the stream has concluded.",
    "chunk_length": 2181
  },
  {
    "chunk_id": 95,
    "source": "pydantic_ai_llms_data",
    "content": "It highlights the dynamic nature of message objects during streaming. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/message-history.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o', system_prompt='Be a helpful assistant.')\n\n\nasync def main():\n    async with agent.run_stream('Tell me a joke.') as result:\n        # incomplete messages before the stream finishes\n        print(result.all_messages())\n        \"\"\"\n        [\n            ModelRequest(\n                parts=[\n                    SystemPromptPart(\n                        content='Be a helpful assistant.',\n                        timestamp=datetime.datetime(...),\n                    ),\n                    UserPromptPart(\n                        content='Tell me a joke.',\n                        timestamp=datetime.datetime(...),\n                    ),\n                ]\n            )\n        ]\n        \"\"\"\n\n        async for text in result.stream_text():\n            print(text)\n            # > Did you hear\n            # > Did you hear about the toothpaste\n            # > Did you hear about the toothpaste scandal? They called\n            # > Did you hear about the toothpaste scandal? They called it Colgate. # complete messages once the stream finishes\n        print(result.all_messages())\n        \"\"\"\n        [\n            ModelRequest(\n                parts=[\n                    SystemPromptPart(\n                        content='Be a helpful assistant.',\n                        timestamp=datetime.datetime(...),\n                    ),\n                    UserPromptPart(\n                        content='Tell me a joke.',\n                        timestamp=datetime.datetime(...),\n                    ),\n                ]\n            ),\n            ModelResponse(\n                parts=[\n                    TextPart(\n                        content='Did you hear about the toothpaste scandal? They called it Colgate.'\n                    )\n                ],\n                usage=Usage(request_tokens=50, response_tokens=12, total_tokens=62),\n                model_name='gpt-4o',\n                timestamp=datetime.datetime(...),\n            ),\n        ]\n        \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Evaluate AI Model for Recipe Generation with LLMJudge\nDESCRIPTION: This snippet demonstrates how to set up and run an evaluation for an AI recipe generation agent using `pydantic-evals`.",
    "chunk_length": 2453
  },
  {
    "chunk_id": 96,
    "source": "pydantic_ai_llms_data",
    "content": "It defines `CustomerOrder` and `Recipe` models, initializes an `Agent` for recipe generation, and creates a `Dataset` with multiple `Case` entries. The evaluation utilizes `LLMJudge` for both case-specific and dataset-level rubrics, allowing for flexible assessment of the generated recipes based on dietary restrictions and general quality. The `evaluate_sync` method runs the evaluation and prints a summary report. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/evals.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations\n\nfrom typing import Any\n\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent, format_as_xml\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import IsInstance, LLMJudge\n\n\nclass CustomerOrder(BaseModel):\n    dish_name: str\n    dietary_restriction: str | None = None\n\n\nclass Recipe(BaseModel):\n    ingredients: list[str]\n    steps: list[str]\n\n\nrecipe_agent = Agent(\n    'groq:llama-3.3-70b-versatile',\n    output_type=Recipe,\n    system_prompt=(\n        'Generate a recipe to cook the dish that meets the dietary restrictions.'\n    ),\n)\n\n\nasync def transform_recipe(customer_order: CustomerOrder) -> Recipe:\n    r = await recipe_agent.run(format_as_xml(customer_order))\n    return r.output\n\n\nrecipe_dataset = Dataset[CustomerOrder, Recipe, Any](\n    cases=[\n        Case(\n            name='vegetarian_recipe',\n            inputs=CustomerOrder(\n                dish_name='Spaghetti Bolognese', dietary_restriction='vegetarian'\n            ),\n            expected_output=None,\n            metadata={'focus': 'vegetarian'},\n            evaluators=(\n                LLMJudge(\n                    rubric='Recipe should not contain meat or animal products',\n                ),\n            ),\n        ),\n        Case(\n            name='gluten_free_recipe',\n            inputs=CustomerOrder(\n                dish_name='Chocolate Cake', dietary_restriction='gluten-free'\n            ),\n            expected_output=None,\n            metadata={'focus': 'gluten-free'},\n            # Case-specific evaluator with a focused rubric\n            evaluators=(\n                LLMJudge(\n                    rubric='Recipe should not contain gluten or wheat products',\n                ),\n            ),\n        ),\n    ],\n    evaluators=[\n        IsInstance(type_name='Recipe'),\n        LLMJudge(\n            rubric='Recipe should have clear steps and relevant ingredients',\n            include_input=True,\n            model='anthropic:claude-3-7-sonnet-latest',\n        ),\n    ],\n)\n\n\nreport = recipe_dataset.evaluate_sync(transform_recipe)\nprint(report)\n```\n\n----------------------------------------\n\nTITLE: Fixing Jupyter Notebook RuntimeError with nest-asyncio\nDESCRIPTION: This snippet demonstrates how to resolve the 'RuntimeError: This event loop is already running' in Jupyter Notebooks, Google Colab, and Marimo.",
    "chunk_length": 2895
  },
  {
    "chunk_id": 97,
    "source": "pydantic_ai_llms_data",
    "content": "The error arises from conflicts between Jupyter's event loop and Pydantic AI's. Applying `nest_asyncio` before executing any agent runs helps manage these conflicts, allowing for smooth operation. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/troubleshooting.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport nest_asyncio\n\nnest_asyncio.apply()\n```\n\n----------------------------------------\n\nTITLE: Initialize GroqModel with Custom GroqProvider\nDESCRIPTION: This Python snippet illustrates how to provide a custom `GroqProvider` instance to the `GroqModel` constructor. This allows for direct configuration of the API key or other provider-specific settings, overriding environment variables if needed. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/groq.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.groq import GroqModel\nfrom pydantic_ai.providers.groq import GroqProvider\n\nmodel = GroqModel(\n    'llama-3.3-70b-versatile', provider=GroqProvider(api_key='your-api-key')\n)\nagent = Agent(model)\n... ```\n\n----------------------------------------\n\nTITLE: Stream Agent Output and Events with run_stream() in Python\nDESCRIPTION: This Python snippet demonstrates how to use `pydantic_ai.Agent.run_stream()` to stream both the final text output and intermediate events during an agent's execution. It shows how to define an `event_stream_handler` to process various `AgentStreamEvent` types, providing insights into tool calls, part deltas, and final result production. The example includes a `weather_forecast` tool and illustrates the sequence of events during a tool-augmented agent run. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/agents.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom collections.abc import AsyncIterable\nfrom datetime import date\nfrom typing import Union\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.messages import (\n    AgentStreamEvent,\n    FinalResultEvent,\n    FunctionToolCallEvent,\n    FunctionToolResultEvent,\n    HandleResponseEvent,\n    PartDeltaEvent,\n    PartStartEvent,\n    TextPartDelta,\n    ThinkingPartDelta,\n    ToolCallPartDelta,\n)\nfrom pydantic_ai.tools import RunContext\n\nweather_agent = Agent(\n    'openai:gpt-4o',\n    system_prompt='Providing a weather forecast at the locations the user provides.',\n)\n\n\n@weather_agent.tool\nasync def weather_forecast(\n    ctx: RunContext,\n    location: str,\n    forecast_date: date,\n) -> str:\n    return f'The forecast in {location} on {forecast_date} is 24C and sunny.'\n\n\noutput_messages: list[str] = []\n\n\nasync def event_stream_handler(\n    ctx: RunContext,\n    event_stream: AsyncIterable[Union[AgentStreamEvent, HandleResponseEvent]],\n):\n    async for event in event_stream:\n        if isinstance(event, PartStartEvent):\n            output_messages.append(f'[Request] Starting part {event.index}: {event.part!r}')\n        elif isinstance(event, PartDeltaEvent):\n            if isinstance(event.delta, TextPartDelta):\n                output_messages.append(f'[Request] Part {event.index} text delta: {event.delta.content_delta!r}')\n            elif isinstance(event.delta, ThinkingPartDelta):\n                output_messages.append(f'[Request] Part {event.index} thinking delta: {event.delta.content_delta!r}')\n            elif isinstance(event.delta, ToolCallPartDelta):\n                output_messages.append(f'[Request] Part {event.index} args delta: {event.delta.args_delta}')\n        elif isinstance(event, FunctionToolCallEvent):\n            output_messages.append(\n                f'[Tools] The LLM calls tool={event.part.tool_name!r} with args={event.part.args} (tool_call_id={event.part.tool_call_id!r})'\n            )\n        elif isinstance(event, FunctionToolResultEvent):\n            output_messages.append(f'[Tools] Tool call {event.tool_call_id!r} returned => {event.result.content}')\n        elif isinstance(event, FinalResultEvent):\n            output_messages.append(f'[Result] The model starting producing a final result (tool_name={event.tool_name})')\n\n\nasync def main():\n    user_prompt = 'What will the weather be like in Paris on Tuesday?'\n\n    async with weather_agent.run_stream(user_prompt, event_stream_handler=event_stream_handler) as run:\n        async for output in run.stream_text():\n            output_messages.append(f'[Output] {output}')\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n\n    print(output_messages)\n    \"\"\"\n    [\n        \"[Request] Starting part 0: ToolCallPart(tool_name='weather_forecast', tool_call_id='0001')\",\n        '[Request] Part 0 args delta: {\"location\":\"Pa',\n        '[Request] Part 0 args delta: ris\",\"forecast_',\n        '[Request] Part 0 args delta: date\":\"2030-01-',\n        '[Request] Part 0 args delta: 01\"}',\n        '[Tools] The LLM calls tool=\\'weather_forecast\\' with args={\"location\":\"Paris\",\"forecast_date\":\"2030-01-01\"} (tool_call_id=\\'0001\\')',\n        \"[Tools] Tool call '0001' returned => The forecast in Paris on 2030-01-01 is 24C and sunny.\",\n        \"[Request] Starting part 0: TextPart(content='It will be ')\",\n        '[Result] The model starting producing a final result (tool_name=None)',\n        '[Output] It will be ',\n        '[Output] It will be warm and sunny ',\n        '[Output] It will be warm and sunny in Paris on ',\n        '[Output] It will be warm and sunny in Paris on Tuesday.',\n    ]\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Limiting Pydantic AI Message History to Recent Messages\nDESCRIPTION: This example illustrates how to use an asynchronous `history_processor` to manage token usage by keeping only the most recent messages in the conversation history.",
    "chunk_length": 5685
  },
  {
    "chunk_id": 98,
    "source": "pydantic_ai_llms_data",
    "content": "The `keep_recent_messages` function ensures that only the last 5 messages are sent to the model, regardless of the total conversation length, which is crucial for cost optimization and maintaining context relevance. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/message-history.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.messages import ModelMessage\n\n\nasync def keep_recent_messages(messages: list[ModelMessage]) -> list[ModelMessage]:\n    \"\"\"Keep only the last 5 messages to manage token usage.\"\"\"\n    return messages[-5:] if len(messages) > 5 else messages\n\nagent = Agent('openai:gpt-4o', history_processors=[keep_recent_messages])\n\n# Example: Even with a long conversation history, only the last 5 messages are sent to the model\nlong_conversation_history: list[ModelMessage] = []  # Your long conversation history here\n# result = agent.run_sync('What did we discuss?', message_history=long_conversation_history)\n```\n\n----------------------------------------\n\nTITLE: Define a Node with Conditional Graph Termination\nDESCRIPTION: This example extends the `MyNode` to allow for conditional graph termination. The `run` method's return type is a union of `AnotherNode` and `End[int]`, enabling the node to either continue the graph execution or terminate it by returning an `End` object with a specified value. The `BaseNode` is parameterized with `None` for dependencies and `int` for the graph's return type. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/graph.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_graph import BaseNode, End, GraphRunContext\n\n\n@dataclass\nclass MyNode(BaseNode[MyState, None, int]):\n    foo: int\n\n    async def run(\n        self,\n        ctx: GraphRunContext[MyState],\n    ) -> AnotherNode | End[int]:\n        if self.foo % 5 == 0:\n            return End(self.foo)\n        else:\n            return AnotherNode()\n```\n\n----------------------------------------\n\nTITLE: Run Graph from File Persistence (Python)\nDESCRIPTION: This Python example demonstrates how to use `FileStatePersistence` with `pydantic-graph` to manage and resume graph execution.",
    "chunk_length": 2197
  },
  {
    "chunk_id": 99,
    "source": "pydantic_ai_llms_data",
    "content": "It initializes a `count_down_graph` with a `FileStatePersistence` object, then repeatedly calls `run_node` which loads the graph state from the file and executes the next available node. This showcases how `pydantic-graph` allows for distributed or interrupted execution by persisting and resuming graph state without relying on external application state. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/graph.md#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\nfrom pathlib import Path\n\nfrom pydantic_graph import End\nfrom pydantic_graph.persistence.file import FileStatePersistence\n\nfrom count_down import CountDown, CountDownState, count_down_graph\n\n\nasync def main():\n    run_id = 'run_abc123'\n    persistence = FileStatePersistence(Path(f'count_down_{run_id}.json'))  # (1)! state = CountDownState(counter=5)\n    await count_down_graph.initialize(  # (2)! CountDown(), state=state, persistence=persistence\n    )\n\n    done = False\n    while not done:\n        done = await run_node(run_id)\n\n\nasync def run_node(run_id: str) -> bool:  # (3)! persistence = FileStatePersistence(Path(f'count_down_{run_id}.json'))\n    async with count_down_graph.iter_from_persistence(persistence) as run:  # (4)! node_or_end = await run.next()  # (5)! print('Node:', node_or_end)\n    # Node: CountDown()\n    # Node: CountDown()\n    # Node: CountDown()\n    # Node: CountDown()\n    # Node: CountDown()\n    # Node: End(data=0)\n    return isinstance(node_or_end, End)  # (6)! ```\n\n----------------------------------------\n\nTITLE: Initialize Pydantic-AI Agent with Groq Model by String Name\nDESCRIPTION: This Python snippet demonstrates how to initialize a `pydantic-ai` Agent by directly providing a Groq model name as a string. The Agent automatically resolves the Groq model using the `groq:` prefix, assuming the `GROQ_API_KEY` environment variable is set. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/groq.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('groq:llama-3.3-70b-versatile')\n... ```\n\n----------------------------------------\n\nTITLE: Overriding Agent Model via Pytest Fixture\nDESCRIPTION: This Python code provides an example of using a Pytest fixture to override an AI agent's model.",
    "chunk_length": 2249
  },
  {
    "chunk_id": 100,
    "source": "pydantic_ai_llms_data",
    "content": "The `override_weather_agent` fixture uses `weather_agent.override(model=TestModel())` to replace the agent's model, ensuring that subsequent tests within its scope use the `TestModel` for predictable behavior. This approach promotes reusability and cleaner test code for scenarios requiring model mocking. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/testing.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport pytest\nfrom weather_app import weather_agent\n\nfrom pydantic_ai.models.test import TestModel\n\n\n@pytest.fixture\ndef override_weather_agent():\n    with weather_agent.override(model=TestModel()):\n        yield\n\n\nasync def test_forecast(override_weather_agent: None):\n    ... # test code here\n```\n\n----------------------------------------\n\nTITLE: Configure Pydantic AI Agent to Exclude Sensitive Content\nDESCRIPTION: This example illustrates how to prevent Pydantic AI agents from sending sensitive data like user prompts, model completions, and tool call arguments to observability platforms. By setting `include_content=False`, only structural information is sent, which is crucial for privacy and compliance in production environments. It covers both individual agent and global instrumentation settings. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/logfire.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai.agent import Agent\nfrom pydantic_ai.models.instrumented import InstrumentationSettings\n\ninstrumentation_settings = InstrumentationSettings(include_content=False)\n\nagent = Agent('gpt-4o', instrument=instrumentation_settings)\n# or to instrument all agents:\nAgent.instrument_all(instrumentation_settings)\n```\n\n----------------------------------------\n\nTITLE: Configure Custom TLS/SSL with httpx.AsyncClient in Pydantic AI\nDESCRIPTION: This Python example demonstrates how to set up a custom `httpx.AsyncClient` with specific TLS/SSL configurations and pass it to a Pydantic AI `MCPServerSSE` instance. It shows how to create an `ssl.SSLContext` to trust a custom CA file and optionally load a client certificate for mutual TLS.",
    "chunk_length": 2084
  },
  {
    "chunk_id": 101,
    "source": "pydantic_ai_llms_data",
    "content": "The configured `httpx.AsyncClient` is then used by the Pydantic AI agent for all MCP traffic, allowing advanced control over network requests. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/mcp/client.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport httpx\nimport ssl\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerSSE\n\n\n# Trust an internal / self-signed CA\nssl_ctx = ssl.create_default_context(cafile=\"/etc/ssl/private/my_company_ca.pem\")\n\n# OPTIONAL: if the server requires **mutual TLS** load your client certificate\nssl_ctx.load_cert_chain(certfile=\"/etc/ssl/certs/client.crt\", keyfile=\"/etc/ssl/private/client.key\",)\n\nhttp_client = httpx.AsyncClient(\n    verify=ssl_ctx,\n    timeout=httpx.Timeout(10.0),\n)\n\nserver = MCPServerSSE(\n    url=\"http://localhost:3001/sse\",\n    http_client=http_client,  # (1)! )\nagent = Agent(\"openai:gpt-4o\", toolsets=[server])\n\nasync def main():\n    async with agent:\n        result = await agent.run('How many days between 2000-01-01 and 2025-03-18?')\n    print(result.output)\n    #> There are 9,208 days between January 1, 2000, and March 18, 2025. ```\n\n----------------------------------------\n\nTITLE: Reusing Pydantic AI Messages Across Different Models\nDESCRIPTION: This example demonstrates how to reuse message history generated by one Pydantic AI agent (using `openai:gpt-4o`) in a subsequent agent run with a different model (`google-gla:gemini-1.5-pro`). It highlights the model-agnostic nature of Pydantic AI's message format, allowing seamless transfer of conversation context using `result.new_messages()`. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/message-history.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o', system_prompt='Be a helpful assistant.')\n\nresult1 = agent.run_sync('Tell me a joke.')\nprint(result1.output)\n#> Did you hear about the toothpaste scandal? They called it Colgate. result2 = agent.run_sync(\n    'Explain?',\n    model='google-gla:gemini-1.5-pro',\n    message_history=result1.new_messages(),\n)\nprint(result2.output)\n#> This is an excellent joke invented by Samuel Colvin, it needs no explanation.",
    "chunk_length": 2180
  },
  {
    "chunk_id": 102,
    "source": "pydantic_ai_llms_data",
    "content": "print(result2.all_messages())\n\"\"\"\n[\n    ModelRequest(\n        parts=[\n            SystemPromptPart(\n                content='Be a helpful assistant.',\n                timestamp=datetime.datetime(...),\n            ),\n            UserPromptPart(\n                content='Tell me a joke.',\n                timestamp=datetime.datetime(...),\n            ),\n        ]\n    ),\n    ModelResponse(\n        parts=[\n            TextPart(\n                content='Did you hear about the toothpaste scandal? They called it Colgate.'\n            )\n        ],\n        usage=Usage(requests=1, request_tokens=60, response_tokens=12, total_tokens=72),\n        model_name='gpt-4o',\n        timestamp=datetime.datetime(...),\n    ),\n    ModelRequest(\n        parts=[\n            UserPromptPart(\n                content='Explain?',\n                timestamp=datetime.datetime(...),\n            )\n        ]\n    ),\n    ModelResponse(\n        parts=[\n            TextPart(\n                content='This is an excellent joke invented by Samuel Colvin, it needs no explanation.'\n            )\n        ],\n        usage=Usage(requests=1, request_tokens=61, response_tokens=26, total_tokens=87),\n        model_name='gemini-1.5-pro',\n        timestamp=datetime.datetime(...),\n    ),\n]\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Integrate Pydantic AI with Local Ollama\nDESCRIPTION: Demonstrates how to configure `pydantic-ai` to use a locally running Ollama instance, defining a Pydantic model for structured output and executing a query. It shows how to parse the output into a `BaseModel` and retrieve usage statistics. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/openai.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\n\nclass CityLocation(BaseModel):\n    city: str\n    country: str\n\n\nollama_model = OpenAIModel(\n    model_name='llama3.2', provider=OpenAIProvider(base_url='http://localhost:11434/v1')\n)\nagent = Agent(ollama_model, output_type=CityLocation)\n\nresult = agent.run_sync('Where were the olympics held in 2012?')\nprint(result.output)\n# city='London' country='United Kingdom'\nprint(result.usage())\n# Usage(requests=1, request_tokens=57, response_tokens=8, total_tokens=65)\n```\n\n----------------------------------------\n\nTITLE: Pydantic AI Agent for AG-UI Event and State Management\nDESCRIPTION: This Python code demonstrates how to define a Pydantic AI `Agent` that interacts with AG-UI for event handling and state management.",
    "chunk_length": 2610
  },
  {
    "chunk_id": 103,
    "source": "pydantic_ai_llms_data",
    "content": "It showcases the use of `StateDeps` to manage application state, and defines `tool` and `tool_plain` functions that return `StateSnapshotEvent` for state updates and `CustomEvent` for custom event emission, respectively. This setup allows Pydantic AI tools to send structured events to the AG-UI frontend. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/ag-ui.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom ag_ui.core import CustomEvent, EventType, StateSnapshotEvent\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent, RunContext\nfrom pydantic_ai.ag_ui import StateDeps\n\n\nclass DocumentState(BaseModel):\n    \"\"\"State for the document being written.\"\"\"\n\n    document: str = ''\n\n\nagent = Agent(\n    'openai:gpt-4.1',\n    instructions='Be fun!',\n    deps_type=StateDeps[DocumentState],\n)\napp = agent.to_ag_ui(deps=StateDeps(DocumentState()))\n\n\n@agent.tool\nasync def update_state(ctx: RunContext[StateDeps[DocumentState]]) -> StateSnapshotEvent:\n    return StateSnapshotEvent(\n        type=EventType.STATE_SNAPSHOT,\n        snapshot=ctx.deps.state,\n    )\n\n\n@agent.tool_plain\nasync def custom_events() -> list[CustomEvent]:\n    return [\n        CustomEvent(\n            type=EventType.CUSTOM,\n            name='count',\n            value=1,\n        ),\n        CustomEvent(\n            type=EventType.CUSTOM,\n            name='count',\n            value=2,\n        ),\n    ]\n```\n\n----------------------------------------\n\nTITLE: Handling and Diagnosing Agent Model Errors with capture_run_messages in Python\nDESCRIPTION: This Python example illustrates how to gracefully handle `UnexpectedModelBehavior` when an agent's tool, `calc_volume`, intentionally raises `ModelRetry` multiple times, simulating a persistent error. It showcases the critical role of `capture_run_messages` in capturing the entire communication flow (requests and responses) between the agent and the model. This captured message history is invaluable for debugging and understanding the root cause of model failures or unexpected retries, providing detailed insights into the interaction leading to the exception.",
    "chunk_length": 2104
  },
  {
    "chunk_id": 104,
    "source": "pydantic_ai_llms_data",
    "content": "SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/agents.md#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent, ModelRetry, UnexpectedModelBehavior, capture_run_messages\n\nagent = Agent('openai:gpt-4o')\n\n\n@agent.tool_plain\ndef calc_volume(size: int) -> int:  # (1)! if size == 42:\n        return size**3\n    else:\n        raise ModelRetry('Please try again.')\n\n\nwith capture_run_messages() as messages:  # (2)! try:\n        result = agent.run_sync('Please get me the volume of a box with size 6.')\n    except UnexpectedModelBehavior as e:\n        print('An error occurred:', e)\n        # > An error occurred: Tool 'calc_volume' exceeded max retries count of 1\n        print('cause:', repr(e.__cause__))\n        # > cause: ModelRetry('Please try again.')\n        print('messages:', messages)\n        \"\"\"\n        messages:\n        [\n            ModelRequest(\n                parts=[\n                    UserPromptPart(\n                        content='Please get me the volume of a box with size 6.',\n                        timestamp=datetime.datetime(...),\n                    )\n                ]\n            ),\n            ModelResponse(\n                parts=[\n                    ToolCallPart(\n                        tool_name='calc_volume',\n                        args={'size': 6},\n                        tool_call_id='pyd_ai_tool_call_id',\n                    )\n                ],\n                usage=(\n                    Usage(\n                        requests=1, request_tokens=62, response_tokens=4, total_tokens=66\n                    )\n                ),\n                model_name='gpt-4o',\n                timestamp=datetime.datetime(...),\n            ),\n            ModelRequest(\n                parts=[\n                    RetryPromptPart(\n                        content='Please try again.',\n                        tool_name='calc_volume',\n                        tool_call_id='pyd_ai_tool_call_id',\n                        timestamp=datetime.datetime(...),\n                    )\n                ]\n            ),\n            ModelResponse(\n                parts=[\n                    ToolCallPart(\n                        tool_name='calc_volume',\n                        args={'size': 6},\n                        tool_call_id='pyd_ai_tool_call_id',\n                    )\n                ],\n                usage=(\n                    Usage(\n                        requests=1, request_tokens=72, response_tokens=8, total_tokens=80\n                    )\n                ),\n                model_name='gpt-4o',\n                timestamp=datetime.datetime(...),\n            ),\n        ]\n        \"\"\"\n    else:\n        print(result.output)\n```\n\n----------------------------------------\n\nTITLE: Initialize pydantic-ai Agent with Hugging Face model by name\nDESCRIPTION: This Python code demonstrates how to create an `Agent` instance by directly passing a Hugging Face model name string (prefixed with 'huggingface:') to its constructor.",
    "chunk_length": 2983
  },
  {
    "chunk_id": 105,
    "source": "pydantic_ai_llms_data",
    "content": "This is a convenient way to quickly use a Hugging Face model with default provider settings. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/huggingface.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('huggingface:Qwen/Qwen3-235B-A22B')\n... ```\n\n----------------------------------------\n\nTITLE: Provide Custom Bedrock Provider with Boto3 Client\nDESCRIPTION: Illustrates how to use a pre-configured `boto3` client to initialize a `BedrockProvider`, which is then passed to `BedrockConverseModel`. This is useful for scenarios where a `boto3` client is already set up with specific configurations or session management. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/bedrock.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport boto3\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.bedrock import BedrockConverseModel\nfrom pydantic_ai.providers.bedrock import BedrockProvider\n\n# Using a pre-configured boto3 client\nbedrock_client = boto3.client('bedrock-runtime', region_name='us-east-1')\nmodel = BedrockConverseModel(\n    'anthropic.claude-3-sonnet-20240229-v1:0',\n    provider=BedrockProvider(bedrock_client=bedrock_client),\n)\nagent = Agent(model)\n... ```\n\n----------------------------------------\n\nTITLE: Set Vercel AI Gateway Environment Variables\nDESCRIPTION: Instructions for setting environment variables (`VERCEL_AI_GATEWAY_API_KEY` or `VERCEL_OIDC_TOKEN`) to authenticate with Vercel AI Gateway. These variables allow `pydantic-ai` to automatically pick up credentials. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/openai.md#_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\nexport VERCEL_AI_GATEWAY_API_KEY='your-ai-gateway-api-key'\n# OR\nexport VERCEL_OIDC_TOKEN='your-oidc-token'\n```\n\n----------------------------------------\n\nTITLE: Define Dataset, Custom Evaluator, and Run Evaluation\nDESCRIPTION: This Python snippet demonstrates the complete workflow for setting up an evaluation with `pydantic-evals`. It shows how to define a `Case`, implement a custom `Evaluator`, create a `Dataset` with both built-in and custom evaluators, run a synchronous evaluation against a target function, and print the resulting `EvaluationReport`.",
    "chunk_length": 2241
  },
  {
    "chunk_id": 106,
    "source": "pydantic_ai_llms_data",
    "content": "SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/evals.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext, IsInstance\n\ncase1 = Case(\n    name='simple_case',\n    inputs='What is the capital of France?',\n    expected_output='Paris',\n    metadata={'difficulty': 'easy'},\n)\n\n\nclass MyEvaluator(Evaluator[str, str]):\n    def evaluate(self, ctx: EvaluatorContext[str, str]) -> float:\n        if ctx.output == ctx.expected_output:\n            return 1.0\n        elif (\n            isinstance(ctx.output, str)\n            and ctx.expected_output.lower() in ctx.output.lower()\n        ):\n            return 0.8\n        else:\n            return 0.0\n\n\ndataset = Dataset(\n    cases=[case1],\n    evaluators=[IsInstance(type_name='str'), MyEvaluator()],\n)\n\n\nasync def guess_city(question: str) -> str:\n    return 'Paris'\n\n\nreport = dataset.evaluate_sync(guess_city)\nreport.print(include_input=True, include_output=True, include_durations=False)\n\"\"\"\n                              Evaluation Summary: guess_city\n\n Case ID      Inputs                          Outputs  Scores             Assertions \n\n simple_case  What is the capital of France?  Paris    MyEvaluator: 1.00            \n\n Averages                                              MyEvaluator: 1.00  100.0%    \n\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Configure Pydantic AI with GitHub Models\nDESCRIPTION: Demonstrates how to initialize an `OpenAIModel` with `GitHubProvider` using a personal access token. This requires a GitHub token with `models: read` permission. The model name uses a prefixed format, e.g., 'xai/grok-3-mini'.",
    "chunk_length": 2089
  },
  {
    "chunk_id": 107,
    "source": "pydantic_ai_llms_data",
    "content": "SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/openai.md#_snippet_25\n\nLANGUAGE: Python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.github import GitHubProvider\n\nmodel = OpenAIModel(\n    'xai/grok-3-mini',  # GitHub Models uses prefixed model names\n    provider=GitHubProvider(api_key='your-github-token'),\n)\nagent = Agent(model)\n... ```\n\n----------------------------------------\n\nTITLE: Initialize MistralModel with custom provider and API key\nDESCRIPTION: Demonstrates how to configure a `MistralModel` with a custom `MistralProvider` instance. This allows for direct specification of the API key and a custom base URL, overriding any environment variables. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/mistral.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.mistral import MistralModel\nfrom pydantic_ai.providers.mistral import MistralProvider\n\nmodel = MistralModel(\n    'mistral-large-latest', provider=MistralProvider(api_key='your-api-key', base_url='https://<mistral-provider-endpoint>')\n)\nagent = Agent(model)\n... ```\n\n----------------------------------------\n\nTITLE: Run Pydantic AI A2A Server with Uvicorn\nDESCRIPTION: This bash command illustrates how to launch the ASGI application generated by `agent.to_a2a()` using the Uvicorn server. It specifies the module (`agent_to_a2a`) and the application object (`app`), along with the host and port for network accessibility. This command is essential for making the Pydantic AI agent available as a live A2A service. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/a2a.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nuvicorn agent_to_a2a:app --host 0.0.0.0 --port 8000\n```\n\n----------------------------------------\n\nTITLE: Provide image input to LLM using Pydantic-AI ImageUrl\nDESCRIPTION: This snippet demonstrates how to send an image to an LLM by providing its direct URL using the `ImageUrl` class from `pydantic-ai`.",
    "chunk_length": 2054
  },
  {
    "chunk_id": 108,
    "source": "pydantic_ai_llms_data",
    "content": "It initializes an `Agent` with a specified model (e.g., 'openai:gpt-4o') and passes a list containing the text prompt and the `ImageUrl` object to the `run_sync` method. The LLM then processes the image and provides a textual response. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/input.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent, ImageUrl\n\nagent = Agent(model='openai:gpt-4o')\nresult = agent.run_sync(\n    [\n        'What company is this logo from?',\n        ImageUrl(url='https://iili.io/3Hs4FMg.png'),\n    ]\n)\nprint(result.output)\n```\n\n----------------------------------------\n\nTITLE: Run Pydantic AI Agent Synchronously for Basic Text Response\nDESCRIPTION: This snippet demonstrates the most basic usage of a Pydantic AI agent, executing a synchronous run to query an LLM and retrieve a simple text-based response. It shows how to initiate a conversation and print the agent's output. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nresult = agent.run_sync('Where does \"hello world\" come from?')\nprint(result.output)\n```\n\n----------------------------------------\n\nTITLE: clai Command Line Interface Reference\nDESCRIPTION: Comprehensive reference for the `clai` command-line tool, detailing its usage, available options, and special interactive mode commands. It allows users to interact with AI models, specify models, agents, and control streaming behavior. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/clai/README.md#_snippet_4\n\nLANGUAGE: APIDOC\nCODE:\n```\nusage: clai [-h] [-m [MODEL]] [-a AGENT] [-l] [-t [CODE_THEME]] [--no-stream] [--version] [prompt]\n\nPydantic AI CLI v... Special prompts:\n* `/exit` - exit the interactive mode (ctrl-c and ctrl-d also work)\n* `/markdown` - show the last markdown output of the last question\n* `/multiline` - toggle multiline mode\n\npositional arguments:\n  prompt                AI Prompt, if omitted fall into interactive mode\n\noptions:\n  -h, --help            show this help message and exit\n  -m [MODEL], --model [MODEL]\n                        Model to use, in format \"<provider>:<model>\" e.g.",
    "chunk_length": 2158
  },
  {
    "chunk_id": 109,
    "source": "pydantic_ai_llms_data",
    "content": "\"openai:gpt-4.1\" or \"anthropic:claude-sonnet-4-0\". Defaults to \"openai:gpt-4.1\". -a AGENT, --agent AGENT\n                        Custom Agent to use, in format \"module:variable\", e.g. \"mymodule.submodule:my_agent\"\n  -l, --list-models     List all available models and exit\n  -t [CODE_THEME], --code-theme [CODE_THEME]\n                        Which colors to use for code, can be \"dark\", \"light\" or any theme from pygments.org/styles/. Defaults to \"dark\" which works well on dark terminals. --no-stream           Disable streaming from the model\n  --version             Show version and exit\n```\n\n----------------------------------------\n\nTITLE: Create a Pydantic AI MCP Server\nDESCRIPTION: This Python code sets up a basic MCP server using `FastMCP` and integrates a Pydantic AI `Agent`. It defines an asynchronous `poet` tool that uses the agent to generate rhyming poems based on a provided theme, demonstrating a simple AI-powered tool within the MCP framework. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/mcp/server.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom mcp.server.fastmcp import FastMCP\n\nfrom pydantic_ai import Agent\n\nserver = FastMCP('Pydantic AI Server')\nserver_agent = Agent(\n    'anthropic:claude-3-5-haiku-latest', system_prompt='always reply in rhyme'\n)\n\n\n@server.tool()\nasync def poet(theme: str) -> str:\n    \"\"\"Poem generator\"\"\"\n    r = await server_agent.run(f'write a poem about {theme}')\n    return r.output\n\n\nif __name__ == '__main__':\n    server.run()\n```\n\n----------------------------------------\n\nTITLE: Configure Pydantic AI Agent with Logfire Instrumentation\nDESCRIPTION: This Python snippet demonstrates how to integrate Pydantic Logfire for instrumentation with a Pydantic AI `Agent`. It shows configuring Logfire and instrumenting `asyncpg` for database query logging, along with the `Agent` initialization including `instrument=True` to enable tracing. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/index.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n... from pydantic_ai import Agent, RunContext\n\nfrom bank_database import DatabaseConn\n\nimport logfire\n\nlogfire.configure()  # (1)!",
    "chunk_length": 2149
  },
  {
    "chunk_id": 110,
    "source": "pydantic_ai_llms_data",
    "content": "logfire.instrument_asyncpg()  # (2)! ... support_agent = Agent(\n    'openai:gpt-4o',\n    deps_type=SupportDependencies,\n    output_type=SupportOutput,\n    system_prompt=(\n        'You are a support agent in our bank, give the '\n        'customer support and judge the risk level of their query.'\n    ),\n    instrument=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Deploy Pydantic AI Slack Qualifier to Modal Production\nDESCRIPTION: Use this command to deploy the Pydantic AI Slack lead qualifier application persistently to your Modal workspace. This makes the application available for continuous operation in a production environment, accessible via a stable URL. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/examples/slack-lead-qualifier.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython/uv-run -m modal deploy -m pydantic_ai_examples.slack_lead_qualifier.modal\n```\n\n----------------------------------------\n\nTITLE: Initialize OpenAIModel directly in pydantic-ai\nDESCRIPTION: Illustrates how to explicitly instantiate an `OpenAIModel` object with a model name and then pass this model instance to the `Agent` constructor. This provides more granular control over the model configuration. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/openai.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIModel\n\nmodel = OpenAIModel('gpt-4o')\nagent = Agent(model)\n... ```\n\n----------------------------------------\n\nTITLE: Configure Pydantic AI Model Settings Precedence\nDESCRIPTION: Demonstrates how `ModelSettings` are applied and merged in Pydantic AI, showing the precedence order from model-level defaults, to agent-level defaults, and finally run-time overrides. It illustrates how specific parameters like `temperature` and `max_tokens` are resolved based on this hierarchy. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/agents.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.settings import ModelSettings\n\n# 1.",
    "chunk_length": 2135
  },
  {
    "chunk_id": 111,
    "source": "pydantic_ai_llms_data",
    "content": "Model-level defaults\nmodel = OpenAIModel(\n    'gpt-4o',\n    settings=ModelSettings(temperature=0.8, max_tokens=500)  # Base defaults\n)\n\n# 2. Agent-level defaults (overrides model defaults by merging)\nagent = Agent(model, model_settings=ModelSettings(temperature=0.5))\n\n# 3. Run-time overrides (highest priority)\nresult_sync = agent.run_sync(\n    'What is the capital of Italy?',\n    model_settings=ModelSettings(temperature=0.0)  # Final temperature: 0.0\n)\nprint(result_sync.output)\n```\n\n----------------------------------------\n\nTITLE: Initialize pydantic-ai Agent with OpenAI model by name\nDESCRIPTION: Shows how to create an `Agent` instance in `pydantic-ai` by directly specifying an OpenAI model name (e.g., 'openai:gpt-4o'). This method assumes the API key is configured via an environment variable. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/openai.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n... ```\n\n----------------------------------------\n\nTITLE: Provide document input to LLM using Pydantic-AI DocumentUrl\nDESCRIPTION: This snippet demonstrates how to provide a document to an LLM using its direct URL via the `DocumentUrl` class from `pydantic-ai`. It initializes an `Agent` with a model capable of document understanding (e.g., 'anthropic:claude-3-sonnet') and passes the document URL along with a text prompt. The LLM then processes the document content to answer questions or summarize its main points. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/input.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent, DocumentUrl\n\nagent = Agent(model='anthropic:claude-3-sonnet')\nresult = agent.run_sync(\n    [\n        'What is the main content of this document?',\n        DocumentUrl(url='https://storage.googleapis.com/cloud-samples-data/generative-ai/pdf/2403.05530.pdf'),\n    ]\n)\nprint(result.output)\n```\n\n----------------------------------------\n\nTITLE: Connect Pydantic AI Agent to MCP SSE Server\nDESCRIPTION: Demonstrates how to initialize an `MCPServerSSE` client with a specified URL and integrate it with a Pydantic AI `Agent`.",
    "chunk_length": 2167
  },
  {
    "chunk_id": 112,
    "source": "pydantic_ai_llms_data",
    "content": "The agent then uses the server to run a natural language query, showcasing communication over Server-Sent Events (SSE) for tool execution. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/mcp/client.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerSSE\n\nserver = MCPServerSSE(url='http://localhost:3001/sse')  # (1)! agent = Agent('openai:gpt-4o', toolsets=[server])  # (2)! async def main():\n    async with agent:  # (3)! result = await agent.run('How many days between 2000-01-01 and 2025-03-18?')\n    print(result.output)\n    # There are 9,208 days between January 1, 2000, and March 18, 2025. ```\n\n----------------------------------------\n\nTITLE: Pydantic AI Components for OpenAI-compatible Model Configuration\nDESCRIPTION: Comprehensive API documentation for configuring OpenAI-compatible models within Pydantic AI. This includes details on the OpenAIModel, various provider classes (OpenAIProvider, DeepSeekProvider), model profiles for behavior customization, and the Agent class for model interaction. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/openai.md#_snippet_13\n\nLANGUAGE: APIDOC\nCODE:\n```\nPydantic AI Components for OpenAI-compatible Models:\n\nOpenAIModel:\n  __init__(model_name: str, provider: Union[OpenAIProvider, str], profile: Optional[ModelProfile] = None)\n    model_name: The name of the OpenAI-compatible model to use (e.g., 'model_name', 'deepseek-chat'). provider: The provider to use. Can be an instance of OpenAIProvider (or a specific subclass like DeepSeekProvider) or a string shorthand (e.g., 'deepseek', 'openrouter'). profile: An optional ModelProfile or OpenAIModelProfile instance to customize model behavior. OpenAIProvider:\n  __init__(base_url: str, api_key: str, http_client: Optional[AsyncClient] = None)\n    base_url: The base URL of the OpenAI-compatible API endpoint. api_key: The API key for authentication with the service. http_client: An optional custom httpx.AsyncClient instance for advanced HTTP request configuration (e.g., timeouts).",
    "chunk_length": 2083
  },
  {
    "chunk_id": 113,
    "source": "pydantic_ai_llms_data",
    "content": "DeepSeekProvider (subclass of OpenAIProvider):\n  __init__(api_key: str, http_client: Optional[AsyncClient] = None)\n    api_key: The DeepSeek API key. http_client: An optional custom httpx.AsyncClient instance. ModelProfile:\n  Used to tweak various aspects of how model requests are constructed, shared among all model classes. Example attribute:\n    json_schema_transformer: A transformer class (e.g., InlineDefsJsonSchemaTransformer) to modify JSON schemas for tool definitions. OpenAIModelProfile (subclass of ModelProfile):\n  Used for behaviors specific to OpenAIModel. Example attribute:\n    openai_supports_strict_tool_definition: A boolean indicating if the OpenAI-compatible API supports strict tool definitions. Agent:\n  __init__(model: OpenAIModel)\n    model: An initialized OpenAIModel instance. Shorthand initialization:\n    Agent(\"<provider>:<model>\"): A convenient shorthand to initialize an Agent with a specific provider and model (e.g., Agent(\"deepseek:deepseek-chat\")). ```\n\n----------------------------------------\n\nTITLE: Verify Model Understanding of DuckDB SQL with clai CLI\nDESCRIPTION: Demonstrates how to use the `clai` command-line interface to query a specified large language model (e.g., a Bedrock Claude model) about its understanding of DuckDB SQL. The interactive output shows the model's affirmative response and a detailed explanation of DuckDB's features and capabilities, confirming its suitability for SQL-based data analysis tasks. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/examples/data-analyst.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nclai -m bedrock:us.anthropic.claude-3-7-sonnet-20250219-v1:0\nclai - Pydantic AI CLI v0.0.1.dev920+41dd069 with bedrock:us.anthropic.claude-3-7-sonnet-20250219-v1:0\nclai  do you understand duckdb sql? # DuckDB SQL\n\nYes, I understand DuckDB SQL. DuckDB is an in-process analytical SQL database\nthat uses syntax similar to PostgreSQL. It specializes in analytical queries\nand is designed for high-performance analysis of structured data. Some key features of DuckDB SQL include:\n\n  OLAP (Online Analytical Processing) optimized\n  Columnar-vectorized query execution\n  Standard SQL support with PostgreSQL compatibility\n  Support for complex analytical queries\n  Efficient handling of CSV/Parquet/JSON files\n\nI can help you with DuckDB SQL queries, schema design, optimization, or other\nDuckDB-related questions. ```\n\n----------------------------------------\n\nTITLE: Configure Pydantic AI with MoonshotAI\nDESCRIPTION: Shows how to integrate `pydantic-ai` with MoonshotAI.",
    "chunk_length": 2565
  },
  {
    "chunk_id": 114,
    "source": "pydantic_ai_llms_data",
    "content": "This configuration requires an API key, which can be created in the Moonshot Console, to access their language models. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/openai.md#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.moonshotai import MoonshotAIProvider\n\nmodel = OpenAIModel(\n    'kimi-k2-0711-preview',\n    provider=MoonshotAIProvider(api_key='your-moonshot-api-key'),\n)\nagent = Agent(model)\n... ```\n\n----------------------------------------\n\nTITLE: Pydantic AI Output Marker Classes and Tool Preparation API\nDESCRIPTION: Documents key classes and types in Pydantic AI for managing model output, including plain text and structured tool-based output, and functions for dynamic tool preparation. This includes `TextOutput`, `ToolOutput`, `ToolsPrepareFunc`, `RunContext`, and `ToolDefinition`. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/output.md#_snippet_6\n\nLANGUAGE: APIDOC\nCODE:\n```\npydantic_ai.output.TextOutput:\n  Description: Marker class to configure an agent for plain text output. Usage: TextOutput(output_function)\n  Parameters:\n    output_function: Callable[[str], Any] - A function that takes the model's raw string output and processes it into the desired type. pydantic_ai.output.ToolOutput:\n  Description: Marker class for structured output via tool calls. Allows customization of tool name, description, and strictness. Usage: ToolOutput(type_or_function, name: str = None, description: str = None, strict: bool = None)\n  Parameters:\n    type_or_function: Type | Callable - A Pydantic BaseModel or a function whose signature defines the output schema. name: str (optional) - Custom name for the output tool. description: str (optional) - Custom description for the output tool. Defaults to the docstring of the type/function. strict: bool (optional) - If True, enforces strict schema validation. pydantic_ai.tools.ToolsPrepareFunc:\n  Description: Type alias for a function used to dynamically modify or filter available output tools before an agent step.",
    "chunk_length": 2119
  },
  {
    "chunk_id": 115,
    "source": "pydantic_ai_llms_data",
    "content": "Signature: Callable[[RunContext, list[ToolDefinition]], list[ToolDefinition] | None]\n  Parameters:\n    context: pydantic_ai.tools.RunContext - The current run context. tool_definitions: list[pydantic_ai.tools.ToolDefinition] - A list of tool definitions available for the current step. Returns: list[pydantic_ai.tools.ToolDefinition] | None - A new list of tool definitions to use, or None to disable all tools for the step. pydantic_ai.tools.RunContext:\n  Description: An object providing context for the current agent run, used in tool preparation functions. (Details not provided in source text)\n\npydantic_ai.tools.ToolDefinition:\n  Description: Represents the definition of a tool, including its schema and metadata. (Details not provided in source text)\n```\n\n----------------------------------------\n\nTITLE: Initialize Agent with CodeExecutionTool\nDESCRIPTION: Shows how to set up a PydanticAI agent with the `CodeExecutionTool` to enable code execution in a secure environment. This allows the agent to perform computational tasks, data analysis, and mathematical operations, such as calculating factorials. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/builtin-tools.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent, CodeExecutionTool\n\nagent = Agent('anthropic:claude-sonnet-4-0', builtin_tools=[CodeExecutionTool()])\n\nresult = agent.run_sync('Calculate the factorial of 15 and show your work')\n# > The factorial of 15 is **1,307,674,368,000**. ```\n\n----------------------------------------\n\nTITLE: Pydantic-AI Bedrock Model and Provider Classes\nDESCRIPTION: Comprehensive documentation for key classes used to interact with AWS Bedrock within `pydantic-ai`. This includes `BedrockConverseModel` for instantiating Bedrock models, `BedrockProvider` for managing AWS authentication and client configurations, and `BedrockModelSettings` for customizing runtime API parameters like guardrails and performance. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/bedrock.md#_snippet_7\n\nLANGUAGE: APIDOC\nCODE:\n```\nBedrockConverseModel:\n  __init__(model_name: str, provider: BedrockProvider = None)\n    model_name: The name of the Bedrock model to use (e.g., 'anthropic.claude-3-sonnet-20240229-v1:0').",
    "chunk_length": 2260
  },
  {
    "chunk_id": 116,
    "source": "pydantic_ai_llms_data",
    "content": "provider: An optional BedrockProvider instance for custom AWS credentials or boto3 client. BedrockProvider:\n  __init__(region_name: str = None, aws_access_key_id: str = None, aws_secret_access_key: str = None, bedrock_client: boto3.client = None)\n    region_name: The AWS region for the Bedrock service (e.g., 'us-east-1'). aws_access_key_id: Your AWS access key ID. aws_secret_access_key: Your AWS secret access key. bedrock_client: An optional pre-configured boto3 Bedrock runtime client. Note: Either explicit credentials (region_name, aws_access_key_id, aws_secret_access_key) or a bedrock_client must be provided. BedrockModelSettings:\n  __init__(bedrock_guardrail_config: dict = None, bedrock_performance_configuration: dict = None)\n    bedrock_guardrail_config: A dictionary for Bedrock guardrail configurations, as per AWS Bedrock documentation (e.g., {'guardrailIdentifier': 'v1', 'guardrailVersion': 'v1', 'trace': 'enabled'}). bedrock_performance_configuration: A dictionary for Bedrock performance settings, as per AWS Bedrock documentation (e.g., {'latency': 'optimized'}). ```\n\n----------------------------------------\n\nTITLE: Pydantic AI Agent Client Using MCP Sampling\nDESCRIPTION: This Python script demonstrates how a Pydantic AI `Agent` can act as a client to an MCP server that supports sampling. It initializes an agent with an `MCPServerStdio` instance, sets the agent's sampling model using `agent.set_mcp_sampling_model()`, and then runs a prompt that triggers the server's sampling capability to generate an image. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/mcp/client.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerStdio\n\nserver = MCPServerStdio(command='python', args=['generate_svg.py'])\nagent = Agent('openai:gpt-4o', toolsets=[server])\n\n\nasync def main():\n    async with agent:\n        agent.set_mcp_sampling_model()\n        result = await agent.run('Create an image of a robot in a punk style.')\n    print(result.output)\n```\n\n----------------------------------------\n\nTITLE: Configure Logfire Instrumentation for Pydantic AI on Modal\nDESCRIPTION: This snippet defines a Python function to set up Logfire instrumentation for Pydantic AI and HTTPX.",
    "chunk_length": 2260
  },
  {
    "chunk_id": 117,
    "source": "pydantic_ai_llms_data",
    "content": "It must be defined within a Modal function context as `logfire` and other requested packages are only available in that environment, not locally. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/examples/slack-lead-qualifier.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# This function sets up Logfire instrumentation for Pydantic AI and HTTPX. # It is designed to run within a Modal function context. def setup_logfire():\n    # Example: logfire.instrument_httpx()\n    # Example: logfire.instrument_pydantic_ai()\n    pass\n```\n\n----------------------------------------\n\nTITLE: Initialize pydantic-ai with OpenAI Responses API Model\nDESCRIPTION: Explains how to use the `OpenAIResponsesModel` class to leverage OpenAI's Responses API, which provides access to built-in tools for enhanced model capabilities. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/openai.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIResponsesModel\n\nmodel = OpenAIResponsesModel('gpt-4o')\nagent = Agent(model)\n... ```\n\n----------------------------------------\n\nTITLE: Initialize MistralModel with custom httpx.AsyncClient\nDESCRIPTION: Shows how to provide a custom `httpx.AsyncClient` to the `MistralProvider`. This enables fine-grained control over HTTP requests, such as setting custom timeouts or configuring proxies for communication with the Mistral API. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/mistral.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom httpx import AsyncClient\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.mistral import MistralModel\nfrom pydantic_ai.providers.mistral import MistralProvider\n\ncustom_http_client = AsyncClient(timeout=30)\nmodel = MistralModel(\n    'mistral-large-latest',\n    provider=MistralProvider(api_key='your-api-key', http_client=custom_http_client),\n)\nagent = Agent(model)\n... ```\n\n----------------------------------------\n\nTITLE: PydanticAI Builtin Tools API Reference\nDESCRIPTION: Comprehensive API documentation for PydanticAI's builtin tools, `WebSearchTool` and `CodeExecutionTool`.",
    "chunk_length": 2146
  },
  {
    "chunk_id": 118,
    "source": "pydantic_ai_llms_data",
    "content": "This section details their purpose, provider compatibility, and configurable parameters for `WebSearchTool`, including notes on provider-specific limitations. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/builtin-tools.md#_snippet_3\n\nLANGUAGE: APIDOC\nCODE:\n```\nWebSearchTool:\n  Purpose: Allows agents to search the web for up-to-date data. Provider Support:\n    - OpenAI: Full feature support. - Anthropic: Full feature support. - Groq: Limited parameter support (requires compound models). - Google, Bedrock, Mistral, Cohere, HuggingFace: Not supported. Configuration Parameters:\n    - search_context_size: 'high' (string) - Context size for search results. Supported by OpenAI only. - user_location: WebSearchUserLocation (object) - Specifies user's location for localized search results. - city: string\n      - country: string\n      - region: string\n      - timezone: string\n      Supported by OpenAI, Anthropic. - blocked_domains: list[str] - Domains to exclude from search results. Supported by Anthropic, Groq. Note for Anthropic: Cannot be used simultaneously with 'allowed_domains'. - allowed_domains: list[str] - Domains to restrict search results to. Supported by Anthropic, Groq. Note for Anthropic: Cannot be used simultaneously with 'blocked_domains'. - max_uses: int - Maximum number of times the tool can be used. Supported by Anthropic only. CodeExecutionTool:\n  Purpose: Enables agents to execute code in a secure environment for computational tasks, data analysis, and mathematical operations. Provider Support:\n    - OpenAI: Supported. - Anthropic: Supported. - Google: Supported. - Groq, Bedrock, Mistral, Cohere, HuggingFace: Not supported. ```\n\n----------------------------------------\n\nTITLE: Connect Pydantic AI Agent to Streamable HTTP MCP Server\nDESCRIPTION: This Python code illustrates how to establish a connection from a Pydantic AI `Agent` to an MCP server using the `MCPServerStreamableHTTP` client. It shows how to define the server URL, register the server as a toolset with the agent, and use an `async with` context manager to manage the connection for agent runs.",
    "chunk_length": 2117
  },
  {
    "chunk_id": 119,
    "source": "pydantic_ai_llms_data",
    "content": "SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/mcp/client.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerStreamableHTTP\n\nserver = MCPServerStreamableHTTP('http://localhost:8000/mcp')\nagent = Agent('openai:gpt-4o', toolsets=[server])\n\nasync def main():\n    async with agent:\n        result = await agent.run('How many days between 2000-01-01 and 2025-03-18?')\n    print(result.output)\n```\n\n----------------------------------------\n\nTITLE: Handle Starlette/FastAPI Request with Pydantic AI Agent\nDESCRIPTION: This snippet demonstrates how to integrate a Pydantic AI `Agent` with a Starlette-based web framework like FastAPI. It shows how to use `handle_ag_ui_request()` to process incoming HTTP requests and return responses, effectively exposing the agent as an AG-UI server. The accompanying shell command illustrates how to run the FastAPI application using Uvicorn. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/ag-ui.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom fastapi import FastAPI\nfrom starlette.requests import Request\nfrom starlette.responses import Response\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.ag_ui import handle_ag_ui_request\n\n\nagent = Agent('openai:gpt-4.1', instructions='Be fun!')\n\napp = FastAPI()\n\n@app.post(\"/\")\nasync def run_agent(request: Request) -> Response:\n    return await handle_ag_ui_request(agent, request)\n```\n\nLANGUAGE: shell\nCODE:\n```\nuvicorn handle_ag_ui_request:app\n```\n\n----------------------------------------\n\nTITLE: Pydantic AI AG-UI Integration Methods Overview\nDESCRIPTION: Comprehensive overview of the three primary methods provided by Pydantic AI for integrating agents with the AG-UI protocol, detailing their purpose, parameters, return values, and suitable use cases for various web frameworks. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/ag-ui.md#_snippet_1\n\nLANGUAGE: APIDOC\nCODE:\n```\nrun_ag_ui(agent: Agent, run_input: RunAgentInput, accept: str = SSE_CONTENT_TYPE, **kwargs)\n  - Description: Processes an AG-UI RunAgentInput object directly, returning a stream of AG-UI events.",
    "chunk_length": 2153
  },
  {
    "chunk_id": 120,
    "source": "pydantic_ai_llms_data",
    "content": "- Parameters:\n    - agent (Agent): The Pydantic AI agent instance to run. - run_input (RunAgentInput): The input object containing the agent's run details. - accept (str, optional): The desired content type for the event stream (default: SSE_CONTENT_TYPE). - **kwargs: Optional arguments passed to Agent.iter(), including 'deps' for dependency injection. - Returns: An iterable stream of AG-UI events, encoded as strings. - Use Case: Ideal for integrating with web frameworks other than Starlette (e.g., Django, Flask) or when fine-grained control over input/output processing is required. handle_ag_ui_request(agent: Agent, request: Request, **kwargs)\n  - Description: Handles an incoming Starlette Request from an AG-UI frontend, processing it with the agent and returning a streaming Starlette Response. - Parameters:\n    - agent (Agent): The Pydantic AI agent instance. - request (Request): The Starlette Request object. - **kwargs: Optional arguments passed to Agent.iter(), including 'deps'. These can be dynamically varied per request, for example, based on the authenticated user. - Returns: A Starlette StreamingResponse object containing AG-UI events. - Use Case: The typical method for integration within Starlette-based web frameworks like FastAPI. Agent.to_ag_ui(**kwargs) -> ASGI Application\n  - Description: Converts the Pydantic AI agent into a standalone ASGI application that can handle AG-UI requests. - Parameters:\n    - **kwargs: Optional arguments passed to Agent.iter(), including 'deps'. These arguments are static for the ASGI app, except for AG-UI state which is injected. - Returns: An ASGI application instance. - Use Case: Can be mounted as a sub-application within an existing FastAPI application, providing a dedicated endpoint for the agent. ```\n\n----------------------------------------\n\nTITLE: Configure OpenAIProvider with API key programmatically\nDESCRIPTION: Demonstrates how to programmatically configure the `OpenAIProvider` by passing the API key directly during its instantiation. This method is useful for managing API keys within the application code rather than relying on environment variables.",
    "chunk_length": 2139
  },
  {
    "chunk_id": 121,
    "source": "pydantic_ai_llms_data",
    "content": "SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/openai.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nmodel = OpenAIModel('gpt-4o', provider=OpenAIProvider(api_key='your-api-key'))\nagent = Agent(model)\n... ```\n\n----------------------------------------\n\nTITLE: Launch Pydantic AI CLI from Agent Instance (Synchronous)\nDESCRIPTION: Launch an interactive `clai` CLI session directly from an `Agent` instance using the `to_cli_sync()` method. This allows immediate interaction with the AI model configured by the agent. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/cli.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4.1', instructions='You always respond in Italian.')\nagent.to_cli_sync()\n```\n\n----------------------------------------\n\nTITLE: Running Pydantic AI A2A App with Uvicorn\nDESCRIPTION: This Bash command illustrates how to serve the Pydantic AI A2A application, created using the `to_a2a` method, with the Uvicorn ASGI server. It specifies the Python module and the ASGI application instance to run, binding the server to all network interfaces on port 8000, making the agent accessible as an A2A endpoint. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/a2a.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nuvicorn agent_to_a2a:app --host 0.0.0.0 --port 8000\n```\n\n----------------------------------------\n\nTITLE: Configure per-model settings for Pydantic AI FallbackModel\nDESCRIPTION: This Python snippet illustrates how to apply individual `ModelSettings` to each model within a `FallbackModel` chain. It highlights the importance of configuring settings like `base_url` or `api_key` directly on each model instance rather than on the `FallbackModel` itself, allowing for distinct configurations for different providers. Dependencies include `pydantic_ai`, `OpenAIModel`, `AnthropicModel`, `FallbackModel`, and `ModelSettings`.",
    "chunk_length": 2066
  },
  {
    "chunk_id": 122,
    "source": "pydantic_ai_llms_data",
    "content": "SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/index.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.anthropic import AnthropicModel\nfrom pydantic_ai.models.fallback import FallbackModel\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.settings import ModelSettings\n```\n\n----------------------------------------\n\nTITLE: Pydantic-AI Agent Dependency Injection Pattern\nDESCRIPTION: Demonstrates how to use dependency injection with the `Agent` class in Pydantic-AI, allowing tools to access shared resources like a database connection via the `RunContext`. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/CLAUDE.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n@dataclass\nclass MyDeps:\n    database: DatabaseConn\n\nagent = Agent('openai:gpt-4o', deps_type=MyDeps)\n\n@agent.tool\nasync def get_data(ctx: RunContext[MyDeps]) -> str:\n    return await ctx.deps.database.fetch_data()\n```\n\n----------------------------------------\n\nTITLE: Run Pydantic AI CLI with uvx\nDESCRIPTION: Execute the `clai` command-line interface using `uvx`, a tool runner from `uv`. This command launches an interactive session for chatting with an AI model. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/cli.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nuvx clai\n```\n\n----------------------------------------\n\nTITLE: Configure New Logfire Project\nDESCRIPTION: Creates and configures a new project within Logfire for sending application data. Alternatively, you can use an existing project. This command sets up the necessary configuration files (e.g., in a .logfire directory) that the Logfire SDK will use at runtime. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/logfire.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npy-cli logfire projects new\n```\n\n----------------------------------------\n\nTITLE: Initialize pydantic-ai Agent with MistralModel object\nDESCRIPTION: Illustrates how to explicitly instantiate a `MistralModel` object and then pass it to the `Agent` constructor.",
    "chunk_length": 2060
  },
  {
    "chunk_id": 123,
    "source": "pydantic_ai_llms_data",
    "content": "This approach provides more explicit control over the model configuration. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/mistral.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.mistral import MistralModel\n\nmodel = MistralModel('mistral-small-latest')\nagent = Agent(model)\n... ```\n\n----------------------------------------\n\nTITLE: Initialize Agent with WebSearchTool\nDESCRIPTION: Demonstrates how to initialize a PydanticAI agent with the `WebSearchTool` to enable web search capabilities. The agent can then be used to run queries requiring up-to-date information, such as current news. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/builtin-tools.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent, WebSearchTool\n\nagent = Agent('anthropic:claude-sonnet-4-0', builtin_tools=[WebSearchTool()])\n\nresult = agent.run_sync('Give me a sentence with the biggest news in AI this week.')\n# > Scientists have developed a universal AI detector that can identify deepfake videos. ```\n\n----------------------------------------\n\nTITLE: Streaming Agent Events and Output in Pydantic-AI\nDESCRIPTION: This snippet demonstrates how to set up an AI agent using `pydantic-ai` with custom tools and stream its execution. It shows how to define a `WeatherService` with `get_forecast` and `get_historic_weather` methods, register `weather_forecast` as an agent tool, and then iterate asynchronously over agent run events to capture and process various stages like user prompts, model requests, and tool calls. It highlights the use of `async for` with `Agent.iter()` and `node.stream()` for real-time event handling. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/agents.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom dataclasses import dataclass\nfrom datetime import date\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.messages import (\n    FinalResultEvent,\n    FunctionToolCallEvent,\n    FunctionToolResultEvent,\n    PartDeltaEvent,\n    PartStartEvent,\n    TextPartDelta,\n    ThinkingPartDelta,\n    ToolCallPartDelta,\n)\nfrom pydantic_ai.tools import RunContext\n\n\n@dataclass\nclass WeatherService:\n    async def get_forecast(self, location: str, forecast_date: date) -> str:\n        # In real code: call weather API, DB queries, etc.",
    "chunk_length": 2352
  },
  {
    "chunk_id": 124,
    "source": "pydantic_ai_llms_data",
    "content": "return f'The forecast in {location} on {forecast_date} is 24C and sunny.'\n\n    async def get_historic_weather(self, location: str, forecast_date: date) -> str:\n        # In real code: call a historical weather API or DB\n        return f'The weather in {location} on {forecast_date} was 18C and partly cloudy.'\n\n\nweather_agent = Agent[WeatherService, str](\n    'openai:gpt-4o',\n    deps_type=WeatherService,\n    output_type=str,  # We'll produce a final answer as plain text\n    system_prompt='Providing a weather forecast at the locations the user provides.',\n)\n\n\n@weather_agent.tool\nasync def weather_forecast(\n    ctx: RunContext[WeatherService],\n    location: str,\n    forecast_date: date,\n) -> str:\n    if forecast_date >= date.today():\n        return await ctx.deps.get_forecast(location, forecast_date)\n    else:\n        return await ctx.deps.get_historic_weather(location, forecast_date)\n\n\noutput_messages: list[str] = []\n\n\nasync def main():\n    user_prompt = 'What will the weather be like in Paris on Tuesday?'\n\n    # Begin a node-by-node, streaming iteration\n    async with weather_agent.iter(user_prompt, deps=WeatherService()) as run:\n        async for node in run:\n            if Agent.is_user_prompt_node(node):\n                # A user prompt node => The user has provided input\n                output_messages.append(f'=== UserPromptNode: {node.user_prompt} ===')\n            elif Agent.is_model_request_node(node):\n                # A model request node => We can stream tokens from the model's request\n                output_messages.append('=== ModelRequestNode: streaming partial request tokens ===')\n                async with node.stream(run.ctx) as request_stream:\n                    final_result_found = False\n                    async for event in request_stream:\n                        if isinstance(event, PartStartEvent):\n                            output_messages.append(f'[Request] Starting part {event.index}: {event.part!r}')\n                        elif isinstance(event, PartDeltaEvent):\n                            if isinstance(event.delta, TextPartDelta):\n                                output_messages.append(\n                                    f'[Request] Part {event.index} text delta: {event.delta.content_delta!r}'\n                                )\n                            elif isinstance(event.delta, ThinkingPartDelta):\n                                output_messages.append(\n                                    f'[Request] Part {event.index} thinking delta: {event.delta.content_delta!r}'\n                                )\n                            elif isinstance(event.delta, ToolCallPartDelta):\n                                output_messages.append(\n                                    f'[Request] Part {event.index} args delta: {event.delta.args_delta}'\n                                )\n                        elif isinstance(event, FinalResultEvent):\n                            output_messages.append(\n                                f'[Result] The model started producing a final result (tool_name={event.tool_name})'\n                            )\n                            final_result_found = True\n                            break\n\n                    if final_result_found:\n                        # Once the final result is found, we can call `AgentStream.stream_text()` to stream the text.",
    "chunk_length": 3365
  },
  {
    "chunk_id": 125,
    "source": "pydantic_ai_llms_data",
    "content": "# A similar `AgentStream.stream_output()` method is available to stream structured output. async for output in request_stream.stream_text():\n                            output_messages.append(f'[Output] {output}')\n            elif Agent.is_call_tools_node(node):\n                # A handle-response node => The model returned some data, potentially calls a tool\n                output_messages.append('=== CallToolsNode: streaming partial response & tool usage ===')\n                async with node.stream(run.ctx) as handle_stream:\n                    async for event in handle_stream:\n                        if isinstance(event, FunctionToolCallEvent):\n                            output_messages.append(\n                                f'[Tools] The LLM calls tool={event.part.tool_name!r} with args={event.part.args} (tool_call_id={event.part.tool_call_id!r})'\n\n```\n\n----------------------------------------\n\nTITLE: Authenticate Modal CLI\nDESCRIPTION: This command authenticates the Modal CLI, which is a necessary prerequisite for deploying or running any applications on the Modal platform. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/examples/slack-lead-qualifier.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython/uv-run -m modal setup\n```\n\n----------------------------------------\n\nTITLE: Set Groq API Key Environment Variable\nDESCRIPTION: This command sets the `GROQ_API_KEY` environment variable, which is required for authenticating with the Groq API. Replace 'your-api-key' with your actual API key obtained from the Groq console. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/groq.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport GROQ_API_KEY='your-api-key'\n```\n\n----------------------------------------\n\nTITLE: Configure Pydantic AI with Perplexity API\nDESCRIPTION: Shows how to configure an `OpenAIModel` to interact with the Perplexity API. It utilizes `OpenAIProvider` by specifying a custom `base_url` and the Perplexity API key, enabling access to models like 'sonar-pro'. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/openai.md#_snippet_27\n\nLANGUAGE: Python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nmodel = OpenAIModel(\n    'sonar-pro',\n    provider=OpenAIProvider(\n        base_url='https://api.perplexity.ai',\n        api_key='your-perplexity-api-key',\n    ),\n)\nagent = Agent(model)\n...",
    "chunk_length": 2477
  },
  {
    "chunk_id": 126,
    "source": "pydantic_ai_llms_data",
    "content": "```\n\n----------------------------------------\n\nTITLE: Integrate Multiple LangChain Tools with Pydantic AI using LangChainToolset in Python\nDESCRIPTION: This snippet demonstrates how to integrate multiple LangChain tools or an entire LangChain toolkit, like `SlackToolkit`, into a Pydantic AI agent using the `LangChainToolset`. This approach allows for grouping related LangChain functionalities and providing them as a single toolset to the Pydantic AI agent. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/tools.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.agent_toolkits import SlackToolkit\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.ext.langchain import LangChainToolset\n\n\ntoolkit = SlackToolkit()\ntoolset = LangChainToolset(toolkit.get_tools())\n\nagent = Agent('openai:gpt-4o', toolsets=[toolset])\n# ... ```\n\n----------------------------------------\n\nTITLE: Initialize GoogleModel with Vertex AI using Application Default Credentials\nDESCRIPTION: This Python code shows how to configure `GoogleModel` to use Vertex AI by setting `vertexai=True` in the `GoogleProvider`. This method leverages Application Default Credentials for authentication, suitable when running within a GCP environment or with `gcloud` CLI configured. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/google.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.google import GoogleModel\nfrom pydantic_ai.providers.google import GoogleProvider\n\nprovider = GoogleProvider(vertexai=True)\nmodel = GoogleModel('gemini-1.5-flash', provider=provider)\nagent = Agent(model)\n... ```\n\n----------------------------------------\n\nTITLE: Use custom AsyncOpenAI client with pydantic-ai\nDESCRIPTION: Shows how to pass a pre-configured `AsyncOpenAI` client instance to `OpenAIProvider`. This allows for custom settings such as `max_retries`, `organization`, or `project` as defined in the OpenAI API documentation. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/openai.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import AsyncOpenAI\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nclient = AsyncOpenAI(max_retries=3)\nmodel = OpenAIModel('gpt-4o', provider=OpenAIProvider(openai_client=client))\nagent = Agent(model)\n...",
    "chunk_length": 2406
  },
  {
    "chunk_id": 127,
    "source": "pydantic_ai_llms_data",
    "content": "```\n\n----------------------------------------\n\nTITLE: Deploy Pydantic AI Slack Qualifier to Modal\nDESCRIPTION: Deploys the Slack lead qualifier application to a Modal workspace for persistent operation. This command makes the application continuously available and accessible without requiring manual execution, suitable for production environments. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/examples/slack-lead-qualifier.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython/uv-run -m modal deploy -m pydantic_ai_examples.slack_lead_qualifier.modal\n```\n\n----------------------------------------\n\nTITLE: Manage Tool Naming Conflicts with Prefixes in Pydantic AI\nDESCRIPTION: Demonstrates how to use the `tool_prefix` parameter with `MCPServerSSE` to avoid naming conflicts when integrating multiple MCP servers that might expose tools with identical names. Each server's tools are prefixed, ensuring unique identification and preventing clashes. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/mcp/client.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerSSE\n\n# Create two servers with different prefixes\nweather_server = MCPServerSSE(\n    url='http://localhost:3001/sse',\n    tool_prefix='weather'  # Tools will be prefixed with 'weather_'\n)\n\ncalculator_server = MCPServerSSE(\n    url='http://localhost:3002/sse',\n    tool_prefix='calc'  # Tools will be prefixed with 'calc_'\n)\n\n# Both servers might have a tool named 'get_data', but they'll be exposed as:\n# - 'weather_get_data'\n# - 'calc_get_data'\nagent = Agent('openai:gpt-4o', toolsets=[weather_server, calculator_server])\n```\n\n----------------------------------------\n\nTITLE: Stream User Profile with Basic Streaming (pydantic-ai)\nDESCRIPTION: This snippet demonstrates how to stream structured output, specifically a `UserProfile` using `pydantic-ai`'s `Agent.run_stream`. It shows defining a `TypedDict` for the output schema and asynchronously iterating over the streamed partial results, printing each update as the profile is built.",
    "chunk_length": 2081
  },
  {
    "chunk_id": 128,
    "source": "pydantic_ai_llms_data",
    "content": "SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/output.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import date\n\nfrom typing_extensions import TypedDict, NotRequired\n\nfrom pydantic_ai import Agent\n\n\nclass UserProfile(TypedDict):\n    name: str\n    dob: NotRequired[date]\n    bio: NotRequired[str]\n\n\nagent = Agent(\n    'openai:gpt-4o',\n    output_type=UserProfile,\n    system_prompt='Extract a user profile from the input',\n)\n\n\nasync def main():\n    user_input = 'My name is Ben, I was born on January 28th 1990, I like the chain the dog and the pyramid.'\n    async with agent.run_stream(user_input) as result:\n        async for profile in result.stream():\n            print(profile)\n```\n\n----------------------------------------\n\nTITLE: Pydantic AI Structured Output with ToolOutput\nDESCRIPTION: Illustrates how to define multiple structured output types using `ToolOutput` marker classes for a Pydantic AI agent. The agent can then return instances of `BaseModel` subclasses (e.g., `Fruit`, `Vehicle`) based on the query, demonstrating how to map model responses to specific data structures and customize tool names. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/output.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent, ToolOutput\n\n\nclass Fruit(BaseModel):\n    name: str\n    color: str\n\n\nclass Vehicle(BaseModel):\n    name: str\n    wheels: int\n\n\nagent = Agent(\n    'openai:gpt-4o',\n    output_type=[ # (1)! ToolOutput(Fruit, name='return_fruit'),\n        ToolOutput(Vehicle, name='return_vehicle'),\n    ],\n)\nresult = agent.run_sync('What is a banana?')\nprint(repr(result.output))\n#> Fruit(name='banana', color='yellow')\n```\n\n----------------------------------------\n\nTITLE: Gradio UI Implementation for Weather Agent\nDESCRIPTION: The Python code responsible for building the Gradio web interface for the Pydantic AI weather agent. This file includes the UI components and integrates with the agent's backend logic to provide an interactive chat experience.",
    "chunk_length": 2055
  },
  {
    "chunk_id": 129,
    "source": "pydantic_ai_llms_data",
    "content": "SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/examples/weather-agent.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n// Code from /examples/pydantic_ai_examples/weather_agent_gradio.py\n```\n\n----------------------------------------\n\nTITLE: Initialize pydantic-ai Agent with HuggingFaceModel object\nDESCRIPTION: This Python code shows how to explicitly instantiate a `HuggingFaceModel` object with a specific model name and then pass this model object to the `Agent` constructor. This approach provides more control and is a prerequisite for further model or provider customization. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/huggingface.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.huggingface import HuggingFaceModel\n\nmodel = HuggingFaceModel('Qwen/Qwen3-235B-A22B')\nagent = Agent(model)\n... ```\n\n----------------------------------------\n\nTITLE: pydantic-ai Mistral Model and Provider Configuration\nDESCRIPTION: Details the configuration options for `MistralModel` and `MistralProvider` within `pydantic-ai`, including how to specify model names, API keys, base URLs, and custom HTTP clients for interacting with the Mistral API. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/mistral.md#_snippet_6\n\nLANGUAGE: APIDOC\nCODE:\n```\nMistralModel(model_name: str, provider: Optional[MistralProvider] = None)\n  - model_name: The name of the Mistral model to use (e.g., 'mistral-large-latest', 'mistral-small-latest'). - provider: An optional instance of MistralProvider for custom configuration. MistralProvider(api_key: str, base_url: Optional[str] = None, http_client: Optional[httpx.AsyncClient] = None)\n  - api_key: Your Mistral API key. Can also be set via MISTRAL_API_KEY environment variable. - base_url: Optional custom base URL for the Mistral API endpoint. - http_client: Optional custom httpx.AsyncClient instance for advanced HTTP request control (e.g., timeouts, proxies). Agent(model: Union[str, MistralModel])\n  - model: The model to use, either as a string identifier (e.g., 'mistral:mistral-large-latest') or a configured MistralModel object.",
    "chunk_length": 2163
  },
  {
    "chunk_id": 130,
    "source": "pydantic_ai_llms_data",
    "content": "```\n\n----------------------------------------\n\nTITLE: Define Pydantic AI Agent with Custom Dependencies\nDESCRIPTION: This Python code defines a `Pydantic AI Agent` (`joke_agent`) that utilizes custom dependencies encapsulated in the `MyDeps` dataclass. `MyDeps` includes an `httpx.AsyncClient` and a `system_prompt_factory` method for dynamically generating system prompts. The `application_code` function demonstrates how to instantiate `MyDeps` and pass it to `joke_agent.run` for execution, showcasing dependency injection. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/dependencies.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom dataclasses import dataclass\n\nimport httpx\n\nfrom pydantic_ai import Agent, RunContext\n\n\n@dataclass\nclass MyDeps:\n    api_key: str\n    http_client: httpx.AsyncClient\n\n    async def system_prompt_factory(self) -> str:  # (1)! response = await self.http_client.get('https://example.com')\n        response.raise_for_status()\n        return f'Prompt: {response.text}'\n\n\njoke_agent = Agent('openai:gpt-4o', deps_type=MyDeps)\n\n\n@joke_agent.system_prompt\nasync def get_system_prompt(ctx: RunContext[MyDeps]) -> str:\n    return await ctx.deps.system_prompt_factory()  # (2)! async def application_code(prompt: str) -> str:  # (3)! ... ... # now deep within application code we call our agent\n    async with httpx.AsyncClient() as client:\n        app_deps = MyDeps('foobar', client)\n        result = await joke_agent.run(prompt, deps=app_deps)  # (4)! return result.output\n```\n\n----------------------------------------\n\nTITLE: In-Browser TypeScript Transpilation and Loading Utility\nDESCRIPTION: This JavaScript function demonstrates a client-side approach to load and transpile TypeScript code from a specified URL (`/chat_app.ts`) using a global `ts` object (e.g., from a CDN). The transpiled JavaScript is then dynamically appended to the document as a module script. This method is explicitly stated as a non-production-ready hack for development or demonstration purposes. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/chat_app.html#_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\nasync function loadTs() {\n  const response = await fetch('/chat_app.ts');\n  const tsCode = await response.text();\n  const jsCode = window.ts.transpile(tsCode, { target: \"es2015\" });\n  let script = document.createElement('script');\n  script.type = 'module';\n  script.text = jsCode;\n  document.body.appendChild(script);\n}\nloadTs().catch((e) => {\n  console.error(e);\n  document.getElementById('error').classList.remove('d-none');\n  document.getElementById('spinner').classList.remove('active');\n});\n```\n\n----------------------------------------\n\nTITLE: Pydantic-AI Agent Streaming API Reference\nDESCRIPTION: Comprehensive documentation for the core API components involved in streaming text responses from pydantic-ai agents, including the Agent class, `run_stream` method, and `stream_text` method.",
    "chunk_length": 2959
  },
  {
    "chunk_id": 131,
    "source": "pydantic_ai_llms_data",
    "content": "SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/output.md#_snippet_14\n\nLANGUAGE: APIDOC\nCODE:\n```\nPydantic-AI Agent Streaming API:\n\nAgent:\n  - Class: pydantic_ai.Agent\n  - Description: The main class for interacting with AI models. - Instantiation: Agent(model_name: str)\n\nAgent.run_stream:\n  - Method of: pydantic_ai.agent.AbstractAgent\n  - Signature: async with agent.run_stream(prompt: str) -> StreamedRunResult\n  - Parameters:\n    - prompt (str): The input prompt for the AI model. - Returns: An asynchronous context manager that yields a StreamedRunResult object. - Purpose: Initiates a streaming interaction with the AI model, allowing for real-time text output. The context manager ensures proper connection closure. StreamedRunResult.stream_text:\n  - Method of: pydantic_ai.result.StreamedRunResult\n  - Signature: async for message in result.stream_text(delta: bool = False) -> AsyncIterator[str]\n  - Parameters:\n    - delta (bool, optional): If True, streams incremental text changes (deltas). If False (default), streams the complete text response with each update. - Returns: An asynchronous iterator yielding string messages. - Purpose: Provides access to the streamed text output from the AI model. - Note: If `delta=True`, the final output message will NOT be added to the result messages. ```\n\n----------------------------------------\n\nTITLE: Pydantic AI Agent Testing Utilities Reference\nDESCRIPTION: This section provides a reference for key utilities used in testing Pydantic AI agents. It covers `Agent.override` for mocking models, `TestModel` for simulating LLM responses, `capture_run_messages` for inspecting agent-model interactions, and `IsNow` for handling dynamic timestamps in assertions. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/testing.md#_snippet_3\n\nLANGUAGE: APIDOC\nCODE:\n```\nAgent.override\n  Description: A context manager used to temporarily replace an agent's underlying model, typically with a mock model like `TestModel`, without needing to modify the agent's `run*` method call sites.",
    "chunk_length": 2059
  },
  {
    "chunk_id": 132,
    "source": "pydantic_ai_llms_data",
    "content": "Usage: Used in testing to control the agent's model behavior. TestModel\n  Description: A mock model used in testing Pydantic AI agents. By default, it returns a JSON string summarizing tool calls and their returns. Parameters:\n    custom_output_text (str, optional): Allows customizing the model's response to a specific string instead of the default JSON summary. Usage: Replaces the real LLM during tests to prevent actual API calls and control responses. capture_run_messages\n  Description: A utility function to inspect the message exchange between an agent and its model during a run. Returns: A list of messages (e.g., `ModelRequest`, `ModelResponse`, `ToolReturnPart`, `TextPart`) representing the interaction. Usage: Used for asserting the exact sequence and content of messages exchanged during an agent's operation. IsNow (from dirty_equals)\n  Description: A helper for declarative assertions, particularly useful for comparing data that includes timestamps, allowing for flexible matching of current or recent timestamps. Usage: Enables robust assertions in tests where exact timestamp matching is problematic. ```\n\n----------------------------------------\n\nTITLE: Initialize Bedrock Model Directly\nDESCRIPTION: Illustrates how to explicitly initialize a `BedrockConverseModel` instance with a specific model name and then pass this model object to an `Agent`, providing more control over model instantiation. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/bedrock.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.bedrock import BedrockConverseModel\n\nmodel = BedrockConverseModel('anthropic.claude-3-sonnet-20240229-v1:0')\nagent = Agent(model)\n... ```\n\n----------------------------------------\n\nTITLE: pydantic_ai.toolsets Module Members\nDESCRIPTION: This entry documents the key classes and functions available within the `pydantic_ai.toolsets` module. These components provide flexible ways to define, combine, filter, and manage tools for AI applications, supporting patterns like deferred execution, prefixing, renaming, and wrapping.",
    "chunk_length": 2119
  },
  {
    "chunk_id": 133,
    "source": "pydantic_ai_llms_data",
    "content": "SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/api/toolsets.md#_snippet_0\n\nLANGUAGE: APIDOC\nCODE:\n```\npydantic_ai.toolsets Module Members:\n\nAbstractToolset\n  - Description: Base abstract class for defining custom toolsets. Provides a common interface for managing and interacting with collections of tools. CombinedToolset\n  - Description: A concrete toolset implementation that aggregates tools from multiple other toolsets. Useful for creating a unified interface from disparate tool sources. DeferredToolset\n  - Description: A toolset that defers the loading or initialization of its tools until they are actually needed. Improves performance by avoiding unnecessary resource allocation. FilteredToolset\n  - Description: Creates a new toolset by applying a filter to an existing toolset, including only tools that match specified criteria. FunctionToolset\n  - Description: A toolset constructed directly from a collection of Python functions, treating each function as a callable tool. PrefixedToolset\n  - Description: Modifies an existing toolset by adding a specified prefix to the names of all its contained tools. Useful for avoiding naming conflicts. RenamedToolset\n  - Description: Allows for the renaming of specific tools within an existing toolset, providing more descriptive or consistent naming conventions. PreparedToolset\n  - Description: A toolset where tools undergo a preparation or transformation step before being made available. WrapperToolset\n  - Description: A toolset that wraps another toolset, allowing for the interception or modification of tool access and behavior without altering the original toolset. ToolsetFunc\n  - Description: Likely a type alias or a utility function signature related to the creation or manipulation of toolsets. ```\n\n----------------------------------------\n\nTITLE: Visualize Agent-LLM Tool Interaction Flow (Mermaid)\nDESCRIPTION: This Mermaid sequence diagram visually represents the interaction flow between a `pydantic-ai` Agent and the Language Model (LLM) during a tool-enabled conversation.",
    "chunk_length": 2068
  },
  {
    "chunk_id": 134,
    "source": "pydantic_ai_llms_data",
    "content": "It highlights the steps where the LLM calls agent tools (`roll_dice`, `get_player_name`), the agent executes them, and returns results, ultimately leading to the LLM's final response. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/tools.md#_snippet_2\n\nLANGUAGE: mermaid\nCODE:\n```\nsequenceDiagram\n    participant Agent\n    participant LLM\n\n    Note over Agent: Send prompts\n    Agent ->> LLM: System: \"You're a dice game...\"<br>User: \"My guess is 4\"\n    activate LLM\n    Note over LLM: LLM decides to use<br>a tool\n\n    LLM ->> Agent: Call tool<br>roll_dice()\n    deactivate LLM\n    activate Agent\n    Note over Agent: Rolls a six-sided die\n\n    Agent -->> LLM: ToolReturn<br>\"4\"\n    deactivate Agent\n    activate LLM\n    Note over LLM: LLM decides to use<br>another tool\n\n    LLM ->> Agent: Call tool<br>get_player_name()\n    deactivate LLM\n    activate Agent\n    Note over Agent: Retrieves player name\n    Agent -->> LLM: ToolReturn<br>\"Anne\"\n    deactivate Agent\n    activate LLM\n    Note over LLM: LLM constructs final response\n\n    LLM ->> Agent: ModelResponse<br>\"Congratulations Anne, ...\"\n    deactivate LLM\n    Note over Agent: Game session complete\n```\n\n----------------------------------------\n\nTITLE: Configure Hugging Face provider with custom AsyncInferenceClient\nDESCRIPTION: This Python code demonstrates how to create a custom `AsyncInferenceClient` from `huggingface_hub` with specific configurations (e.g., `bill_to`, `api_key`, `provider`) and then pass this client to the `HuggingFaceProvider`. This enables advanced customization of the underlying Hugging Face client behavior, such as billing to a specific organization or using a custom base URL. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/huggingface.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import AsyncInferenceClient\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.huggingface import HuggingFaceModel\nfrom pydantic_ai.providers.huggingface import HuggingFaceProvider\n\nclient = AsyncInferenceClient(\n    bill_to='openai',\n    api_key='hf_token',\n    provider='fireworks-ai',\n)\n\nmodel = HuggingFaceModel(\n    'Qwen/Qwen3-235B-A22B',\n    provider=HuggingFaceProvider(hf_client=client),\n)\nagent = Agent(model)\n...",
    "chunk_length": 2258
  },
  {
    "chunk_id": 135,
    "source": "pydantic_ai_llms_data",
    "content": "```\n\n----------------------------------------\n\nTITLE: Pydantic AI Provider Classes\nDESCRIPTION: Documentation for various provider classes in `pydantic-ai`, enabling integration with different LLM services. Each provider class handles specific authentication and endpoint configurations for its respective service. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/openai.md#_snippet_24\n\nLANGUAGE: APIDOC\nCODE:\n```\nOpenAIProvider:\n  __init__(base_url: str = 'https://api.openai.com/v1', api_key: str = None)\n    base_url: The base URL for the OpenAI-compatible API endpoint. Defaults to OpenAI's official API. api_key: (Optional) Your OpenAI API key. Can be omitted if set via environment variables. AzureProvider:\n  __init__(azure_endpoint: str, api_version: str, api_key: str)\n    azure_endpoint: The base URL for your Azure AI service endpoint. api_version: The API version to use (e.g., '2024-02-01'). api_key: Your Azure API key. OpenRouterProvider:\n  __init__(api_key: str)\n    api_key: Your OpenRouter API key. VercelProvider:\n  __init__(api_key: str = None)\n    api_key: (Optional) Your Vercel AI Gateway API key. If not provided, attempts to read from VERCEL_AI_GATEWAY_API_KEY or VERCEL_OIDC_TOKEN environment variables. GrokProvider:\n  __init__(api_key: str)\n    api_key: Your xAI API key for Grok models. MoonshotAIProvider:\n  __init__(api_key: str)\n    api_key: Your MoonshotAI API key. ```\n\n----------------------------------------\n\nTITLE: Integrate Pydantic AI with Vercel AI Gateway\nDESCRIPTION: Demonstrates configuring `pydantic-ai` to use Vercel AI Gateway. It shows two methods: automatic credential detection via environment variables and direct API key passing during provider instantiation. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/openai.md#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.vercel import VercelProvider\n\n# Uses environment variable automatically\nmodel = OpenAIModel(\n    'anthropic/claude-4-sonnet',\n    provider=VercelProvider(),\n)\nagent = Agent(model)\n\n# Or pass the API key directly\nmodel = OpenAIModel(\n    'anthropic/claude-4-sonnet',\n    provider=VercelProvider(api_key='your-vercel-ai-gateway-api-key'),\n)\nagent = Agent(model)\n...",
    "chunk_length": 2324
  },
  {
    "chunk_id": 136,
    "source": "pydantic_ai_llms_data",
    "content": "```\n\n----------------------------------------\n\nTITLE: Set OpenAI API key environment variable\nDESCRIPTION: Demonstrates how to set the `OPENAI_API_KEY` environment variable. This is the default method `pydantic-ai` uses to authenticate with the OpenAI API. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/openai.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY='your-api-key'\n```\n\n----------------------------------------\n\nTITLE: pydantic-ai Direct API Request Methods Reference\nDESCRIPTION: Comprehensive reference for the low-level request methods available in `pydantic_ai.direct`. These functions provide direct control over LLM interactions, supporting various request patterns (sync/async, streamed/non-streamed) while handling input/output schema translation. They are ideal for custom abstractions or fine-grained control. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/direct.md#_snippet_3\n\nLANGUAGE: APIDOC\nCODE:\n```\npydantic_ai.direct.model_request(\n    model_name: str,\n    messages: list[ModelRequest],\n    model_request_parameters: ModelRequestParameters = None,\n    **kwargs\n) -> ModelResponse\n  - Purpose: Make a non-streamed asynchronous request to a model. - Parameters:\n    - model_name (str): The identifier for the LLM (e.g., 'anthropic:claude-3-5-haiku-latest', 'openai:gpt-4.1-nano'). - messages (list[ModelRequest]): A list of message objects representing the conversation history or prompt. - model_request_parameters (ModelRequestParameters, optional): Additional parameters for the model request, such as function tools or output allowances. - Returns: ModelResponse object containing the model's reply, usage information, etc. pydantic_ai.direct.model_request_sync(\n    model_name: str,\n    messages: list[ModelRequest],\n    model_request_parameters: ModelRequestParameters = None,\n    **kwargs\n) -> ModelResponse\n  - Purpose: Make a non-streamed synchronous request to a model. - Parameters: Same as model_request. - Returns: Same as model_request. pydantic_ai.direct.model_request_stream(\n    model_name: str,\n    messages: list[ModelRequest],\n    model_request_parameters: ModelRequestParameters = None,\n    **kwargs\n) -> AsyncIterator[ModelResponse]\n  - Purpose: Make a streamed asynchronous request to a model.",
    "chunk_length": 2294
  },
  {
    "chunk_id": 137,
    "source": "pydantic_ai_llms_data",
    "content": "- Parameters: Same as model_request. - Returns: An asynchronous iterator yielding ModelResponse objects as the stream progresses. pydantic_ai.direct.model_request_stream_sync(\n    model_name: str,\n    messages: list[ModelRequest],\n    model_request_parameters: ModelRequestParameters = None,\n    **kwargs\n) -> Iterator[ModelResponse]\n  - Purpose: Make a streamed synchronous request to a model. - Parameters: Same as model_request. - Returns: A synchronous iterator yielding ModelResponse objects as the stream progresses. ```\n\n----------------------------------------\n\nTITLE: Integrate DeepSeek provider with OpenAIModel\nDESCRIPTION: This snippet demonstrates how to use Pydantic AI's OpenAIModel with the DeepSeek API. It shows the instantiation of DeepSeekProvider with an API key, simplifying the connection to DeepSeek's services without needing to manually specify the base URL. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/openai.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.deepseek import DeepSeekProvider\n\nmodel = OpenAIModel(\n    'deepseek-chat',\n    provider=DeepSeekProvider(api_key='your-deepseek-api-key'),\n)\nagent = Agent(model)\n... ```\n\n----------------------------------------\n\nTITLE: Convert Pydantic AI Agent to ASGI Application\nDESCRIPTION: This Python snippet demonstrates how to instantiate a Pydantic AI `Agent` and convert it into a standard ASGI application using the `agent.to_a2a()` method. The resulting `app` object can then be served by any ASGI-compatible web server, enabling the agent to function as an A2A server. This method simplifies the exposure of AI agents as network services. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/a2a.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4.1', instructions='Be fun!')\napp = agent.to_a2a()\n```\n\n----------------------------------------\n\nTITLE: Connect Pydantic AI Agent to MCP Stdio Server\nDESCRIPTION: Illustrates connecting a Pydantic AI `Agent` to an MCP server using the standard I/O (stdio) transport via `MCPServerStdio`.",
    "chunk_length": 2203
  },
  {
    "chunk_id": 138,
    "source": "pydantic_ai_llms_data",
    "content": "The server is launched as a subprocess with specified arguments, and the agent communicates with it over stdin/stdout for tool execution. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/mcp/client.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerStdio\n\nserver = MCPServerStdio(  # (1)! 'deno',\n    args=[\n        'run',\n        '-N',\n        '-R=node_modules',\n        '-W=node_modules',\n        '--node-modules-dir=auto',\n        'jsr:@pydantic/mcp-run-python',\n        'stdio',\n    ]\n)\nagent = Agent('openai:gpt-4o', toolsets=[server])\n\n\nasync def main():\n    async with agent:\n        result = await agent.run('How many days between 2000-01-01 and 2025-03-18?')\n    print(result.output)\n    # There are 9,208 days between January 1, 2000, and March 18, 2025. ```\n\n----------------------------------------\n\nTITLE: Set GitHub API Key Environment Variable\nDESCRIPTION: Illustrates how to set the `GITHUB_API_KEY` environment variable. This provides an alternative method for authenticating with GitHub Models, allowing the API key to be managed outside the code. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/openai.md#_snippet_26\n\nLANGUAGE: Bash\nCODE:\n```\nexport GITHUB_API_KEY='your-github-token'\n```\n\n----------------------------------------\n\nTITLE: Initialize CohereModel with Custom Provider and API Key\nDESCRIPTION: This Python code demonstrates how to provide a custom `CohereProvider` instance to the `CohereModel` constructor, allowing direct specification of the API key within the code. This is useful when the API key is not set as an environment variable or needs to be dynamically managed for different model instances. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/cohere.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.cohere import CohereModel\nfrom pydantic_ai.providers.cohere import CohereProvider\n\nmodel = CohereModel('command', provider=CohereProvider(api_key='your-api-key'))\nagent = Agent(model)\n...",
    "chunk_length": 2093
  },
  {
    "chunk_id": 139,
    "source": "pydantic_ai_llms_data",
    "content": "```\n\n----------------------------------------\n\nTITLE: pydantic-ai GoogleModelSettings API Reference\nDESCRIPTION: Defines the configurable parameters for `GoogleModel` instances within the `pydantic-ai` framework, allowing fine-tuning of model behavior, thinking processes, and safety thresholds. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/google.md#_snippet_10\n\nLANGUAGE: APIDOC\nCODE:\n```\nClass: pydantic_ai.models.google.GoogleModelSettings\n\nParameters:\n  temperature (float, optional): Controls the randomness of the output. Higher values mean more random. max_tokens (int, optional): The maximum number of tokens to generate in the response. google_thinking_config (dict, optional): Configuration for the model's 'thinking' process. - thinking_budget (int): Budget for thinking, setting to 0 disables thinking. google_safety_settings (list[dict], optional): A list of safety settings to apply. Each dictionary should contain:\n    - category (HarmCategory): The safety category (e.g., HARM_CATEGORY_HATE_SPEECH). - threshold (HarmBlockThreshold): The blocking threshold for the category (e.g., BLOCK_LOW_AND_ABOVE). Usage Context:\n  Used to initialize pydantic_ai.Agent with a GoogleModel, e.g., Agent(model, model_settings=settings). ```\n\n----------------------------------------\n\nTITLE: Integrate Multiple LangChain Tools with Pydantic AI using LangChainToolset\nDESCRIPTION: This Python code demonstrates how to integrate multiple LangChain tools or an entire LangChain toolkit into a Pydantic AI agent using `LangChainToolset`. This is useful for grouping related functionalities, such as all tools from the `SlackToolkit`, and providing them to the agent as a single toolset. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/tools.md#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.agent_toolkits import SlackToolkit\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.ext.langchain import LangChainToolset\n\n\ntoolkit = SlackToolkit()\ntoolset = LangChainToolset(toolkit.get_tools())\n\nagent = Agent('openai:gpt-4o', toolsets=[toolset])\n# ...",
    "chunk_length": 2099
  },
  {
    "chunk_id": 140,
    "source": "pydantic_ai_llms_data",
    "content": "```\n\n----------------------------------------\n\nTITLE: Dynamically Registering Agent Toolsets with Pydantic-AI\nDESCRIPTION: This Python snippet demonstrates how to dynamically register toolsets with an `Agent` in Pydantic-AI using a function decorated with `@agent.toolset`. It shows how to switch between different toolsets (weather or datetime) based on agent dependencies (`RunContext`) and how to inspect available tools using `TestModel`. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/toolsets.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom dataclasses import dataclass\nfrom typing import Literal\n\nfrom function_toolset import weather_toolset, datetime_toolset\n\nfrom pydantic_ai import Agent, RunContext\nfrom pydantic_ai.models.test import TestModel\n\n\n@dataclass\nclass ToggleableDeps:\n    active: Literal['weather', 'datetime']\n\n    def toggle(self):\n        if self.active == 'weather':\n            self.active = 'datetime'\n        else:\n            self.active = 'weather'\n\ntest_model = TestModel()  # (1)! agent = Agent(\n    test_model,\n    deps_type=ToggleableDeps  # (2)! )\n\n@agent.toolset\ndef toggleable_toolset(ctx: RunContext[ToggleableDeps]):\n    if ctx.deps.active == 'weather':\n        return weather_toolset\n    else:\n        return datetime_toolset\n\n@agent.tool\ndef toggle(ctx: RunContext[ToggleableDeps]):\n    ctx.deps.toggle()\n\ndeps = ToggleableDeps('weather')\n\nresult = agent.run_sync('Toggle the toolset', deps=deps)\nprint([t.name for t in test_model.last_model_request_parameters.function_tools])  # (3)! # > ['toggle', 'now']\n\nresult = agent.run_sync('Toggle the toolset', deps=deps)\nprint([t.name for t in test_model.last_model_request_parameters.function_tools])\n# > ['toggle', 'temperature_celsius', 'temperature_fahrenheit', 'conditions']\n```\n\n----------------------------------------\n\nTITLE: Initialize pydantic-ai Agent with Mistral model name string\nDESCRIPTION: Shows how to create an `Agent` instance in `pydantic-ai` by directly passing a string identifier for a Mistral model. This method assumes the Mistral API key is configured via an environment variable.",
    "chunk_length": 2110
  },
  {
    "chunk_id": 141,
    "source": "pydantic_ai_llms_data",
    "content": "SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/mistral.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('mistral:mistral-large-latest')\n... ```\n\n----------------------------------------\n\nTITLE: Initialize Agent with Cohere Model by Name String\nDESCRIPTION: This Python code demonstrates how to initialize an `Agent` instance from `pydantic_ai` by directly specifying a Cohere model using its 'cohere:model_name' string identifier. This approach simplifies model instantiation when the API key is managed via environment variables. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/cohere.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('cohere:command')\n... ```\n\n----------------------------------------\n\nTITLE: Python: GenAI Email Feedback System Implementation\nDESCRIPTION: This Python code implements a generative AI system for writing and refining welcome emails using Pydantic AI and Pydantic Graph. It defines `User`, `Email`, and `State` dataclasses, along with `Agent` instances for email generation and feedback. The `WriteEmail` and `Feedback` nodes orchestrate the graph's execution, demonstrating how to integrate AI agents into a structured workflow. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/graph.md#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nfrom __future__ import annotations as _annotations\n\nfrom dataclasses import dataclass, field\n\nfrom pydantic import BaseModel, EmailStr\n\nfrom pydantic_ai import Agent, format_as_xml\nfrom pydantic_ai.messages import ModelMessage\nfrom pydantic_graph import BaseNode, End, Graph, GraphRunContext\n\n\n@dataclass\nclass User:\n    name: str\n    email: EmailStr\n    interests: list[str]\n\n\n@dataclass\nclass Email:\n    subject: str\n    body: str\n\n\n@dataclass\nclass State:\n    user: User\n    write_agent_messages: list[ModelMessage] = field(default_factory=list)\n\n\nemail_writer_agent = Agent(\n    'google-vertex:gemini-1.5-pro',\n    output_type=Email,\n    system_prompt='Write a welcome email to our tech blog.',\n)\n\n\n@dataclass\nclass WriteEmail(BaseNode[State]):\n    email_feedback: str | None = None\n\n    async def run(self, ctx: GraphRunContext[State]) -> Feedback:\n        if self.email_feedback:\n            prompt = (\n                f'Rewrite the email for the user:\\n'\n                f'{format_as_xml(ctx.state.user)}\\n'\n                f'Feedback: {self.email_feedback}'\n            )\n        else:\n            prompt = (\n                f'Write a welcome email for the user:\\n'\n                f'{format_as_xml(ctx.state.user)}'\n            )\n\n        result = await email_writer_agent.run(\n            prompt,\n            message_history=ctx.state.write_agent_messages,\n        )\n        ctx.state.write_agent_messages += result.new_messages()\n        return Feedback(result.output)\n\n\nclass EmailRequiresWrite(BaseModel):\n    feedback: str\n\n\nclass EmailOk(BaseModel):\n    pass\n\n\nfeedback_agent = Agent[None, EmailRequiresWrite | EmailOk](\n    'openai:gpt-4o',\n    output_type=EmailRequiresWrite | EmailOk,  # type: ignore\n    system_prompt=(\n        'Review the email and provide feedback, email must reference the users specific interests.'\n    ),\n)\n\n\n@dataclass\nclass Feedback(BaseNode[State, None, Email]):\n    email: Email\n\n    async def run(\n        self,\n        ctx: GraphRunContext[State],\n    ) -> WriteEmail | End[Email]:\n        prompt = format_as_xml({'user': ctx.state.user, 'email': self.email})\n        result = await feedback_agent.run(prompt)\n        if isinstance(result.output, EmailRequiresWrite):\n            return WriteEmail(email_feedback=result.output.feedback)\n        else:\n            return End(self.email)\n\n\nasync def main():\n    user = User(\n        name='John Doe',\n        email='john.joe@example.com',\n        interests=['Haskel', 'Lisp', 'Fortran'],\n    )\n    state = State(user)\n    feedback_graph = Graph(nodes=(WriteEmail, Feedback))\n    result = await feedback_graph.run(WriteEmail(), state=state)\n    print(result.output)\n    \"\"\"\n    Email(\n        subject='Welcome to our tech blog!',\n        body='Hello John, Welcome to our tech blog!",
    "chunk_length": 4170
  },
  {
    "chunk_id": 142,
    "source": "pydantic_ai_llms_data",
    "content": "...',\n    )\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Initialize GoogleModel with Vertex AI and Custom Location\nDESCRIPTION: This Python code illustrates how to specify a custom location (region) when using `GoogleModel` with Vertex AI. By passing the `location` argument to the `GoogleProvider`, you can control data residency and potentially improve latency. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/google.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.google import GoogleModel\nfrom pydantic_ai.providers.google import GoogleProvider\n\nprovider = GoogleProvider(vertexai=True, location='asia-east1')\nmodel = GoogleModel('gemini-1.5-flash', provider=provider)\nagent = Agent(model)\n... ```\n\n----------------------------------------\n\nTITLE: Chat Application UI Styling\nDESCRIPTION: This CSS defines the visual appearance and layout for a chat application's main container, conversation elements (user and AI messages), and an animated loading spinner. It includes basic styling for text, display, and a keyframe animation for the spinner. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/chat_app.html#_snippet_0\n\nLANGUAGE: CSS\nCODE:\n```\nmain { max-width: 700px; }\n#conversation .user::before { content: 'You asked: '; font-weight: bold; display: block; }\n#conversation .model::before { content: 'AI Response: '; font-weight: bold; display: block; }\n#spinner { opacity: 0; transition: opacity 500ms ease-in; width: 30px; height: 30px; border: 3px solid #222; border-bottom-color: transparent; border-radius: 50%; animation: rotation 1s linear infinite; }\n@keyframes rotation { 0% { transform: rotate(0deg); } 100% { transform: rotate(360deg); } }\n#spinner.active { opacity: 1; }\n```\n\n----------------------------------------\n\nTITLE: Execute and Persist Human-in-the-Loop AI Q&A Graph\nDESCRIPTION: This Python script demonstrates how to execute the previously defined `question_graph` using `pydantic-graph`'s state persistence.",
    "chunk_length": 2064
  },
  {
    "chunk_id": 143,
    "source": "pydantic_ai_llms_data",
    "content": "It shows how to load the graph's state from a file, provide user input (an answer via command line), and continue the graph's execution from the last saved point. This enables a multi-invocation, interactive human-in-the-loop workflow. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/graph.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport sys\nfrom pathlib import Path\n\nfrom pydantic_graph import End\nfrom pydantic_graph.persistence.file import FileStatePersistence\nfrom pydantic_ai.messages import ModelMessage  # noqa: F401\n\nfrom ai_q_and_a_graph import Ask, question_graph, Evaluate, QuestionState, Answer\n\n\nasync def main():\n    answer: str | None = sys.argv[1] if len(sys.argv) > 1 else None  # (1)! persistence = FileStatePersistence(Path('question_graph.json'))  # (2)! persistence.set_graph_types(question_graph)  # (3)! if snapshot := await persistence.load_next():  # (4)! state = snapshot.state\n        assert answer is not None\n        node = Evaluate(answer)\n    else:\n        state = QuestionState()\n        node = Ask()  # (5)! async with question_graph.iter(node, state=state, persistence=persistence) as run:\n        while True:\n            node = await run.next()  # (6)! if isinstance(node, End):  # (7)! print('END:', node.data)\n                history = await persistence.load_all()  # (8)! print([e.node for e in history])\n                break\n            elif isinstance(node, Answer):  # (9)! print(node.question)\n                #> What is the capital of France? break\n            # otherwise just continue\n```\n\n----------------------------------------\n\nTITLE: Initialize Pydantic-AI with OpenAI-compatible provider and custom retry client\nDESCRIPTION: This Python code demonstrates how to configure `pydantic-ai` to work with any OpenAI-compatible API endpoint. It shows the initialization of an `OpenAIModel` using a custom `OpenAIProvider` that integrates a retrying HTTP client, enabling robust handling of transient network issues and rate limits. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/retries.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nfrom smart_retry_example import create_retrying_client\n\nclient = create_retrying_client()\nmodel = OpenAIModel(\n    'your-model-name',  # Replace with actual model name\n    provider=OpenAIProvider(\n        base_url='https://api.example.com/v1',  # Replace with actual API URL\n        api_key='your-api-key',  # Replace with actual API key\n        http_client=client\n    )\n)\nagent = Agent(model)\n```\n\n----------------------------------------\n\nTITLE: Configure AWS Credentials via Environment Variables\nDESCRIPTION: Demonstrates how to set AWS credentials and default region as environment variables, which `boto3` and `pydantic-ai` can automatically pick up for authenticating with AWS Bedrock.",
    "chunk_length": 2937
  },
  {
    "chunk_id": 144,
    "source": "pydantic_ai_llms_data",
    "content": "SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/bedrock.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport AWS_BEARER_TOKEN_BEDROCK='your-api-key'\n# or:\nexport AWS_ACCESS_KEY_ID='your-access-key'\nexport AWS_SECRET_ACCESS_KEY='your-secret-key'\nexport AWS_DEFAULT_REGION='us-east-1'  # or your preferred region\n```\n\n----------------------------------------\n\nTITLE: Customize Tool Calls in Pydantic AI MCP Server\nDESCRIPTION: Shows how to implement a `process_tool_call` function to inject custom metadata (like dependencies) into tool call requests before they are sent to the MCP server. This allows for dynamic modification of tool arguments or context, enabling more flexible tool interactions. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/mcp/client.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import CallToolFunc, MCPServerStdio, ToolResult\nfrom pydantic_ai.models.test import TestModel\nfrom pydantic_ai.tools import RunContext\n\n\nasync def process_tool_call(\n    ctx: RunContext[int],\n    call_tool: CallToolFunc,\n    name: str,\n    tool_args: dict[str, Any],\n) -> ToolResult:\n    \"\"\"A tool call processor that passes along the deps.\"\"\"\n    return await call_tool(name, tool_args, {'deps': ctx.deps})\n\n\nserver = MCPServerStdio('python', ['mcp_server.py'], process_tool_call=process_tool_call)\nagent = Agent(\n    model=TestModel(call_tools=['echo_deps']),\n    deps_type=int,\n    toolsets=[server]\n)\n\n\nasync def main():\n    async with agent:\n        result = await agent.run('Echo with deps set to 42', deps=42)\n    print(result.output)\n    # {\"echo_deps\":{\"echo\":\"This is an echo message\",\"deps\":42}}\n```\n\n----------------------------------------\n\nTITLE: Simulate Slack team_join Event for Testing\nDESCRIPTION: This API documentation describes how to simulate a Slack 'team_join' event by sending a POST request to a specified webhook endpoint. It includes the required JSON payload structure for the event, useful for testing the lead qualification agent's response to new user signups.",
    "chunk_length": 2098
  },
  {
    "chunk_id": 145,
    "source": "pydantic_ai_llms_data",
    "content": "SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/examples/slack-lead-qualifier.md#_snippet_6\n\nLANGUAGE: APIDOC\nCODE:\n```\nPOST <webhook endpoint URL>\nContent-Type: application/json\n\nBody (JSON):\n{\n    \"type\": \"event_callback\",\n    \"event\": {\n        \"type\": \"team_join\",\n        \"user\": {\n            \"profile\": {\n                \"email\": \"samuel@pydantic.dev\",\n                \"first_name\": \"Samuel\",\n                \"last_name\": \"Colvin\",\n                \"display_name\": \"Samuel Colvin\"\n            }\n        }\n    }\n}\n\nExample Usage (cURL):\ncurl -X POST <webhook endpoint URL> \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"type\": \"event_callback\",\n    \"event\": {\n        \"type\": \"team_join\",\n        \"user\": {\n            \"profile\": {\n                \"email\": \"samuel@pydantic.dev\",\n                \"first_name\": \"Samuel\",\n                \"last_name\": \"Colvin\",\n                \"display_name\": \"Samuel Colvin\"\n            }\n        }\n    }\n}'\n```\n\n----------------------------------------\n\nTITLE: Configure Pydantic AI with Azure AI Foundry\nDESCRIPTION: Illustrates how to set up `pydantic-ai` to leverage Azure AI Foundry. This configuration requires specifying the Azure endpoint, API version, and an API key for authentication with the Azure service. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/openai.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.azure import AzureProvider\n\nmodel = OpenAIModel(\n    'gpt-4o',\n    provider=AzureProvider(\n        azure_endpoint='your-azure-endpoint',\n        api_version='your-api-version',\n        api_key='your-api-key',\n    ),\n)\nagent = Agent(model)\n... ```\n\n----------------------------------------\n\nTITLE: Customize Bedrock Runtime API Settings\nDESCRIPTION: Demonstrates how to apply custom settings to Bedrock Runtime API calls using `BedrockModelSettings`. This includes configuring guardrails and performance optimizations for the model, which are then passed to the `Agent`.",
    "chunk_length": 2071
  },
  {
    "chunk_id": 146,
    "source": "pydantic_ai_llms_data",
    "content": "SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/bedrock.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.bedrock import BedrockConverseModel, BedrockModelSettings\n\n# Define Bedrock model settings with guardrail and performance configurations\nbedrock_model_settings = BedrockModelSettings(\n    bedrock_guardrail_config={\n        'guardrailIdentifier': 'v1',\n        'guardrailVersion': 'v1',\n        'trace': 'enabled'\n    },\n    bedrock_performance_configuration={\n        'latency': 'optimized'\n    }\n)\n\n\nmodel = BedrockConverseModel(model_name='us.amazon.nova-pro-v1:0')\n\nagent = Agent(model=model, model_settings=bedrock_model_settings)\n```\n\n----------------------------------------\n\nTITLE: Pydantic AI Agent.to_a2a Method Reference\nDESCRIPTION: This API documentation describes the `Agent.to_a2a` method, a convenience function for transforming a Pydantic AI agent into an ASGI application suitable for A2A server deployment. It highlights that the method accepts arguments mirroring the `FastA2A` constructor and details its built-in capabilities, including automatic conversation history storage, context management for `context_id`-based messages, and the structured persistence of agent results as `TextPart` or `DataPart` artifacts with rich metadata. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/a2a.md#_snippet_5\n\nLANGUAGE: APIDOC\nCODE:\n```\nAgent.to_a2a(*args, **kwargs)\n  - Converts the Pydantic AI Agent into an ASGI application. - Parameters:\n    - *args, **kwargs: Arguments are passed directly to the `FastA2A` constructor. - Returns: An ASGI application object. - Automatic functionalities provided:\n    - Stores complete conversation history (including tool calls and responses) in the context storage. - Ensures that subsequent messages with the same `context_id` have access to the full conversation history. - Persists agent results as A2A artifacts:\n      - String results become `TextPart` artifacts and also appear in the message history.",
    "chunk_length": 2049
  },
  {
    "chunk_id": 147,
    "source": "pydantic_ai_llms_data",
    "content": "- Structured data (Pydantic models, dataclasses, tuples, etc.) become `DataPart` artifacts with the data wrapped as `{\"result\": <your_data>}`. - Artifacts include metadata with type information and JSON schema when available. ```\n\n----------------------------------------\n\nTITLE: pydantic_ai.models Module API Reference\nDESCRIPTION: Detailed API documentation for the `pydantic_ai.models` module, outlining its public members. This module is central to defining and managing AI model interactions, including data models for request parameters, handling streamed responses, and controlling access permissions for model requests. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/api/models/base.md#_snippet_0\n\nLANGUAGE: APIDOC\nCODE:\n```\nModule: pydantic_ai.models\n\nThis module provides core components for defining and managing AI model interactions within the pydantic_ai library. Members:\n  - KnownModelName:\n      Type: Class/Enum\n      Description: Represents a predefined set of known AI model names. - ModelRequestParameters:\n      Type: Class\n      Description: Pydantic model defining the parameters for making a request to an AI model. - Model:\n      Type: Class\n      Description: Abstract base class or interface for AI models, likely defining common methods or properties. - AbstractToolDefinition:\n      Type: Class\n      Description: Abstract base class for defining tools that AI models can use or interact with. - StreamedResponse:\n      Type: Class\n      Description: Handles and processes streamed responses from AI models, often used for real-time output. - ALLOW_MODEL_REQUESTS:\n      Type: Constant (bool)\n      Description: A global flag or configuration variable indicating whether AI model requests are generally permitted. - check_allow_model_requests():\n      Type: Function\n      Description: Checks the current status of the ALLOW_MODEL_REQUESTS flag to determine if model requests are allowed. Signature: check_allow_model_requests() -> bool\n  - override_allow_model_requests(allow: bool):\n      Type: Function\n      Description: Temporarily overrides the ALLOW_MODEL_REQUESTS flag, typically used as a context manager or decorator. Signature: override_allow_model_requests(allow: bool) -> None\n```\n\n----------------------------------------\n\nTITLE: Initialize AnthropicModel using Agent with model name string\nDESCRIPTION: This Python snippet demonstrates how to initialize an `Agent` instance from `pydantic_ai` by directly passing a string identifier for an Anthropic model.",
    "chunk_length": 2517
  },
  {
    "chunk_id": 148,
    "source": "pydantic_ai_llms_data",
    "content": "The `Agent` internally handles the creation of the `AnthropicModel` based on the provided name, simplifying model instantiation. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/anthropic.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('anthropic:claude-3-5-sonnet-latest')\n... ```\n\n----------------------------------------\n\nTITLE: Pydantic AI File URL Processing Behavior\nDESCRIPTION: Describes the default and model-specific behaviors for handling file URLs (e.g., ImageUrl, DocumentUrl) within Pydantic AI, including when files are downloaded by the user's application versus when URLs are passed directly to the AI model. It also covers the 'force_download' option for Google models. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/input.md#_snippet_4\n\nLANGUAGE: APIDOC\nCODE:\n```\nPydantic AI File URL Handling:\n\nGeneral Rule:\n  - When providing a URL using `ImageUrl`, `AudioUrl`, `VideoUrl`, or `DocumentUrl`, Pydantic AI typically downloads the file content and sends it as part of the API request. Model-Specific Behaviors:\n\nAnthropicModel:\n  - `DocumentUrl` (specifically for PDF documents): The URL is sent directly in the API request; no user-side download occurs. GoogleModel (on Vertex AI):\n  - All `FileUrl` types (`ImageUrl`, `AudioUrl`, `VideoUrl`, `DocumentUrl`): URLs are sent as-is in the API request; no data is downloaded beforehand. - Supported URLs (as per Gemini API docs for Vertex AI):\n    - Cloud Storage bucket URIs (e.g., `gs://bucket/path/to/file`)\n    - Public HTTP(S) URLs\n    - Public YouTube video URL (maximum one URL per request)\n  - Crawling Restrictions & `force_download`:\n    - If Gemini cannot access certain URLs due to crawling restrictions, you can instruct Pydantic AI to download the file content and send that instead of the URL. - To do this, set the boolean flag `force_download` to `True` on any object inheriting from `FileUrl`. - `force_download`: boolean, defaults to `False`. When `True`, forces Pydantic AI to download the content locally and send the data, rather than the URL.",
    "chunk_length": 2107
  },
  {
    "chunk_id": 149,
    "source": "pydantic_ai_llms_data",
    "content": "GoogleModel (on GLA):\n  - YouTube video URLs: Sent directly in the request to the model. ```\n\n----------------------------------------\n\nTITLE: Run OpenTelemetry TUI backend via Docker\nDESCRIPTION: This shell command provides instructions to launch the `otel-tui` OpenTelemetry terminal user interface backend using Docker. It exposes port 4318, allowing other applications to send OTLP (OpenTelemetry Protocol) traces to this local viewer for real-time debugging and visualization. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/logfire.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ndocker run --rm -it -p 4318:4318 --name otel-tui ymtdzzz/otel-tui:latest\n\n```\n\n----------------------------------------\n\nTITLE: AgentRun Object API Reference\nDESCRIPTION: Documentation for accessing usage statistics and final results from the `AgentRun` object in `pydantic-ai`. This includes the `usage()` method for runtime metrics and the `result` attribute for the final output. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/agents.md#_snippet_8\n\nLANGUAGE: APIDOC\nCODE:\n```\nAgentRun Object API:\n  Method: usage()\n    Description: Retrieves usage statistics (tokens, requests, etc.) at any time from the AgentRun object. Returns: A `pydantic_ai.usage.Usage` object containing the usage data. Attribute: result\n    Description: Becomes available once the run finishes. It holds the final output and related metadata. Type: `pydantic_ai.agent.AgentRunResult` object. ```\n\n----------------------------------------\n\nTITLE: Set OpenAI API Key Environment Variable\nDESCRIPTION: This command sets the `OPENAI_API_KEY` environment variable, which is required for `clai` to authenticate with the OpenAI API. Replace 'your-api-key-here' with your actual API key before running `clai`. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/clai/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY='your-api-key-here'\n```\n\n----------------------------------------\n\nTITLE: Integrate Pydantic AI with Remote Ollama\nDESCRIPTION: Shows how to connect `pydantic-ai` to a remote Ollama server by specifying its IP address and port in the `base_url` parameter.",
    "chunk_length": 2176
  },
  {
    "chunk_id": 150,
    "source": "pydantic_ai_llms_data",
    "content": "This enables distributed model inference using a model hosted elsewhere. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/openai.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nollama_model = OpenAIModel(\n    model_name='qwen2.5-coder:7b',  # (1)! provider=OpenAIProvider(base_url='http://192.168.1.74:11434/v1'),  # (2)! )\n\n\nclass CityLocation(BaseModel):\n    city: str\n    country: str\n\n\nagent = Agent(model=ollama_model, output_type=CityLocation)\n\nresult = agent.run_sync('Where were the olympics held in 2012?')\nprint(result.output)\n# city='London' country='United Kingdom'\nprint(result.usage())\n# Usage(requests=1, request_tokens=57, response_tokens=8, total_tokens=65)\n```\n\n----------------------------------------\n\nTITLE: Simulate Slack Team Join Event via cURL\nDESCRIPTION: This cURL command sends a POST request to the application's webhook endpoint, simulating a 'team_join' event. It includes a JSON payload with a mock user profile, which is highly useful for testing the lead qualification logic without requiring an actual Slack signup. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/examples/slack-lead-qualifier.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST <webhook endpoint URL> \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"type\": \"event_callback\",\n    \"event\": {\n        \"type\": \"team_join\",\n        \"user\": {\n            \"profile\": {\n                \"email\": \"samuel@pydantic.dev\",\n                \"first_name\": \"Samuel\",\n                \"last_name\": \"Colvin\",\n                \"display_name\": \"Samuel Colvin\"\n            }\n        }\n    }\n}'\n```\n\nLANGUAGE: APIDOC\nCODE:\n```\n{\n    \"type\": \"event_callback\",\n    \"event\": {\n        \"type\": \"team_join\",\n        \"user\": {\n            \"profile\": {\n                \"email\": \"samuel@pydantic.dev\",\n                \"first_name\": \"Samuel\",\n                \"last_name\": \"Colvin\",\n                \"display_name\": \"Samuel Colvin\"\n            }\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: AI Question and Answer Graph with Customizations\nDESCRIPTION: This Python code defines a question-answering graph using Pydantic Graph, demonstrating how to integrate AI agents (e.g., GPT-4o) into a state-based flow.",
    "chunk_length": 2404
  },
  {
    "chunk_id": 151,
    "source": "pydantic_ai_llms_data",
    "content": "It showcases advanced customization for Mermaid diagram generation by adding labels to edges using `Edge(label='...')` and enabling docstring-based notes for nodes via `docstring_notes = True` to enrich the visual representation of the graph. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/graph.md#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Annotated\n\nfrom pydantic_graph import BaseNode, End, Graph, GraphRunContext, Edge\n\nask_agent = Agent('openai:gpt-4o', output_type=str, instrument=True)\n\n\n@dataclass\nclass QuestionState:\n    question: str | None = None\n    ask_agent_messages: list[ModelMessage] = field(default_factory=list)\n    evaluate_agent_messages: list[ModelMessage] = field(default_factory=list)\n\n\n@dataclass\nclass Ask(BaseNode[QuestionState]):\n    \"\"\"Generate question using GPT-4o.\"\"\"\n    docstring_notes = True\n    async def run(\n        self, ctx: GraphRunContext[QuestionState]\n    ) -> Annotated[Answer, Edge(label='Ask the question')]:\n        result = await ask_agent.run(\n            'Ask a simple question with a single correct answer.',\n            message_history=ctx.state.ask_agent_messages,\n        )\n        ctx.state.ask_agent_messages += result.new_messages()\n        ctx.state.question = result.output\n        return Answer(result.output)\n\n\n@dataclass\nclass Answer(BaseNode[QuestionState]):\n    question: str\n\n    async def run(self, ctx: GraphRunContext[QuestionState]) -> Evaluate:\n        answer = input(f'{self.question}: ')\n        return Evaluate(answer)\n\n\nclass EvaluationResult(BaseModel, use_attribute_docstrings=True):\n    correct: bool\n    \"\"\"Whether the answer is correct.\"\"\"\n    comment: str\n    \"\"\"Comment on the answer, reprimand the user if the answer is wrong.\"\"\"\n\n\nevaluate_agent = Agent(\n    'openai:gpt-4o',\n    output_type=EvaluationResult,\n    system_prompt='Given a question and answer, evaluate if the answer is correct.',\n)\n\n\n@dataclass\nclass Evaluate(BaseNode[QuestionState, None, str]):\n    answer: str\n\n    async def run(\n        self,\n        ctx: GraphRunContext[QuestionState],\n    ) -> Annotated[End[str], Edge(label='success')] | Reprimand:\n        assert ctx.state.question is not None\n        result = await evaluate_agent.run(\n            format_as_xml({'question': ctx.state.question, 'answer': self.answer}),\n            message_history=ctx.state.evaluate_agent_messages,\n        )\n        ctx.state.evaluate_agent_messages += result.new_messages()\n        if result.output.correct:\n            return End(result.output.comment)\n        else:\n            return Reprimand(result.output.comment)\n\n\n@dataclass\nclass Reprimand(BaseNode[QuestionState]):\n    comment: str\n\n    async def run(self, ctx: GraphRunContext[QuestionState]) -> Ask:\n        print(f'Comment: {self.comment}')\n        ctx.state.question = None\n        return Ask()\n\n\nquestion_graph = Graph(\n    nodes=(Ask, Answer, Evaluate, Reprimand), state_type=QuestionState\n)\n```\n\n----------------------------------------\n\nTITLE: Configure Hugging Face API token environment variable\nDESCRIPTION: This command sets the `HF_TOKEN` environment variable, which is required for authenticating with Hugging Face Inference Providers.",
    "chunk_length": 3188
  },
  {
    "chunk_id": 152,
    "source": "pydantic_ai_llms_data",
    "content": "Replace 'hf_token' with your actual Hugging Face access token obtained from your Hugging Face settings. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/huggingface.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport HF_TOKEN='hf_token'\n```\n\n----------------------------------------\n\nTITLE: Use custom AsyncAzureOpenAI client with pydantic-ai\nDESCRIPTION: Demonstrates configuring `pydantic-ai` to use Azure OpenAI by providing an `AsyncAzureOpenAI` client instance. This client requires specific parameters like `azure_endpoint`, `api_version`, and `api_key` for connecting to Azure's service. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/openai.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import AsyncAzureOpenAI\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nclient = AsyncAzureOpenAI(\n    azure_endpoint='...',\n    api_version='2024-07-01-preview',\n    api_key='your-api-key',\n)\n\nmodel = OpenAIModel(\n    'gpt-4o',\n    provider=OpenAIProvider(openai_client=client),\n)\nagent = Agent(model)\n... ```\n\n----------------------------------------\n\nTITLE: Set Google API Key Environment Variable\nDESCRIPTION: This command sets the `GOOGLE_API_KEY` environment variable, which is used by `GoogleModel` to authenticate with the Generative Language API. Replace 'your-api-key' with your actual Google API key. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/google.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport GOOGLE_API_KEY=your-api-key\n```\n\n----------------------------------------\n\nTITLE: Pydantic AI Output Definition and Agent Execution API\nDESCRIPTION: Comprehensive documentation for Pydantic AI's core functionalities related to defining structured outputs and executing agents, including methods for streaming results and handling validation. This covers `StructuredDict` for custom schemas, the `@agent.output_validator` decorator for custom validation, and various agent execution methods like `run_stream()`, `run()`, and `iter()` for different streaming and completion behaviors.",
    "chunk_length": 2150
  },
  {
    "chunk_id": 153,
    "source": "pydantic_ai_llms_data",
    "content": "SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/output.md#_snippet_11\n\nLANGUAGE: APIDOC\nCODE:\n```\npydantic_ai.output.StructuredDict(schema: dict, name: str = None, description: str = None) -> Type[dict[str, Any]]\n  - Purpose: Generates a `dict[str, Any]` subclass with an attached JSON schema that Pydantic AI will pass to the model. - Parameters:\n    - `schema` (dict): The JSON schema definition (e.g., `{\"type\": \"object\", \"properties\": {...}}`). - `name` (str, optional): A descriptive name for the structured output, providing additional context to the model. - `description` (str, optional): A detailed description for the structured output, aiding the model's understanding. - Returns: A `dict[str, Any]` subclass that can be used as an `output_type` for an agent. - Notes: Pydantic AI does not perform validation of the received JSON object against this schema; it's up to the model to correctly interpret it. Your code should defensively read from the resulting dictionary. pydantic_ai.Agent.output_validator(func: Callable[[RunContext, OutputType], Awaitable[OutputType]]) -> Callable\n  - Purpose: A decorator to register an asynchronous validation function for an agent's output. - Parameters:\n    - `func` (Callable): An async function that takes `RunContext` and the agent's `output` as arguments. It should return the validated output or raise `ModelRetry` to prompt the model to try again. - Usage: Ideal for validation logic that requires I/O (e.g., database checks) or is asynchronous, complementing Pydantic's built-in validators. pydantic_ai.Agent.run_stream(...) -> pydantic_ai.result.StreamedRunResult\n  - Purpose: Executes the agent and streams the results, performing 'partial validation' of structured responses as they arrive. - Behavior: Streams just enough of the response to determine if it's a tool call or an output. Once the first output matching the `output_type` is identified, it is considered the final output, and the agent graph stops running, meaning no further tool calls will be executed. - Returns: A `StreamedRunResult` object, allowing access to the streamed output.",
    "chunk_length": 2123
  },
  {
    "chunk_id": 154,
    "source": "pydantic_ai_llms_data",
    "content": "pydantic_ai.agent.AbstractAgent.run(...) -> Any\n  - Purpose: Runs the agent to completion, processing all model responses and executing any tool calls. - Usage: Can be used with an `event_stream_handler` to stream all events (model responses, tool calls, etc.) from the agent's execution, providing full visibility into the agent's workflow. pydantic_ai.agent.AbstractAgent.iter(...) -> AsyncIterator[Any]\n  - Purpose: Provides an asynchronous iterator over all events and outputs generated during the agent's execution. - Usage: Similar to `run()` with an `event_stream_handler`, it ensures the agent graph runs to completion and allows for processing all intermediate events and the final output in a streaming fashion. ```\n\n----------------------------------------\n\nTITLE: Configure OpenAI model for thinking parts\nDESCRIPTION: This snippet demonstrates how to enable thinking parts for OpenAI models using `OpenAIResponsesModel`. It requires setting `openai_reasoning_effort` and `openai_reasoning_summary` fields within the `OpenAIResponsesModelSettings` to control the level of detail for the reasoning output. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/thinking.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIResponsesModel, OpenAIResponsesModelSettings\n\nmodel = OpenAIResponsesModel('o3-mini')\nsettings = OpenAIResponsesModelSettings(\n    openai_reasoning_effort='low',\n    openai_reasoning_summary='detailed',\n)\nagent = Agent(model, model_settings=settings)\n... ```\n\n----------------------------------------\n\nTITLE: Customize AnthropicProvider with httpx.AsyncClient\nDESCRIPTION: This Python snippet demonstrates how to inject a custom `httpx.AsyncClient` into the `AnthropicProvider` when initializing `AnthropicModel`. This allows for fine-grained control over HTTP request parameters like timeouts, proxies, or other client-specific configurations, enhancing flexibility for network interactions with the Anthropic API. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/anthropic.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom httpx import AsyncClient\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.anthropic import AnthropicModel\nfrom pydantic_ai.providers.anthropic import AnthropicProvider\n\ncustom_http_client = AsyncClient(timeout=30)\nmodel = AnthropicModel(\n    'claude-3-5-sonnet-latest',\n    provider=AnthropicProvider(api_key='your-api-key', http_client=custom_http_client),\n)\nagent = Agent(model)\n...",
    "chunk_length": 2538
  },
  {
    "chunk_id": 155,
    "source": "pydantic_ai_llms_data",
    "content": "```\n\n----------------------------------------\n\nTITLE: Configure OpenAIModel with custom base URL and API key\nDESCRIPTION: This snippet demonstrates how to connect Pydantic AI's OpenAIModel to any OpenAI-compatible API endpoint by explicitly specifying the base_url and api_key via an OpenAIProvider instance. This is useful for self-hosted or alternative OpenAI-compatible services. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/openai.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nmodel = OpenAIModel(\n    'model_name',\n    provider=OpenAIProvider(\n        base_url='https://<openai-compatible-api-endpoint>.com', api_key='your-api-key'\n    ),\n)\nagent = Agent(model)\n... ```\n\n----------------------------------------\n\nTITLE: Pydantic-AI Custom Toolset API\nDESCRIPTION: This API documentation describes how to define a fully custom toolset in Pydantic-AI by subclassing `AbstractToolset`. It outlines the required methods for listing and calling tools, as well as optional asynchronous context manager methods for resource management. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/toolsets.md#_snippet_9\n\nLANGUAGE: APIDOC\nCODE:\n```\nAbstractToolset:\n  - get_tools() -> List[Tool]:\n      Abstract method. Subclasses must implement this to return a list of available tools. - call_tool(tool_name: str, **kwargs) -> Any:\n      Abstract method. Subclasses must implement this to handle the invocation of a specific tool by its name and arguments. - __aenter__() -> Self:\n      Optional asynchronous context manager entry point. If implemented, this method will be called when the agent using the toolset is entered via an `async with` statement. Useful for setting up network connections or sessions. - __aexit__(exc_type, exc_val, exc_tb) -> None:\n      Optional asynchronous context manager exit point. If implemented, this method will be called when the agent using the toolset exits an `async with` block.",
    "chunk_length": 2078
  },
  {
    "chunk_id": 156,
    "source": "pydantic_ai_llms_data",
    "content": "Useful for cleaning up resources. ```\n\n----------------------------------------\n\nTITLE: Launch Pydantic AI CLI from Agent Instance (Asynchronous)\nDESCRIPTION: Launch an interactive `clai` CLI session asynchronously from an `Agent` instance using the `to_cli()` method. This method is suitable for integration into asynchronous Python applications. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/cli.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4.1', instructions='You always respond in Italian.')\n\nasync def main():\n    await agent.to_cli()\n```\n\n----------------------------------------\n\nTITLE: Pydantic-AI Type-Safe Agent Output Validation\nDESCRIPTION: Illustrates how to define a type-safe `Agent` with a Pydantic `BaseModel` for output validation, ensuring structured and predictable responses from the agent. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/CLAUDE.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nclass OutputModel(BaseModel):\n    result: str\n    confidence: float\n\nagent: Agent[MyDeps, OutputModel] = Agent(\n    'openai:gpt-4o',\n    deps_type=MyDeps,\n    output_type=OutputModel\n)\n```\n\n----------------------------------------\n\nTITLE: Run Pydantic AI CLI with Custom Agent\nDESCRIPTION: Execute the `clai` CLI using a custom agent defined in a Python module. The `--agent` flag takes a `module:variable` path, allowing the CLI to load and use the specified `Agent` instance for interactions. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/cli.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nuvx clai --agent custom_agent:agent \"What's the weather today?\"\n```\n\n----------------------------------------\n\nTITLE: Pydantic Evals Core API Reference\nDESCRIPTION: This section provides an overview of key classes and methods within the `pydantic-evals` library, essential for defining evaluation workflows. It covers how to structure test cases, implement custom evaluation logic, and manage the evaluation process. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/evals.md#_snippet_4\n\nLANGUAGE: APIDOC\nCODE:\n```\npydantic_evals.Case:\n  Represents a single test case for evaluation.",
    "chunk_length": 2188
  },
  {
    "chunk_id": 157,
    "source": "pydantic_ai_llms_data",
    "content": "__init__(name: str, inputs: Any, expected_output: Any, metadata: dict)\n    - name: A unique identifier for the case. - inputs: The input(s) to be passed to the function under evaluation. - expected_output: The expected output for the given inputs. - metadata: Optional dictionary for additional case-specific data. pydantic_evals.evaluators.Evaluator:\n  Abstract base class for custom evaluation logic. evaluate(self, ctx: EvaluatorContext[InputType, OutputType]) -> float\n    - ctx: An EvaluatorContext object containing the function's output, expected output, and other context. - Returns: A float score between 0.0 and 1.0, indicating the evaluation result. pydantic_evals.evaluators.EvaluatorContext:\n  Context object passed to an Evaluator's `evaluate` method. Attributes:\n    - output: The actual output from the function being evaluated. - expected_output: The expected output defined in the Case. - inputs: The inputs defined in the Case. - case: The full Case object being evaluated. pydantic_evals.evaluators.IsInstance:\n  A built-in evaluator that checks if the output is an instance of a specified type. __init__(type_name: str)\n    - type_name: The name of the type to check against (e.g., 'str', 'int'). pydantic_evals.Dataset:\n  Manages a collection of test cases and evaluators for a comprehensive evaluation. __init__(cases: list[Case], evaluators: list[Evaluator])\n    - cases: A list of Case objects to be evaluated. - evaluators: A list of Evaluator instances (custom or built-in) to apply to each case. evaluate_sync(self, func: Callable) -> EvaluationReport\n    - func: The synchronous function to be evaluated against all cases in the dataset. - Returns: An EvaluationReport object containing the results. pydantic_evals.reporting.EvaluationReport:\n  Stores and presents the results of an evaluation run. print(self, include_input: bool = False, include_output: bool = False, include_durations: bool = False)\n    - Prints a formatted summary of the evaluation results to the console. - include_input: If True, includes the input for each case in the report.",
    "chunk_length": 2080
  },
  {
    "chunk_id": 158,
    "source": "pydantic_ai_llms_data",
    "content": "- include_output: If True, includes the actual output for each case in the report. - include_durations: If True, includes the execution duration for each case. ```\n\n----------------------------------------\n\nTITLE: Pydantic-Graph Core Components API Reference\nDESCRIPTION: This section details the core components of the `pydantic-graph` library, including `GraphRunContext` for managing graph state and `End` for signaling graph termination. Both are generic types, allowing for flexible state and return value definitions within graph runs. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/graph.md#_snippet_1\n\nLANGUAGE: APIDOC\nCODE:\n```\nGraphRunContext:\n  Description: The context for the graph run, similar to Pydantic AI's RunContext. This holds the state of the graph and dependencies and is passed to nodes when they're run. Generics:\n    StateT: The state type of the graph it's used in. End:\n  Description: Return value to indicate the graph run should end. Generics:\n    RunEndT: The graph return type of the graph it's used in. ```\n\n----------------------------------------\n\nTITLE: Modal Application Secrets Configuration\nDESCRIPTION: Explains how to configure essential API keys and tokens (Slack, Logfire, OpenAI) as custom secrets within the Modal platform. These secrets are crucial for the application's secure operation and access to external services. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/examples/slack-lead-qualifier.md#_snippet_1\n\nLANGUAGE: APIDOC\nCODE:\n```\nModal Secrets Configuration:\n  - Name: slack\n    Key: SLACK_API_KEY\n    Value: Your Slack Access Token (obtained during Slack app installation)\n  - Name: logfire\n    Key: LOGFIRE_TOKEN\n    Value: Your Logfire Write Token (obtained from Logfire project settings)\n  - Name: openai\n    Key: OPENAI_API_KEY\n    Value: Your OpenAI API Key (obtained from OpenAI platform settings)\n```\n\n----------------------------------------\n\nTITLE: Question Graph State Diagram\nDESCRIPTION: A Mermaid state diagram illustrating the workflow of the question graph.",
    "chunk_length": 2061
  },
  {
    "chunk_id": 159,
    "source": "pydantic_ai_llms_data",
    "content": "It visualizes the progression through different states: 'Ask', 'Answer', 'Evaluate', 'Congratulate', and 'Castigate'. The diagram clearly shows the transitions between these states, including the conditions for moving from 'Evaluate' to either 'Congratulate' (success) or 'Castigate' (try again), and the loop back to 'Ask' if 'Castigate' occurs. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/examples/question-graph.md#_snippet_1\n\nLANGUAGE: mermaid\nCODE:\n```\n--- \ntitle: question_graph\n---\nstateDiagram-v2\n  [*] --> Ask\n  Ask --> Answer: ask the question\n  Answer --> Evaluate: answer the question\n  Evaluate --> Congratulate\n  Evaluate --> Castigate\n  Congratulate --> [*]: success\n  Castigate --> Ask: try again\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Type Mismatches in Pydantic AI Agents\nDESCRIPTION: This snippet illustrates how Pydantic AI agents leverage type hints for static analysis. It shows common type errors when `deps_type` and `output_type` are mismatched with function signatures or usage, and how `mypy` identifies these issues, ensuring robust application development. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/agents.md#_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_ai import Agent, RunContext\n\n\n@dataclass\nclass User:\n    name: str\n\n\nagent = Agent(\n    'test',\n    deps_type=User,  # (1)! output_type=bool,\n)\n\n\n@agent.system_prompt\ndef add_user_name(ctx: RunContext[str]) -> str:  # (2)! return f\"The user's name is {ctx.deps}.\"\n\n\ndef foobar(x: bytes) -> None:\n    pass\n\n\nresult = agent.run_sync('Does their name start with \"A\"?', deps=User('Anne'))\nfoobar(result.output)  # (3)! ```\n\nLANGUAGE: Bash\nCODE:\n```\n uv run mypy type_mistakes.py\ntype_mistakes.py:18: error: Argument 1 to \"system_prompt\" of \"Agent\" has incompatible type \"Callable[[RunContext[str]], str]\"; expected \"Callable[[RunContext[User]], str]\"  [arg-type]\ntype_mistakes.py:28: error: Argument 1 to \"foobar\" has incompatible type \"bool\"; expected \"bytes\"  [arg-type]\nFound 2 errors in 1 file (checked 1 source file)\n```\n\n----------------------------------------\n\nTITLE: Instrument Pydantic AI Agent with Logfire\nDESCRIPTION: This Python snippet demonstrates how to integrate Logfire for enhanced observability with Pydantic AI.",
    "chunk_length": 2326
  },
  {
    "chunk_id": 160,
    "source": "pydantic_ai_llms_data",
    "content": "By configuring Logfire and instrumenting `pydantic_ai`, developers can gain detailed insights into agent operations, tool calls, and the overall flow of AI interactions. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/mcp/client.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport logfire\n\nlogfire.configure()\nlogfire.instrument_pydantic_ai()\n```\n\n----------------------------------------\n\nTITLE: Pydantic AI Agent Execution and Event Streaming\nDESCRIPTION: This section details the `agent.run()` and `agent.run_stream()` methods for executing Pydantic AI agents, emphasizing their event streaming capabilities. It describes how to use an `event_stream_handler` to capture `PartStartEvent` and `PartDeltaEvent` for real-time insights into the agent's processing. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/agents.md#_snippet_4\n\nLANGUAGE: APIDOC\nCODE:\n```\nAbstractAgent.run(user_prompt: str, event_stream_handler: Optional[Callable] = None)\n  - Purpose: Runs the agent graph to completion, optionally streaming all events. - Parameters:\n    - user_prompt (str): The input prompt for the agent. - event_stream_handler (Optional[Callable]): A handler function to process streaming events. - Behavior with event_stream_handler: Requires manual reassembly of streamed text from PartStartEvent and PartDeltaEvent. - Returns: The final output of the agent run. AbstractAgent.run_stream()\n  - Purpose: Streams the agent's response, allowing for partial results before completion. Events for Streaming:\n  - PartStartEvent: Indicates the start of a new part in the streamed response. - PartDeltaEvent: Provides incremental updates for a part in the streamed response. ```\n\n----------------------------------------\n\nTITLE: Unit Testing Tool Calls with FunctionModel\nDESCRIPTION: This Python code demonstrates how to unit test an AI agent's tool calls using Pydantic-AI's `FunctionModel`. It defines a custom `call_weather_forecast` function that intercepts and simulates the LLM's behavior, extracting a date from the prompt and returning a `ToolCallPart` or `TextPart`.",
    "chunk_length": 2091
  },
  {
    "chunk_id": 161,
    "source": "pydantic_ai_llms_data",
    "content": "The `FunctionModel` is used to override the agent's default model, allowing for controlled testing of tool interactions and ensuring the `WeatherService.get_forecast` is properly exercised. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/testing.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport re\n\nimport pytest\n\nfrom pydantic_ai import models\nfrom pydantic_ai.messages import (\n    ModelMessage,\n    ModelResponse,\n    TextPart,\n    ToolCallPart,\n)\nfrom pydantic_ai.models.function import AgentInfo, FunctionModel\n\nfrom fake_database import DatabaseConn\nfrom weather_app import run_weather_forecast, weather_agent\n\npytestmark = pytest.mark.anyio\nmodels.ALLOW_MODEL_REQUESTS = False\n\n\ndef call_weather_forecast(  # (1)! messages: list[ModelMessage], info: AgentInfo\n) -> ModelResponse:\n    if len(messages) == 1:\n        # first call, call the weather forecast tool\n        user_prompt = messages[0].parts[-1]\n        m = re.search(r'\\d{4}-\\d{2}-\\d{2}', user_prompt.content)\n        assert m is not None\n        args = {'location': 'London', 'forecast_date': m.group()}  # (2)! return ModelResponse(parts=[ToolCallPart('weather_forecast', args)])\n    else:\n        # second call, return the forecast\n        msg = messages[-1].parts[0]\n        assert msg.part_kind == 'tool-return'\n        return ModelResponse(parts=[TextPart(f'The forecast is: {msg.content}')])\n\n\nasync def test_forecast_future():\n    conn = DatabaseConn()\n    user_id = 1\n    with weather_agent.override(model=FunctionModel(call_weather_forecast)):  # (3)! prompt = 'What will the weather be like in London on 2032-01-01?'\n        await run_weather_forecast([(prompt, user_id)], conn)\n\n    forecast = await conn.get_forecast(user_id)\n    assert forecast == 'The forecast is: Rainy with a chance of sun'\n```\n\n----------------------------------------\n\nTITLE: Instrument pydantic-ai direct API calls with Logfire\nDESCRIPTION: This snippet demonstrates how to enable OpenTelemetry/Logfire instrumentation for `pydantic-ai` direct API calls. By configuring Logfire and instrumenting `pydantic-ai`, all subsequent model requests will automatically generate traces and logs, aiding in observability and debugging.",
    "chunk_length": 2192
  },
  {
    "chunk_id": 162,
    "source": "pydantic_ai_llms_data",
    "content": "SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/direct.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport logfire\n\nfrom pydantic_ai.direct import model_request_sync\nfrom pydantic_ai.messages import ModelRequest\n\nlogfire.configure()\nlogfire.instrument_pydantic_ai()\n\n# Make a synchronous request to the model\nmodel_response = model_request_sync(\n    'anthropic:claude-3-5-haiku-latest',\n    [ModelRequest.user_text_prompt('What is the capital of France?')],\n)\n\nprint(model_response.parts[0].content)\n# The capital of France is Paris. ```\n\n----------------------------------------\n\nTITLE: Control Flow Diagram for Flight and Seat Booking Agents\nDESCRIPTION: This Mermaid graph visualizes the control flow of the application. It illustrates the interaction between the user, the `flight_search_agent`, and the `seat_preference_agent`, showing how the application progresses from asking for flight details to asking for seat preferences, and the iterative nature of agent interactions within their respective subgraphs. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/multi-agent-applications.md#_snippet_6\n\nLANGUAGE: mermaid\nCODE:\n```\ngraph TB\n  START --> ask_user_flight[\"ask user for flight\"]\n\n  subgraph find_flight\n    flight_search_agent --> ask_user_flight\n    ask_user_flight --> flight_search_agent\n  end\n\n  flight_search_agent --> ask_user_seat[\"ask user for seat\"]\n  flight_search_agent --> END\n\n  subgraph find_seat\n    seat_preference_agent --> ask_user_seat\n    ask_user_seat --> seat_preference_agent\n  end\n\n  seat_preference_agent --> END\n```\n\n----------------------------------------\n\nTITLE: Expose Pydantic AI Agent as A2A Server\nDESCRIPTION: This Python code snippet demonstrates how to instantiate a Pydantic AI Agent and convert it into an A2A (Agent2Agent) server using the `to_a2a()` convenience method. This allows the agent to communicate and receive requests following the A2A protocol, making it interoperable with other A2A-compliant agents. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/a2a.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4.1', instructions='Be fun!')\napp = agent.to_a2a()\n```\n\n----------------------------------------\n\nTITLE: Instrument a Specific Pydantic AI Model Instance\nDESCRIPTION: Python code showing how to apply `InstrumentationSettings` to a particular `Pydantic AI Model` instance.",
    "chunk_length": 2431
  },
  {
    "chunk_id": 163,
    "source": "pydantic_ai_llms_data",
    "content": "This allows for granular control over observability settings for individual models, enabling different tracing or logging configurations for distinct AI components within an application. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/logfire.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.instrumented import InstrumentationSettings, InstrumentedModel\n\nsettings = InstrumentationSettings()\nmodel = InstrumentedModel('gpt-4o', settings)\nagent = Agent(model)\n```\n\n----------------------------------------\n\nTITLE: Pydantic AI Agent Configuration and Usage Limits API\nDESCRIPTION: This section details key API components for configuring Pydantic AI agents, managing usage limits, and handling model retries. It covers the `UsageLimits` class for controlling token and request consumption, the `Agent` class methods for running agents and defining tools, and relevant exceptions like `ModelRetry` and `UsageLimitExceeded`. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/agents.md#_snippet_13\n\nLANGUAGE: APIDOC\nCODE:\n```\npydantic_ai.usage.UsageLimits:\n  A structure to help limit usage (tokens and/or requests) on model runs. Parameters:\n    response_tokens_limit: int, optional\n      Maximum number of response tokens allowed. request_limit: int, optional\n      Maximum number of requests allowed. pydantic_ai.Agent:\n  __init__(model_name: str, retries: int = 0, output_type: Type = None, system_prompt: str = None, ...)\n    Initializes an Agent instance. Parameters:\n      model_name: str\n        The name of the model to use (e.g., 'anthropic:claude-3-5-sonnet-latest'). retries: int, optional\n        Number of retries for model calls (default: 0). output_type: Type, optional\n        The expected output type for the agent. system_prompt: str, optional\n        A system prompt to guide the agent's behavior. run_sync(prompt: str, usage_limits: UsageLimits = None, ...) -> AgentRunResult\n    Synchronously runs the agent with a given prompt. Parameters:\n      prompt: str\n        The input prompt for the agent.",
    "chunk_length": 2091
  },
  {
    "chunk_id": 164,
    "source": "pydantic_ai_llms_data",
    "content": "usage_limits: UsageLimits, optional\n        An instance of UsageLimits to apply restrictions on tokens and requests. Returns:\n      AgentRunResult: The result of the agent run, including output and usage. tool_plain(retries: int = 0) -> Callable\n    Decorator for defining plain Python functions as agent tools. Parameters:\n      retries: int, optional\n        Number of retries for this specific tool call (default: 0). pydantic_ai.exceptions.UsageLimitExceeded:\n  Exception raised when a configured usage limit (e.g., response_tokens_limit, request_limit) is exceeded during an agent run. pydantic_ai.ModelRetry:\n  Exception that can be raised within a tool to signal the model to retry the current step. ```\n\n----------------------------------------\n\nTITLE: Customize Google Model Safety Settings\nDESCRIPTION: Illustrates how to apply custom safety settings to a Google model by defining `google_safety_settings` with a list of dictionaries, each specifying a `HarmCategory` and its corresponding `HarmBlockThreshold`. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/google.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom google.genai.types import HarmBlockThreshold, HarmCategory\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.google import GoogleModel, GoogleModelSettings\n\nmodel_settings = GoogleModelSettings(\n    google_safety_settings=[\n        {\n            'category': HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n            'threshold': HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n        }\n    ]\n)\nmodel = GoogleModel('gemini-2.0-flash')\nagent = Agent(model, model_settings=model_settings)\n... ```\n\n----------------------------------------\n\nTITLE: Pydantic Evals Dataset Generation and Management API\nDESCRIPTION: This section details the core API for generating and managing test datasets within Pydantic Evals. It covers the `generate_dataset` asynchronous function for creating datasets using LLMs based on defined Pydantic schemas, and the `Dataset` class methods for saving these datasets to various file formats.",
    "chunk_length": 2052
  },
  {
    "chunk_id": 165,
    "source": "pydantic_ai_llms_data",
    "content": "SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/evals.md#_snippet_12\n\nLANGUAGE: APIDOC\nCODE:\n```\npydantic_evals.generation.generate_dataset:\n  async def generate_dataset(\n      dataset_type: Dataset[InputSchema, OutputSchema, MetadataSchema],\n      n_examples: int,\n      extra_instructions: str\n  ) -> Dataset:\n    - description: Generates a test dataset using an LLM based on provided Pydantic schemas. - parameters:\n      - name: dataset_type\n        type: Dataset[InputSchema, OutputSchema, MetadataSchema]\n        description: A Pydantic Evals Dataset type hint specifying the Pydantic models for inputs, expected outputs, and optional metadata. - name: n_examples\n        type: int\n        description: The number of examples (test cases) to generate for the dataset. - name: extra_instructions\n        type: str\n        description: Additional instructions or context to guide the LLM during dataset generation. - returns:\n      - type: Dataset\n        description: An instance of the generated Dataset containing the test cases. pydantic_evals.Dataset:\n  class Dataset[InputSchema, OutputSchema, MetadataSchema]:\n    - description: Represents a collection of test cases, each with inputs, expected outputs, and optional metadata. - methods:\n      - name: to_file\n        signature: to_file(output_path: Path) -> None\n        description: Saves the dataset to the specified file path. Automatically generates a corresponding JSON schema file alongside the dataset for validation and auto-completion. parameters:\n          - name: output_path\n            type: Path\n            description: The file path where the dataset should be saved. Supports .yaml and .json extensions. ```\n\n----------------------------------------\n\nTITLE: Run Background Slack Member Processing on Modal with Tracing\nDESCRIPTION: This snippet defines a Modal function to wrap `process_slack_member`, enabling it to run as a background task. It ensures that the Logfire context is attached to maintain distributed tracing across the application flow. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/examples/slack-lead-qualifier.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# This function wraps process_slack_member to run it in the background on Modal.",
    "chunk_length": 2262
  },
  {
    "chunk_id": 166,
    "source": "pydantic_ai_llms_data",
    "content": "# It ensures Logfire context is propagated for distributed tracing. import modal\n# Assuming process_slack_member is in app.py\n# from .app import process_slack_member\n\napp = modal.App()\n\n@app.function()\ndef background_process_slack_member(member_data, logfire_context=None):\n    # Attach Logfire context if provided for distributed tracing\n    if logfire_context:\n        # logfire.set_context(logfire_context) # Example of how context might be used\n        pass\n    # process_slack_member(member_data)\n    pass\n```\n\n----------------------------------------\n\nTITLE: Print Pydantic AI Evaluation Report\nDESCRIPTION: This Python snippet demonstrates how to print a comprehensive evaluation report using the `report.print` method. It allows for customization of the output by including or excluding specific details such as input values, processed outputs, and performance durations. Setting `include_durations` to `False` ensures consistent output across multiple runs by omitting time-based metrics. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/evals.md#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nreport.print(include_input=True, include_output=True, include_durations=False)\n```\n\n----------------------------------------\n\nTITLE: Set Cohere API Key Environment Variable\nDESCRIPTION: This bash command sets the `CO_API_KEY` environment variable, which is used by `pydantic-ai` to authenticate with the Cohere API. It is a common and secure way to manage API keys without hardcoding them directly into your application code. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/cohere.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport CO_API_KEY='your-api-key'\n```\n\n----------------------------------------\n\nTITLE: Set Heroku AI Environment Variables\nDESCRIPTION: Explains how to set the `HEROKU_INFERENCE_KEY` and `HEROKU_INFERENCE_URL` environment variables. These variables provide a flexible way to configure authentication and the base URL for Heroku AI outside of the application code. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/openai.md#_snippet_31\n\nLANGUAGE: Bash\nCODE:\n```\nexport HEROKU_INFERENCE_KEY='your-heroku-inference-key'\nexport HEROKU_INFERENCE_URL='https://us.inference.heroku.com'\n```\n\n----------------------------------------\n\nTITLE: Provide Custom Bedrock Provider with AWS Credentials\nDESCRIPTION: Shows how to explicitly pass AWS credentials (access key, secret key, region) to a `BedrockProvider` instance, which is then used to initialize `BedrockConverseModel`.",
    "chunk_length": 2539
  },
  {
    "chunk_id": 167,
    "source": "pydantic_ai_llms_data",
    "content": "This method offers direct control over authentication. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/bedrock.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.bedrock import BedrockConverseModel\nfrom pydantic_ai.providers.bedrock import BedrockProvider\n\n# Using AWS credentials directly\nmodel = BedrockConverseModel(\n    'anthropic.claude-3-sonnet-20240229-v1:0',\n    provider=BedrockProvider(\n        region_name='us-east-1',\n        aws_access_key_id='your-access-key',\n        aws_secret_access_key='your-secret-key',\n    ),\n)\nagent = Agent(model)\n... ```\n\n----------------------------------------\n\nTITLE: Configure Groq model for thinking parts format\nDESCRIPTION: This snippet illustrates how to enable and specify the format for thinking parts when using Groq models. The `groq_reasoning_format` field in `GroqModelSettings` can be set to 'raw' (included in text with tags), 'hidden' (not in text), or 'parsed' (as a separate `ThinkingPart` object). SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/thinking.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.groq import GroqModel, GroqModelSettings\n\nmodel = GroqModel('qwen-qwq-32b')\nsettings = GroqModelSettings(groq_reasoning_format='parsed')\nagent = Agent(model, model_settings=settings)\n... ```\n\n----------------------------------------\n\nTITLE: Pydantic AI MCP Sampling Configuration API\nDESCRIPTION: API details for configuring MCP sampling behavior in Pydantic AI, including setting the sampling model on the server or agent, and controlling whether sampling is allowed. These components enable fine-grained control over how LLM calls are proxied via the MCP client. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/mcp/client.md#_snippet_14\n\nLANGUAGE: APIDOC\nCODE:\n```\npydantic_ai.mcp.MCPServerStdio.sampling_model\n  - Description: Property to set or get the sampling model for an MCP server instance. This model will be used for LLM calls proxied through the client.",
    "chunk_length": 2074
  },
  {
    "chunk_id": 168,
    "source": "pydantic_ai_llms_data",
    "content": "- Type: str (model identifier) or None\n  - Usage: Can be set via the constructor keyword argument or directly on the property after instantiation. pydantic_ai.Agent.set_mcp_sampling_model(model: Optional[str] = None)\n  - Description: Sets the specified model (or the agent's default model if not provided) as the sampling model on all MCP servers registered with the agent. This simplifies configuring multiple servers. - Parameters:\n    - model (Optional[str]): The model name to use for sampling. If None, the agent's primary model is used. - Returns: None\n\npydantic_ai.mcp.MCPServerStdio(..., allow_sampling: bool = True, ...)\n  - Description: Constructor parameter for `MCPServerStdio` to control whether sampling is permitted for the server instance. - Parameters:\n    - allow_sampling (bool): If set to `False`, the server will explicitly not be able to proxy LLM calls through the client. Defaults to `True`. ```\n\n----------------------------------------\n\nTITLE: Set Mistral API Key environment variable\nDESCRIPTION: Demonstrates how to set the `MISTRAL_API_KEY` environment variable in a bash shell. This environment variable is automatically picked up by `pydantic-ai` for authenticating with the Mistral API. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/mistral.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport MISTRAL_API_KEY='your-api-key'\n```\n\n----------------------------------------\n\nTITLE: Specify AI Model for Pydantic AI CLI\nDESCRIPTION: Specify a particular AI model to use with the `clai` CLI by using the `--model` flag. The format is `provider:model_name`, allowing selection of models like Anthropic's Claude Sonnet. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/cli.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nuvx clai --model anthropic:claude-sonnet-4-0\n```\n\n----------------------------------------\n\nTITLE: Slack App Required API Scopes\nDESCRIPTION: Details the necessary API scopes for the Slack application to function correctly, allowing it to read user information for lead qualification.",
    "chunk_length": 2054
  },
  {
    "chunk_id": 169,
    "source": "pydantic_ai_llms_data",
    "content": "These scopes are requested during the Slack app creation process. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/examples/slack-lead-qualifier.md#_snippet_0\n\nLANGUAGE: APIDOC\nCODE:\n```\nSlack App Scopes:\n  - users.read: Allows the app to read basic user information. - users.read.email: Allows the app to read user email addresses. - users.profile.read: Allows the app to read user profile information. ```\n\n----------------------------------------\n\nTITLE: Inspect Agent Conversation Messages (Python)\nDESCRIPTION: This Python snippet shows how to retrieve and print the complete message history of an agent's run. It illustrates the sequence of `ModelRequest`, `ModelResponse`, `ToolCallPart`, and `ToolReturnPart` objects, providing a detailed trace of the agent's interactions with the language model and its tool executions. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/tools.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dice_game import dice_result\n\nprint(dice_result.all_messages())\n```\n\n----------------------------------------\n\nTITLE: Configure Logfire to send traces to an alternative OpenTelemetry backend\nDESCRIPTION: This Python code illustrates how to redirect `pydantic-ai`'s OpenTelemetry traces from Logfire's default backend to an alternative OTLP endpoint, such as `otel-tui`. It involves setting the `OTEL_EXPORTER_OTLP_ENDPOINT` environment variable and configuring Logfire with `send_to_logfire=False` to prevent duplicate data transmission, enabling integration with any OTel-compatible system. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/logfire.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nimport logfire\n\nfrom pydantic_ai import Agent\n\nos.environ['OTEL_EXPORTER_OTLP_ENDPOINT'] = 'http://localhost:4318'\nlogfire.configure(send_to_logfire=False)\nlogfire.instrument_pydantic_ai()\nlogfire.instrument_httpx(capture_all=True)\n\nagent = Agent('openai:gpt-4o')\nresult = agent.run_sync('What is the capital of France?')\nprint(result.output)\n\n```\n\n----------------------------------------\n\nTITLE: Specify Dependency Version Constraints with Inline Metadata\nDESCRIPTION: This snippet demonstrates how to use PEP 723 inline script metadata to specify version constraints for Python package dependencies, such as pinning `rich` to a version less than 13.",
    "chunk_length": 2328
  },
  {
    "chunk_id": 170,
    "source": "pydantic_ai_llms_data",
    "content": "This is useful for ensuring compatibility or specific behavior. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/mcp/run-python.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# /// script\n# dependencies = [\"rich<13\"]\n# ///\n```\n\n----------------------------------------\n\nTITLE: Apply Google-Specific Model Settings in Pydantic AI\nDESCRIPTION: Shows how to use a subclass of `ModelSettings`, specifically `GoogleModelSettings`, to configure model-specific parameters like `gemini_safety_settings`. It also demonstrates handling `UnexpectedModelBehavior` when safety thresholds are exceeded by the model's response. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/agents.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent, UnexpectedModelBehavior\nfrom pydantic_ai.models.google import GoogleModelSettings\n\nagent = Agent('google-gla:gemini-1.5-flash')\n\ntry:\n    result = agent.run_sync(\n        'Write a list of 5 very rude things that I might say to the universe after stubbing my toe in the dark:',\n        model_settings=GoogleModelSettings(\n            temperature=0.0,  # general model settings can also be specified\n            gemini_safety_settings=[\n                {\n                    'category': 'HARM_CATEGORY_HARASSMENT',\n                    'threshold': 'BLOCK_LOW_AND_ABOVE',\n                },\n                {\n                    'category': 'HARM_CATEGORY_HATE_SPEECH',\n                    'threshold': 'BLOCK_LOW_AND_ABOVE',\n                },\n            ],\n        ),\n    )\nexcept UnexpectedModelBehavior as e:\n    print(e)\n```\n\n----------------------------------------\n\nTITLE: Set OpenAI API Key Environment Variable\nDESCRIPTION: Before using the `clai` CLI with OpenAI, set the `OPENAI_API_KEY` environment variable to your personal API key. This authenticates your requests to the OpenAI service. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/cli.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY='your-api-key-here'\n```\n\n----------------------------------------\n\nTITLE: Mermaid Diagram: Email Feedback Graph Structure\nDESCRIPTION: This Mermaid diagram visualizes the state transitions within the email feedback graph.",
    "chunk_length": 2216
  },
  {
    "chunk_id": 171,
    "source": "pydantic_ai_llms_data",
    "content": "It shows the flow from an initial state to email writing, then to feedback, with a loop back to writing based on feedback, and a final end state. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/graph.md#_snippet_11\n\nLANGUAGE: Mermaid\nCODE:\n```\n---\ntitle: feedback_graph\n---\nstateDiagram-v2\n  [*] --> WriteEmail\n  WriteEmail --> Feedback\n  Feedback --> WriteEmail\n  Feedback --> [*]\n```\n\n----------------------------------------\n\nTITLE: Manually controlling Pydantic Graph iteration with `GraphRun.next()`\nDESCRIPTION: This snippet shows how to manually drive graph iteration using `GraphRun.next(node)`, allowing selective execution or skipping of nodes. It illustrates breaking the loop early based on state, which results in `run.result` being `None` if the graph doesn't complete. The `FullStatePersistence` is used to track the history of executed steps. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/graph.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_graph import End, FullStatePersistence\nfrom count_down import CountDown, CountDownState, count_down_graph\n\n\nasync def main():\n    state = CountDownState(counter=5)\n    persistence = FullStatePersistence()\n    async with count_down_graph.iter(\n        CountDown(), state=state, persistence=persistence\n    ) as run:\n        node = run.next_node\n        while not isinstance(node, End):\n            print('Node:', node)\n            if state.counter == 2:\n                break\n            node = await run.next(node)\n\n        print(run.result)\n\n        for step in persistence.history:\n            print('History Step:', step.state, step.state)\n```\n\n----------------------------------------\n\nTITLE: Integrate Retrying HTTP Client with OpenAI API in Python\nDESCRIPTION: Shows how to use a pre-configured `httpx` client, equipped with retry logic (e.g., from `smart_retry_example.py`), when initializing the `OpenAIProvider` for `pydantic-ai`. This ensures all API calls to OpenAI benefit from the defined retry mechanisms. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/retries.md#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nfrom smart_retry_example import create_retrying_client\n\nclient = create_retrying_client()\nmodel = OpenAIModel('gpt-4o', provider=OpenAIProvider(http_client=client))\nagent = Agent(model)\n```\n\n----------------------------------------\n\nTITLE: MCP Sampling Data Flow Diagram\nDESCRIPTION: This Mermaid diagram illustrates the interaction and data flow between an LLM, MCP Client, and MCP Server during an MCP sampling operation.",
    "chunk_length": 2693
  },
  {
    "chunk_id": 172,
    "source": "pydantic_ai_llms_data",
    "content": "It visually represents how the client proxies LLM calls on behalf of the server, including tool calls and sampling responses. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/mcp/client.md#_snippet_10\n\nLANGUAGE: mermaid\nCODE:\n```\nsequenceDiagram\n    participant LLM\n    participant MCP_Client as MCP client\n    participant MCP_Server as MCP server\n\n    MCP_Client->>LLM: LLM call\n    LLM->>MCP_Client: LLM tool call response\n\n    MCP_Client->>MCP_Server: tool call\n    MCP_Server->>MCP_Client: sampling \"create message\"\n\n    MCP_Client->>LLM: LLM call\n    LLM->>MCP_Client: LLM text response\n\n    MCP_Client->>MCP_Server: sampling response\n    MCP_Server->>MCP_Client: tool call response\n```\n\n----------------------------------------\n\nTITLE: Define Human-in-the-Loop AI Q&A Graph with Pydantic-Graph\nDESCRIPTION: This Python code defines a `pydantic-graph` for a human-in-the-loop AI question-and-answer system. It includes nodes for an AI to ask questions, a user to provide answers, and an AI to evaluate those answers, demonstrating state persistence and agent interaction. The graph allows for interruption and resumption, enabling interactive user input. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/graph.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import annotations as _annotations\n\nfrom typing import Annotated\nfrom pydantic_graph import Edge\nfrom dataclasses import dataclass, field\nfrom pydantic import BaseModel\nfrom pydantic_graph import (\n    BaseNode,\n    End,\n    Graph,\n    GraphRunContext,\n)\nfrom pydantic_ai import Agent, format_as_xml\nfrom pydantic_ai.messages import ModelMessage\n\nask_agent = Agent('openai:gpt-4o', output_type=str, instrument=True)\n\n\n@dataclass\nclass QuestionState:\n    question: str | None = None\n    ask_agent_messages: list[ModelMessage] = field(default_factory=list)\n    evaluate_agent_messages: list[ModelMessage] = field(default_factory=list)\n\n\n@dataclass\nclass Ask(BaseNode[QuestionState]):\n    \"\"\"Generate question using GPT-4o.\"\"\"\n    docstring_notes = True\n    async def run(\n        self, ctx: GraphRunContext[QuestionState]\n    ) -> Annotated[Answer, Edge(label='Ask the question')]:\n        result = await ask_agent.run(\n            'Ask a simple question with a single correct answer.',\n            message_history=ctx.state.ask_agent_messages,\n        )\n        ctx.state.ask_agent_messages += result.new_messages()\n        ctx.state.question = result.output\n        return Answer(result.output)\n\n\n@dataclass\nclass Answer(BaseNode[QuestionState]):\n    question: str\n\n    async def run(self, ctx: GraphRunContext[QuestionState]) -> Evaluate:\n        answer = input(f'{self.question}: ')\n        return Evaluate(answer)\n\n\nclass EvaluationResult(BaseModel, use_attribute_docstrings=True):\n    correct: bool\n    \"\"\"Whether the answer is correct.\"\"\"\n    comment: str\n    \"\"\"Comment on the answer, reprimand the user if the answer is wrong.\"\"\"\n\n\nevaluate_agent = Agent(\n    'openai:gpt-4o',\n    output_type=EvaluationResult,\n    system_prompt='Given a question and answer, evaluate if the answer is correct.',\n)\n\n\n@dataclass\nclass Evaluate(BaseNode[QuestionState, None, str]):\n    answer: str\n\n    async def run(\n        self,\n        ctx: GraphRunContext[QuestionState],\n    ) -> Annotated[End[str], Edge(label='success')] | Reprimand:\n        assert ctx.state.question is not None\n        result = await evaluate_agent.run(\n            format_as_xml({'question': ctx.state.question, 'answer': self.answer}),\n            message_history=ctx.state.evaluate_agent_messages,\n        )\n        ctx.state.evaluate_agent_messages += result.new_messages()\n        if result.output.correct:\n            return End(result.output.comment)\n        else:\n            return Reprimand(result.output.comment)\n\n\n@dataclass\nclass Reprimand(BaseNode[QuestionState]):\n    comment: str\n\n    async def run(self, ctx: GraphRunContext[QuestionState]) -> Ask:\n        print(f'Comment: {self.comment}')\n        ctx.state.question = None\n        return Ask()\n\n\nquestion_graph = Graph(\n    nodes=(Ask, Answer, Evaluate, Reprimand), state_type=QuestionState\n)\n```\n\n----------------------------------------\n\nTITLE: Define Custom Pydantic AI Agent\nDESCRIPTION: Define a custom `Agent` instance in Python, specifying the AI model and initial instructions.",
    "chunk_length": 4321
  },
  {
    "chunk_id": 173,
    "source": "pydantic_ai_llms_data",
    "content": "This agent can then be used with the `clai` CLI to customize AI behavior. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/cli.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4.1', instructions='You always respond in Italian.')\n```\n\n----------------------------------------\n\nTITLE: Set Anthropic API Key environment variable\nDESCRIPTION: This command sets the `ANTHROPIC_API_KEY` environment variable, which `pydantic-ai` uses to authenticate with the Anthropic API. It is the recommended way to manage your API key securely. Remember to replace 'your-api-key' with your actual API key obtained from the Anthropic console. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/anthropic.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport ANTHROPIC_API_KEY='your-api-key'\n```\n\n----------------------------------------\n\nTITLE: Schedule Daily Summary Function on Modal via Cron\nDESCRIPTION: This snippet illustrates how to define a scheduled function on Modal using the `@app.function()` decorator with a `schedule` argument. It configures the `send_daily_summary` function to run automatically every day at 8 AM UTC. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/examples/slack-lead-qualifier.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# This function schedules the daily summary task to run every day at 8 AM UTC. import modal\nfrom modal import Daily\n# Assuming send_daily_summary is in app.py\n# from .app import send_daily_summary\n\napp = modal.App()\n\n@app.function(schedule=Daily(8, 0))\ndef scheduled_daily_summary_task():\n    # send_daily_summary()\n    pass\n```\n\n----------------------------------------\n\nTITLE: Monitor HTTP requests with Logfire and HTTPX instrumentation\nDESCRIPTION: This Python snippet demonstrates how to enable comprehensive monitoring of HTTP requests made by `pydantic-ai` agents using Logfire's HTTPX instrumentation. By calling `logfire.instrument_httpx(capture_all=True)`, it ensures that both request and response headers and bodies are captured, providing deep visibility into interactions with model providers like OpenAI.",
    "chunk_length": 2147
  },
  {
    "chunk_id": 174,
    "source": "pydantic_ai_llms_data",
    "content": "SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/logfire.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport logfire\n\nfrom pydantic_ai import Agent\n\nlogfire.configure()\nlogfire.instrument_pydantic_ai()\nlogfire.instrument_httpx(capture_all=True)\nagent = Agent('openai:gpt-4o')\nresult = agent.run_sync('What is the capital of France?')\nprint(result.output)\n\n```\n\n----------------------------------------\n\nTITLE: Define Pydantic-AI Agent and Weather Tool\nDESCRIPTION: This Python code defines a `pydantic-ai` agent (`weather_agent`) with a `weather_forecast` tool. The tool retrieves historic or future weather data based on the provided date and location, interacting with external services like `WeatherService` and `DatabaseConn`. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/testing.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom datetime import date\n\nfrom pydantic_ai import Agent, RunContext\n\nfrom fake_database import DatabaseConn  # (1)! from weather_service import WeatherService  # (2)! weather_agent = Agent(\n    'openai:gpt-4o',\n    deps_type=WeatherService,\n    system_prompt='Providing a weather forecast at the locations the user provides.',\n)\n\n\n@weather_agent.tool\ndef weather_forecast(\n    ctx: RunContext[WeatherService], location: str, forecast_date: date\n) -> str:\n    if forecast_date < date.today():  # (3)! return ctx.deps.get_historic_weather(location, forecast_date)\n    else:\n        return ctx.deps.get_forecast(location, forecast_date)\n\n\nasync def run_weather_forecast(  # (4)! user_prompts: list[tuple[str, int]], conn: DatabaseConn\n):\n    \"\"\"Run weather forecast for a list of user prompts and save.\"\"\"\n    async with WeatherService() as weather_service:\n\n        async def run_forecast(prompt: str, user_id: int):\n            result = await weather_agent.run(prompt, deps=weather_service)\n            await conn.store_forecast(user_id, result.output)\n\n        # run all prompts in parallel\n        await asyncio.gather(\n            *(run_forecast(prompt, user_id) for (prompt, user_id) in user_prompts)\n        )\n```\n\n----------------------------------------\n\nTITLE: Combine Multiple Function Toolsets for AI Agent\nDESCRIPTION: This snippet demonstrates how to use `CombinedToolset` in pydantic-ai to merge multiple `FunctionToolset` instances into a single, unified toolset.",
    "chunk_length": 2347
  },
  {
    "chunk_id": 175,
    "source": "pydantic_ai_llms_data",
    "content": "It shows how to initialize `CombinedToolset` with a list of existing toolsets and then integrate this combined toolset with an `Agent`. This allows the agent to access all tools defined across the individual toolsets as if they were part of a single collection. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/toolsets.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom function_toolset import weather_toolset, datetime_toolset\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.test import TestModel\nfrom pydantic_ai.toolsets import CombinedToolset\n\n\ncombined_toolset = CombinedToolset([weather_toolset, datetime_toolset])\n\ntest_model = TestModel()\nagent = Agent(test_model, toolsets=[combined_toolset])\nresult = agent.run_sync('What tools are available?')\nprint([t.name for t in test_model.last_model_request_parameters.function_tools])\n#> ['temperature_celsius', 'temperature_fahrenheit', 'conditions', 'now']\n```\n\n----------------------------------------\n\nTITLE: Display Mermaid Graph Image in Jupyter Notebook\nDESCRIPTION: This Python code demonstrates how to render and display a `pydantic-graph` as an image within a Jupyter Notebook environment. It utilizes `IPython.display` to show the generated Mermaid diagram image, making graph visualization interactive. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/graph.md#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nfrom graph_example import DivisibleBy5, fives_graph\nfrom IPython.display import Image, display\n\ndisplay(Image(fives_graph.mermaid_image(start_node=DivisibleBy5)))\n```\n\n----------------------------------------\n\nTITLE: Mermaid Graph for ModelMessage Structure\nDESCRIPTION: Visual representation of the `ModelMessage` components and their relationships within the `pydantic_ai.messages` module, showing how different prompt and response parts contribute to the overall message structure. This graph helps in understanding the data flow and composition of messages in the pydantic-ai library. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/api/messages.md#_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\ngraph RL\n    SystemPromptPart(SystemPromptPart) --- ModelRequestPart\n    UserPromptPart(UserPromptPart) --- ModelRequestPart\n    ToolReturnPart(ToolReturnPart) --- ModelRequestPart\n    RetryPromptPart(RetryPromptPart) --- ModelRequestPart\n    TextPart(TextPart) --- ModelResponsePart\n    ToolCallPart(ToolCallPart) --- ModelResponsePart\n    ThinkingPart(ThinkingPart) --- ModelResponsePart\n    ModelRequestPart(\"ModelRequestPart<br>(Union)\") --- ModelRequest\n    ModelRequest(\"ModelRequest(parts=list[...])\") --- ModelMessage\n    ModelResponsePart(\"ModelResponsePart<br>(Union)\") --- ModelResponse\n    ModelResponse(\"ModelResponse(parts=list[...])\") --- ModelMessage(\"ModelMessage<br>(Union)\")\n```\n\n----------------------------------------\n\nTITLE: Handle Fallback Model Failures (Python 3.11+)\nDESCRIPTION: Demonstrates how to catch `ModelHTTPError` exceptions using the `except*` syntax introduced in Python 3.11.",
    "chunk_length": 3017
  },
  {
    "chunk_id": 176,
    "source": "pydantic_ai_llms_data",
    "content": "This allows for handling `FallbackExceptionGroup` which contains all individual exceptions encountered when all models in the `FallbackModel` chain fail. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/index.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.exceptions import ModelHTTPError\nfrom pydantic_ai.models.anthropic import AnthropicModel\nfrom pydantic_ai.models.fallback import FallbackModel\nfrom pydantic_ai.models.openai import OpenAIModel\n\nopenai_model = OpenAIModel('gpt-4o')\nanthropic_model = AnthropicModel('claude-3-5-sonnet-latest')\nfallback_model = FallbackModel(openai_model, anthropic_model)\n\nagent = Agent(fallback_model)\ntry:\n    response = agent.run_sync('What is the capital of France?')\nexcept* ModelHTTPError as exc_group:\n    for exc in exc_group.exceptions:\n        print(exc)\n```\n\n----------------------------------------\n\nTITLE: Generate Mermaid Diagram Code for a Pydantic Graph\nDESCRIPTION: This Python snippet illustrates how to programmatically generate the Mermaid diagram code for a `pydantic-graph` instance. It imports the previously defined `fives_graph` and uses its `mermaid_code` method to output a string representation suitable for Mermaid rendering. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/graph.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nfrom graph_example import DivisibleBy5, fives_graph\n\nfives_graph.mermaid_code(start_node=DivisibleBy5)\n```\n\n----------------------------------------\n\nTITLE: Configure Anthropic model for thinking\nDESCRIPTION: This snippet shows how to enable thinking for Anthropic models. Unlike other providers, Anthropic includes a signature in the thinking part for tamper-proofing. Thinking is enabled by configuring the `anthropic_thinking` field in the `AnthropicModelSettings`, specifying its type and an optional token budget. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/thinking.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.anthropic import AnthropicModel, AnthropicModelSettings\n\nmodel = AnthropicModel('claude-3-7-sonnet-latest')\nsettings = AnthropicModelSettings(\n    anthropic_thinking={'type': 'enabled', 'budget_tokens': 1024},\n)\nagent = Agent(model, model_settings=settings)\n...",
    "chunk_length": 2315
  },
  {
    "chunk_id": 177,
    "source": "pydantic_ai_llms_data",
    "content": "```\n\n----------------------------------------\n\nTITLE: Handle Fallback Model Failures (Python <3.11)\nDESCRIPTION: Illustrates how to manage `ModelHTTPError` exceptions for Python versions older than 3.11. It uses the `exceptiongroup` backport package and its `catch` context manager to handle `FallbackExceptionGroup` when all models in the `FallbackModel` chain fail. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/index.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom exceptiongroup import catch\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.exceptions import ModelHTTPError\nfrom pydantic_ai.models.anthropic import AnthropicModel\nfrom pydantic_ai.models.fallback import FallbackModel\nfrom pydantic_ai.models.openai import OpenAIModel\n\n\ndef model_status_error_handler(exc_group: BaseExceptionGroup) -> None:\n    for exc in exc_group.exceptions:\n        print(exc)\n\n\nopenai_model = OpenAIModel('gpt-4o')\nanthropic_model = AnthropicModel('claude-3-5-sonnet-latest')\nfallback_model = FallbackModel(openai_model, anthropic_model)\n\nagent = Agent(fallback_model)\nwith catch({ModelHTTPError: model_status_error_handler}):\n    response = agent.run_sync('What is the capital of France?')\n```\n\n----------------------------------------\n\nTITLE: Unit Test Pydantic-AI Agent with TestModel\nDESCRIPTION: This Python unit test demonstrates how to use `pydantic-ai`'s `TestModel` to test an agent's functionality without external API calls. It captures messages and asserts the agent's behavior and output, showcasing `TestModel`'s ability to simulate tool calls and generate structured responses. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/testing.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import timezone\nimport pytest\n\nfrom dirty_equals import IsNow, IsStr\n\nfrom pydantic_ai import models, capture_run_messages\nfrom pydantic_ai.models.test import TestModel\nfrom pydantic_ai.messages import (\n    ModelResponse,\n    SystemPromptPart,\n    TextPart,\n    ToolCallPart,\n    ToolReturnPart,\n    UserPromptPart,\n    ModelRequest,\n)\nfrom pydantic_ai.usage import Usage\n\nfrom fake_database import DatabaseConn\nfrom weather_app import run_weather_forecast, weather_agent\n\npytestmark = pytest.mark.anyio  # (1)!",
    "chunk_length": 2245
  },
  {
    "chunk_id": 178,
    "source": "pydantic_ai_llms_data",
    "content": "models.ALLOW_MODEL_REQUESTS = False  # (2)! async def test_forecast():\n    conn = DatabaseConn()\n    user_id = 1\n    with capture_run_messages() as messages:\n        with weather_agent.override(model=TestModel()):  # (3)! prompt = 'What will the weather be like in London on 2024-11-28?'\n            await run_weather_forecast([(prompt, user_id)], conn)  # (4)! forecast = await conn.get_forecast(user_id)\n    assert forecast == '{\"weather_forecast\":\"Sunny with a chance of rain\"}'  # (5)! assert messages == [  # (6)! ModelRequest(\n            parts=[\n                SystemPromptPart(\n                    content='Providing a weather forecast at the locations the user provides.',\n                    timestamp=IsNow(tz=timezone.utc),\n                ),\n                UserPromptPart(\n                    content='What will the weather be like in London on 2024-11-28?',\n                    timestamp=IsNow(tz=timezone.utc),  # (7)! ),\n            ]\n        ),\n        ModelResponse(\n            parts=[\n                ToolCallPart(\n                    tool_name='weather_forecast',\n                    args={\n                        'location': 'a',\n                        'forecast_date': '2024-01-01',  # (8)! ```\n\n----------------------------------------\n\nTITLE: Override Pydantic AI Agent Dependencies for Testing\nDESCRIPTION: This Python test code demonstrates how to override the dependencies of a `Pydantic AI Agent` for testing purposes. It defines `TestMyDeps`, a subclass of `MyDeps`, to mock the `system_prompt_factory` method. The `joke_agent.override` context manager is used to temporarily replace the agent's dependencies with `TestMyDeps` instances, allowing for isolated testing of the `application_code` without external HTTP calls. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/dependencies.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom joke_app import MyDeps, application_code, joke_agent\n\n\nclass TestMyDeps(MyDeps):  # (1)! async def system_prompt_factory(self) -> str:\n        return 'test prompt'\n\n\nasync def test_application_code():\n    test_deps = TestMyDeps('test_key', None)  # (2)!",
    "chunk_length": 2133
  },
  {
    "chunk_id": 179,
    "source": "pydantic_ai_llms_data",
    "content": "with joke_agent.override(deps=test_deps):  # (3)! joke = await application_code('Tell me a joke.')  # (4)! assert joke.startswith('Did you hear about the toothpaste scandal?')\n```\n\n----------------------------------------\n\nTITLE: Configure Google model for thinking\nDESCRIPTION: This snippet demonstrates how to enable thinking for Google models. The `google_thinking_config` field within `GoogleModelSettings` is used to include thoughts, typically by setting `include_thoughts` to `True`. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/thinking.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.google import GoogleModel, GoogleModelSettings\n\nmodel = GoogleModel('gemini-2.5-pro-preview-03-25')\nsettings = GoogleModelSettings(google_thinking_config={'include_thoughts': True})\nagent = Agent(model, model_settings=settings)\n... ```\n\n----------------------------------------\n\nTITLE: Pydantic AI Agent Graph Iteration\nDESCRIPTION: This section describes how to gain deeper control and insight into a Pydantic AI agent's execution flow by iterating over its underlying `pydantic-graph`. It covers the `Agent.iter()` method, the `AgentRun` object returned, and how to manually advance the graph using `next()` until an `End` node is reached. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/agents.md#_snippet_5\n\nLANGUAGE: APIDOC\nCODE:\n```\nAgent.iter()\n  - Purpose: Returns an AgentRun object to asynchronously iterate over the agent's execution graph or drive it node-by-node. - Returns: AgentRun\n\nAgentRun\n  - Purpose: An object representing an ongoing agent execution, allowing iteration over graph steps. - Methods:\n    - next(): Manually advances the agent's execution to the next node in the graph. - Termination: Iteration concludes when the graph returns an End node. pydantic_graph.nodes.End\n  - Purpose: Represents the final state or completion of a pydantic-graph execution. ```\n\n----------------------------------------\n\nTITLE: Pydantic AI Agent.override Method\nDESCRIPTION: The `override` method of the `pydantic_ai.Agent` class provides a mechanism to temporarily replace the agent's registered dependencies.",
    "chunk_length": 2195
  },
  {
    "chunk_id": 180,
    "source": "pydantic_ai_llms_data",
    "content": "It is designed to be used as a context manager, ensuring that the original dependencies are restored upon exiting the `with` block. This functionality is crucial for testing, allowing developers to inject mock or test-specific dependency implementations without altering the agent's global state. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/dependencies.md#_snippet_6\n\nLANGUAGE: APIDOC\nCODE:\n```\npydantic_ai.Agent.override(deps: Any) -> ContextManager\n  - Temporarily overrides the agent's dependencies within a context. - Parameters:\n    - deps: An instance of the dependency type (or a subclass) to be used during the override. - Returns: A context manager that restores original dependencies upon exit. - Usage:\n    with agent_instance.override(deps=test_dependencies):\n        # Code that uses the agent with overridden dependencies\n        pass\n```\n\n----------------------------------------\n\nTITLE: Stream AI Agent Response (Full Text)\nDESCRIPTION: This snippet demonstrates how to stream the complete text response from a pydantic-ai agent. It uses the `Agent.run_stream()` method as an asynchronous context manager and iterates over `result.stream_text()` to receive the full response progressively. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/output.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('google-gla:gemini-1.5-flash')\n\n\nasync def main():\n    async with agent.run_stream('Where does \"hello world\" come from?') as result:\n        async for message in result.stream_text():\n            print(message)\n            # The first known\n            # The first known use of \"hello,\"\n            # The first known use of \"hello, world\" was in\n            # The first known use of \"hello, world\" was in a 1974 textbook\n            # The first known use of \"hello, world\" was in a 1974 textbook about the C\n            # The first known use of \"hello, world\" was in a 1974 textbook about the C programming language. ```\n\n----------------------------------------\n\nTITLE: FallbackModel Constructor and Customization\nDESCRIPTION: This API documentation describes the `FallbackModel` constructor, focusing on its `fallback_on` parameter.",
    "chunk_length": 2218
  },
  {
    "chunk_id": 181,
    "source": "pydantic_ai_llms_data",
    "content": "This parameter allows developers to customize which exception types will trigger the `FallbackModel` to attempt the next model in its sequence. By default, the `FallbackModel` only falls back if a `ModelHTTPError` is raised by the current model, but this behavior can be extended to other exception types. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/index.md#_snippet_5\n\nLANGUAGE: APIDOC\nCODE:\n```\nFallbackModel(\n    *models: BaseModel,\n    fallback_on: Type[Exception] | tuple[Type[Exception], ...] = ModelHTTPError\n)\n  - models: One or more instances of `BaseModel` (e.g., `OpenAIModel`, `AnthropicModel`) that the `FallbackModel` will attempt to use in the specified order. - fallback_on: An exception type or a tuple of exception types. If any of these exceptions are raised by the current model during execution, the `FallbackModel` will automatically attempt to use the next model in the sequence. Defaults to `ModelHTTPError`. ```\n\n----------------------------------------\n\nTITLE: Define Custom JSON Schema with Pydantic AI StructuredDict\nDESCRIPTION: Demonstrates how to use `StructuredDict` to define a custom JSON schema for structured output when Pydantic `BaseModel`, dataclass, or `TypedDict` are not suitable (e.g., for dynamically generated or external schemas). Pydantic AI will pass this schema to the model, but will not perform validation itself. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/output.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent, StructuredDict\n\nHumanDict = StructuredDict(\n    {\n        \"type\": \"object\",\n        \"properties\": {\n            \"name\": {\"type\": \"string\"},\n            \"age\": {\"type\": \"integer\"}\n        },\n        \"required\": [\"name\", \"age\"]\n    },\n    name=\"Human\",\n    description=\"A human with a name and age\",\n)\n\nagent = Agent('openai:gpt-4o', output_type=HumanDict)\nresult = agent.run_sync(\"Create a person\")\n# {'name': 'John Doe', 'age': 30}\n```\n\n----------------------------------------\n\nTITLE: Instrument Pydantic AI Agent Runs with Logfire\nDESCRIPTION: Demonstrates how to integrate Pydantic Logfire with Pydantic AI to automatically trace agent runs.",
    "chunk_length": 2179
  },
  {
    "chunk_id": 182,
    "source": "pydantic_ai_llms_data",
    "content": "It configures the Logfire SDK and enables instrumentation for Pydantic AI, ensuring that a trace is generated for each agent run, with spans emitted for model calls and tool function executions. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/logfire.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport logfire\n\nfrom pydantic_ai import Agent\n\nlogfire.configure()  # (1)! Configures the SDK, finding the write token from the .logfire directory or accepting it directly. logfire.instrument_pydantic_ai()  # (2)! Enables instrumentation of Pydantic AI. agent = Agent('openai:gpt-4o', instructions='Be concise, reply with one sentence.')\nresult = agent.run_sync('Where does \"hello world\" come from?')  # (3)! Generates a trace for each run with spans for model calls and tool execution. print(result.output)\n\"\"\"\nThe first known use of \"hello, world\" was in a 1974 textbook about the C programming language. \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Accessing all messages from a Pydantic-AI run\nDESCRIPTION: Demonstrates how to retrieve all messages (requests and responses) from a completed `pydantic-ai` agent run using `result.all_messages()`. The accompanying multi-line string shows the structured `ModelRequest` and `ModelResponse` objects that are returned, including system prompts, user prompts, and the model's text response along with usage statistics. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/message-history.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nprint(result.all_messages())\n\"\"\"\n[\n    ModelRequest(\n        parts=[\n            SystemPromptPart(\n                content='Be a helpful assistant.',\n                timestamp=datetime.datetime(...),\n            ),\n            UserPromptPart(\n                content='Tell me a joke.',\n                timestamp=datetime.datetime(...),\n            ),\n        ]\n    ),\n    ModelResponse(\n        parts=[\n            TextPart(\n                content='Did you hear about the toothpaste scandal? They called it Colgate.'\n            )\n        ],\n        usage=Usage(requests=1, request_tokens=60, response_tokens=12, total_tokens=72),\n        model_name='gpt-4o',\n        timestamp=datetime.datetime(...),\n    ),\n]\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Configure Pydantic AI Agent to Exclude Binary Content\nDESCRIPTION: This snippet demonstrates how to configure a Pydantic AI agent to exclude binary content from its instrumentation events.",
    "chunk_length": 2467
  },
  {
    "chunk_id": 183,
    "source": "pydantic_ai_llms_data",
    "content": "This is useful for reducing data volume or avoiding sending large binary data to observability platforms. It shows both per-agent configuration and global configuration for all agents. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/logfire.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai.agent import Agent, InstrumentationSettings\n\ninstrumentation_settings = InstrumentationSettings(include_binary_content=False)\n\nagent = Agent('gpt-4o', instrument=instrumentation_settings)\n# or to instrument all agents:\nAgent.instrument_all(instrumentation_settings)\n```\n\n----------------------------------------\n\nTITLE: Mermaid State Diagram for Vending Machine (LR Direction)\nDESCRIPTION: This Mermaid code defines a state diagram for a vending machine, illustrating its various states and transitions. The diagram is configured to flow from left to right (LR), showing the sequence of operations from coin insertion to product purchase and reset. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/graph.md#_snippet_24\n\nLANGUAGE: mermaid\nCODE:\n```\n---\ntitle: vending_machine_graph\n---\nstateDiagram-v2\n  direction LR\n  [*] --> InsertCoin\n  InsertCoin --> CoinsInserted\n  CoinsInserted --> SelectProduct\n  CoinsInserted --> Purchase\n  SelectProduct --> Purchase\n  Purchase --> InsertCoin\n  Purchase --> SelectProduct\n  Purchase --> [*]\n```\n\n----------------------------------------\n\nTITLE: Manage Conversations Across Multiple Pydantic AI Runs\nDESCRIPTION: Illustrates how to maintain context in a conversation by chaining multiple `agent.run_sync` calls. It shows how to pass `message_history` from a previous run to ensure the model understands the context of subsequent queries, enabling a continuous dialogue. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/agents.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\n# First run\nresult1 = agent.run_sync('Who was Albert Einstein?')\nprint(result1.output)\n\n# Second run, passing previous messages\nresult2 = agent.run_sync(\n    'What was his most famous equation?',\n    message_history=result1.new_messages(),  # (1)!",
    "chunk_length": 2165
  },
  {
    "chunk_id": 184,
    "source": "pydantic_ai_llms_data",
    "content": ")\nprint(result2.output)\n```\n\n----------------------------------------\n\nTITLE: Mermaid State Diagram with Left-to-Right Flow\nDESCRIPTION: This Mermaid code defines a state diagram for a vending machine, explicitly setting its flow direction to 'LR' (Left to Right) using the `direction LR` directive. It illustrates the transitions between states such as `InsertCoin`, `CoinsInserted`, `SelectProduct`, and `Purchase`, providing a visual representation of the vending machine's operational flow. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/graph.md#_snippet_22\n\nLANGUAGE: Mermaid\nCODE:\n```\n--- \ntitle: vending_machine_graph\n---\nstateDiagram-v2\n  direction LR\n  [*] --> InsertCoin\n  InsertCoin --> CoinsInserted\n  CoinsInserted --> SelectProduct\n  CoinsInserted --> Purchase\n  SelectProduct --> Purchase\n  Purchase --> InsertCoin\n  Purchase --> SelectProduct\n  Purchase --> [*]\n```\n\n----------------------------------------\n\nTITLE: Configure AsyncClient with Smart HTTP Request Retries in Pydantic AI\nDESCRIPTION: Demonstrates how to create an `httpx.AsyncClient` with advanced retry logic using `pydantic_ai.retries.AsyncTenacityTransport`. It configures retries for `HTTPStatusError` and `ConnectionError`, implements a smart `wait_retry_after` strategy respecting `Retry-After` headers, and integrates the client with `Pydantic AI` models for resilient API calls. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/retries.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom httpx import AsyncClient, HTTPStatusError\nfrom tenacity import (\n    AsyncRetrying,\n    stop_after_attempt,\n    wait_exponential,\n    retry_if_exception_type\n)\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai.retries import AsyncTenacityTransport, wait_retry_after\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\ndef create_retrying_client():\n    \"\"\"Create a client with smart retry handling for multiple error types.\"\"\"\n\n    def should_retry_status(response):\n        \"\"\"Raise exceptions for retryable HTTP status codes.\"\"\"\n        if response.status_code in (429, 502, 503, 504):\n            response.raise_for_status()  # This will raise HTTPStatusError\n\n    transport = AsyncTenacityTransport(\n        controller=AsyncRetrying(\n            # Retry on HTTP errors and connection issues\n            retry=retry_if_exception_type((HTTPStatusError, ConnectionError)),\n            # Smart waiting: respects Retry-After headers, falls back to exponential backoff\n            wait=wait_retry_after(\n                fallback_strategy=wait_exponential(multiplier=1, max=60),\n                max_wait=300\n            ),\n            # Stop after 5 attempts\n            stop=stop_after_attempt(5),\n            # Re-raise the last exception if all retries fail\n            reraise=True\n        ),\n        validate_response=should_retry_status\n    )\n    return AsyncClient(transport=transport)\n\n# Use the retrying client with a model\nclient = create_retrying_client()\nmodel = OpenAIModel('gpt-4o', provider=OpenAIProvider(http_client=client))\nagent = Agent(model)\n```\n\n----------------------------------------\n\nTITLE: Initialize HTTP Client with Custom Transport\nDESCRIPTION: Demonstrates the fundamental step of creating an `httpx` client instance by passing a configured transport layer.",
    "chunk_length": 3338
  },
  {
    "chunk_id": 185,
    "source": "pydantic_ai_llms_data",
    "content": "This transport is where custom behaviors like retry logic are injected. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/retries.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nclient = Client(transport=transport)\n```\n\n----------------------------------------\n\nTITLE: Define Pydantic AI Agent for Seat Preference and Extraction Logic\nDESCRIPTION: This snippet defines an AI agent (`seat_preference_agent`) using `pydantic-ai` to extract seat preferences from user input. It specifies `openai:gpt-4o` as the model and `Union[SeatPreference, Failed]` as the output type for structured data or failure. The `find_seat` asynchronous function continuously prompts the user for their seat preference, runs the agent, and handles cases where the preference cannot be understood, maintaining message history for context. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/multi-agent-applications.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nseat_preference_agent = Agent[None, Union[SeatPreference, Failed]](  # (5)! 'openai:gpt-4o',\n    output_type=Union[SeatPreference, Failed],  # type: ignore\n    system_prompt=(\n        \"Extract the user's seat preference. \"\n        'Seats A and F are window seats. '\n        'Row 1 is the front row and has extra leg room. '\n        'Rows 14, and 20 also have extra leg room. '\n    ),\n)\n\n\nasync def find_seat(usage: Usage) -> SeatPreference:  # (6)! message_history: Union[list[ModelMessage], None] = None\n    while True:\n        answer = Prompt.ask('What seat would you like?')\n\n        result = await seat_preference_agent.run(\n            answer,\n            message_history=message_history,\n            usage=usage,\n            usage_limits=usage_limits,\n        )\n        if isinstance(result.output, SeatPreference):\n            return result.output\n        else:\n            print('Could not understand seat preference. Please try again.')\n            message_history = result.all_messages()\n```\n\n----------------------------------------\n\nTITLE: Implement Custom and Built-in Pydantic Evals Evaluators\nDESCRIPTION: This Python code demonstrates how to add both built-in and custom evaluators to a Pydantic Evals `Dataset`.",
    "chunk_length": 2179
  },
  {
    "chunk_id": 186,
    "source": "pydantic_ai_llms_data",
    "content": "It shows the use of `IsInstance` for basic type checking and defines a custom `MyEvaluator` class that inherits from `Evaluator` to provide a scoring logic based on output matching the expected output, including partial matches. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/evals.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom dataclasses import dataclass\n\nfrom simple_eval_dataset import dataset\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\nfrom pydantic_evals.evaluators.common import IsInstance\n\ndataset.add_evaluator(IsInstance(type_name='str'))  # (1)! @dataclass\nclass MyEvaluator(Evaluator):\n    async def evaluate(self, ctx: EvaluatorContext[str, str]) -> float:  # (2)! if ctx.output == ctx.expected_output:\n            return 1.0\n        elif (\n            isinstance(ctx.output, str)\n            and ctx.expected_output.lower() in ctx.output.lower()\n        ):\n            return 0.8\n        else:\n            return 0.0\n\n\ndataset.add_evaluator(MyEvaluator())\n```\n\n----------------------------------------\n\nTITLE: pydantic_graph.nodes Module Members\nDESCRIPTION: This entry documents the public members exposed by the `pydantic_graph.nodes` module. It lists types and classes such as `StateT`, `GraphRunContext`, `BaseNode`, `End`, `Edge`, `DepsT`, `RunEndT`, and `NodeRunEndT`, which are integral to defining and managing graph structures within the pydantic_graph library. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/api/pydantic_graph/nodes.md#_snippet_0\n\nLANGUAGE: APIDOC\nCODE:\n```\nModule: pydantic_graph.nodes\n\nMembers:\n- StateT\n- GraphRunContext\n- BaseNode\n- End\n- Edge\n- DepsT\n- RunEndT\n- NodeRunEndT\n```\n\n----------------------------------------\n\nTITLE: Disabling MCP Sampling for a Pydantic AI Server\nDESCRIPTION: This Python snippet shows how to explicitly disallow sampling when configuring an `MCPServerStdio` instance. By setting the `allow_sampling` parameter to `False` during server initialization, the server will not be able to proxy LLM calls through the client, overriding the default behavior.",
    "chunk_length": 2084
  },
  {
    "chunk_id": 187,
    "source": "pydantic_ai_llms_data",
    "content": "SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/mcp/client.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai.mcp import MCPServerStdio\n\nserver = MCPServerStdio(\n    command='python',\n    args=['generate_svg.py'],\n    allow_sampling=False,\n)\n```\n\n----------------------------------------\n\nTITLE: Implement Smart Wait Strategy with wait_retry_after for HTTP Retries\nDESCRIPTION: Illustrates the usage of `pydantic_ai.retries.wait_retry_after` for intelligent backoff in retry mechanisms. It shows how to configure it to automatically respect HTTP `Retry-After` headers, fall back to exponential backoff when no header is present, and set a maximum wait time to prevent excessive delays during retries. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/retries.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai.retries import wait_retry_after\nfrom tenacity import wait_exponential\n\n# Basic usage - respects Retry-After headers, falls back to exponential backoff\nwait_strategy_1 = wait_retry_after()\n\n# Custom configuration\nwait_strategy_2 = wait_retry_after(\n    fallback_strategy=wait_exponential(multiplier=2, max=120),\n    max_wait=600  # Never wait more than 10 minutes\n)\n```\n\n----------------------------------------\n\nTITLE: Configure Synchronous HTTP Client with TenacityTransport for Retries\nDESCRIPTION: Illustrates how to set up an `httpx.Client` with `pydantic_ai.retries.TenacityTransport` for synchronous HTTP request retries. It demonstrates integrating a `Retrying` controller from `tenacity` and an optional `validate_response` function to treat specific HTTP status codes (e.g., 4xx/5xx) as retryable failures, ensuring robust synchronous API interactions. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/retries.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom httpx import Client\nfrom tenacity import Retrying, stop_after_attempt\nfrom pydantic_ai.retries import TenacityTransport\n\n# Create the basic components\nretrying = Retrying(stop=stop_after_attempt(3), reraise=True)\n\ndef validator(response):\n    \"\"\"Treat responses with HTTP status 4xx/5xx as failures that need to be retried.",
    "chunk_length": 2158
  },
  {
    "chunk_id": 188,
    "source": "pydantic_ai_llms_data",
    "content": "Without a response validator, only network errors and timeouts will result in a retry. \"\"\"\n    response.raise_for_status()\n\n# Create the transport\ntransport = TenacityTransport(\n    controller=retrying,       # Retrying instance\n    validate_response=validator # Optional response validator\n)\n```\n\n----------------------------------------\n\nTITLE: Configure Asynchronous HTTP Client with AsyncTenacityTransport for Retries\nDESCRIPTION: Shows how to set up an `httpx.AsyncClient` with `pydantic_ai.retries.AsyncTenacityTransport` for asynchronous HTTP request retries. It demonstrates integrating an `AsyncRetrying` controller from `tenacity` and an optional `validate_response` function to treat specific HTTP status codes (e.g., 4xx/5xx) as retryable failures, beyond just network errors. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/retries.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom httpx import AsyncClient\nfrom tenacity import AsyncRetrying, stop_after_attempt\nfrom pydantic_ai.retries import AsyncTenacityTransport\n\n# Create the basic components\nasync_retrying = AsyncRetrying(stop=stop_after_attempt(3), reraise=True)\n\ndef validator(response):\n    \"\"\"Treat responses with HTTP status 4xx/5xx as failures that need to be retried. Without a response validator, only network errors and timeouts will result in a retry. \"\"\"\n    response.raise_for_status()\n\n# Create the transport\ntransport = AsyncTenacityTransport(\n    controller=async_retrying,   # AsyncRetrying instance\n    validate_response=validator  # Optional response validator\n)\n\n# Create a client using the transport:\nclient = AsyncClient(transport=transport)\n```\n\n----------------------------------------\n\nTITLE: Define an Intermediate Node in Pydantic-Graph\nDESCRIPTION: This Python code demonstrates how to create a basic intermediate node (`MyNode`) in a pydantic-graph. It's defined as a dataclass with a field `foo` and an asynchronous `run` method that processes context and returns another node, indicating an outgoing edge. This node cannot terminate the graph run.",
    "chunk_length": 2059
  },
  {
    "chunk_id": 189,
    "source": "pydantic_ai_llms_data",
    "content": "SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/graph.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_graph import BaseNode, GraphRunContext\n\n\n@dataclass\nclass MyNode(BaseNode[MyState]):\n    foo: int\n\n    async def run(\n        self,\n        ctx: GraphRunContext[MyState],\n    ) -> AnotherNode:\n        ... return AnotherNode()\n```\n\n----------------------------------------\n\nTITLE: Generate Vending Machine State Diagram with LR Direction in Python\nDESCRIPTION: This Python snippet demonstrates how to generate a Mermaid state diagram for a vending machine, explicitly setting its direction to 'Left to Right' (LR) using the `mermaid_code` method. It requires the `vending_machine.py` module for its functionality. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/graph.md#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nfrom vending_machine import InsertCoin, vending_machine_graph\n\nvending_machine_graph.mermaid_code(start_node=InsertCoin, direction='LR')\n```\n\n----------------------------------------\n\nTITLE: Handle Rate Limits with Retry-After Headers in Python\nDESCRIPTION: This function creates an `httpx.AsyncClient` configured to automatically respect `Retry-After` headers from 429 (Too Many Requests) responses. It uses `AsyncTenacityTransport` with `wait_retry_after` for intelligent waiting, falling back to exponential backoff if no header is present. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/retries.md#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nfrom httpx import AsyncClient, HTTPStatusError\nfrom tenacity import AsyncRetrying, stop_after_attempt, retry_if_exception_type, wait_exponential\nfrom pydantic_ai.retries import AsyncTenacityTransport, wait_retry_after\n\ndef create_rate_limit_client():\n    \"\"\"Create a client that respects Retry-After headers from rate limiting responses.\"\"\"\n    transport = AsyncTenacityTransport(\n        controller=AsyncRetrying(\n            retry=retry_if_exception_type(HTTPStatusError),\n            wait=wait_retry_after(\n                fallback_strategy=wait_exponential(multiplier=1, max=60),\n                max_wait=300  # Don't wait more than 5 minutes\n            ),\n            stop=stop_after_attempt(10),\n            reraise=True\n        ),\n        validate_response=lambda r: r.raise_for_status()  # Raises HTTPStatusError for 4xx/5xx\n    )\n    return AsyncClient(transport=transport)\n\n# Example usage\nclient = create_rate_limit_client()\n# Client is now ready to use with any HTTP requests and will respect Retry-After headers\n```\n\n----------------------------------------\n\nTITLE: Handle Tool Execution Retries with ModelRetry in Python\nDESCRIPTION: This snippet demonstrates how to explicitly request a tool execution retry using the `ModelRetry` exception in Pydantic AI.",
    "chunk_length": 2814
  },
  {
    "chunk_id": 190,
    "source": "pydantic_ai_llms_data",
    "content": "Raising `ModelRetry` allows a tool's internal logic to inform the LLM of an issue, prompting it to correct parameters and retry the call, similar to how `ValidationError` works for argument validation failures. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/tools.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import ModelRetry\n\ndef my_flaky_tool(query: str) -> str:\n    if query == 'bad':\n        # Tell the LLM the query was bad and it should try again\n        raise ModelRetry(\"The query 'bad' is not allowed. Please provide a different query.\")\n    # ... process query ... return 'Success!'\n```\n\n----------------------------------------\n\nTITLE: Serializing and deserializing Pydantic AI messages to JSON\nDESCRIPTION: This snippet demonstrates how to persist Pydantic AI message histories by serializing them to JSON-compatible Python objects or direct JSON strings, and then deserializing them back. It utilizes `ModelMessagesTypeAdapter` for validation and conversion, enabling storage or sharing of conversation states. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/message-history.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_core import to_jsonable_python\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.messages import ModelMessagesTypeAdapter\n\nagent = Agent('openai:gpt-4o', system_prompt='Be a helpful assistant.')\n\nresult1 = agent.run_sync('Tell me a joke.')\nhistory_step_1 = result1.all_messages()\nas_python_objects = to_jsonable_python(history_step_1)\nsame_history_as_step_1 = ModelMessagesTypeAdapter.validate_python(as_python_objects)\n\nresult2 = agent.run_sync(\n    'Tell me a different joke.', message_history=same_history_as_step_1\n)\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_core import to_json\n... as_json_objects = to_json(history_step_1)\nsame_history_as_step_1 = ModelMessagesTypeAdapter.validate_json(as_json_objects)\n```\n\n----------------------------------------\n\nTITLE: Disable Google Model Thinking\nDESCRIPTION: Shows how to disable the 'thinking' feature for a Google model by setting the `thinking_budget` to `0` within the `google_thinking_config` dictionary when initializing `GoogleModelSettings`.",
    "chunk_length": 2190
  },
  {
    "chunk_id": 191,
    "source": "pydantic_ai_llms_data",
    "content": "SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/google.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.google import GoogleModel, GoogleModelSettings\n\nmodel_settings = GoogleModelSettings(google_thinking_config={'thinking_budget': 0})\nmodel = GoogleModel('gemini-2.0-flash')\nagent = Agent(model, model_settings=model_settings)\n... ```\n\n----------------------------------------\n\nTITLE: Create Custom Retry Logic for HTTP Status Codes and Network Errors in Python\nDESCRIPTION: Illustrates how to define a custom retry condition to selectively retry requests based on HTTP status codes (e.g., 5xx server errors) and network exceptions. This allows fine-grained control over when retries are performed, combining smart `Retry-After` handling with custom backoff. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/retries.md#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nimport httpx\nfrom tenacity import AsyncRetrying, wait_exponential, stop_after_attempt\nfrom pydantic_ai.retries import AsyncTenacityTransport, wait_retry_after\n\ndef create_custom_retry_client():\n    \"\"\"Create a client with custom retry logic.\"\"\"\n    def custom_retry_condition(exception):\n        \"\"\"Custom logic to determine if we should retry.\"\"\"\n        if isinstance(exception, httpx.HTTPStatusError):\n            # Retry on server errors but not client errors\n            return 500 <= exception.response.status_code < 600\n        return isinstance(exception, (httpx.TimeoutException, httpx.ConnectError))\n\n    transport = AsyncTenacityTransport(\n        controller=AsyncRetrying(\n            retry=custom_retry_condition,\n            # Use wait_retry_after for smart waiting on rate limits,\n            # with custom exponential backoff as fallback\n            wait=wait_retry_after(\n                fallback_strategy=wait_exponential(multiplier=2, max=30),\n                max_wait=120\n            ),\n            stop=stop_after_attempt(5),\n            reraise=True\n        ),\n        validate_response=lambda r: r.raise_for_status()\n    )\n    return httpx.AsyncClient(transport=transport)\n\nclient = create_custom_retry_client()\n# Client will retry server errors (5xx) and network errors, but not client errors (4xx)\n```\n\n----------------------------------------\n\nTITLE: Configure Pydantic AI OpenTelemetry Event Mode\nDESCRIPTION: Python code illustrating how to change the default OpenTelemetry event collection behavior in Pydantic AI.",
    "chunk_length": 2485
  },
  {
    "chunk_id": 192,
    "source": "pydantic_ai_llms_data",
    "content": "By setting `event_mode='logs'` during `logfire.instrument_pydantic_ai`, messages are captured as individual events (logs) rather than a single large JSON array attribute on the request span. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/logfire.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport logfire\n\nfrom pydantic_ai import Agent\n\nlogfire.configure()\nlogfire.instrument_pydantic_ai(event_mode='logs')\nagent = Agent('openai:gpt-4o')\nresult = agent.run_sync('What is the capital of France?')\nprint(result.output)\n#> The capital of France is Paris. ```\n\n----------------------------------------\n\nTITLE: Set State Diagram Direction in Python\nDESCRIPTION: This Python snippet demonstrates how to specify the layout direction for a state diagram generated by `vending_machine_graph.mermaid_code`. It sets the direction to 'LR' (Left to Right) using the `direction` parameter. This functionality requires the `vending_machine` module and is compatible with Python 3.10 or newer. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/graph.md#_snippet_21\n\nLANGUAGE: Python\nCODE:\n```\nfrom vending_machine import InsertCoin, vending_machine_graph\n\nvending_machine_graph.mermaid_code(start_node=InsertCoin, direction='LR')\n```\n\n----------------------------------------\n\nTITLE: Test History Processors with FunctionModel in Python\nDESCRIPTION: This snippet demonstrates a robust method for testing history processors using `pytest` and `FunctionModel`. By injecting a `FunctionModel` that captures the messages sent to the underlying model provider, developers can verify that their history processors are correctly modifying the message history before it's processed by the AI model. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/message-history.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport pytest\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.messages import (\n    ModelMessage,\n    ModelRequest,\n    ModelResponse,\n    TextPart,\n    UserPromptPart,\n)\nfrom pydantic_ai.models.function import AgentInfo, FunctionModel\n\n\n@pytest.fixture\ndef received_messages() -> list[ModelMessage]:\n    return []\n\n\n@pytest.fixture\ndef function_model(received_messages: list[ModelMessage]) -> FunctionModel:\n    def capture_model_function(messages: list[ModelMessage], info: AgentInfo) -> ModelResponse:\n        # Capture the messages that the provider actually receives\n        received_messages.clear()\n        received_messages.extend(messages)\n        return ModelResponse(parts=[TextPart(content='Provider response')])\n\n    return FunctionModel(capture_model_function)\n\n\ndef test_history_processor(function_model: FunctionModel, received_messages: list[ModelMessage]):\n    def filter_responses(messages: list[ModelMessage]) -> list[ModelMessage]:\n        return [msg for msg in messages if isinstance(msg, ModelRequest)]\n\n    agent = Agent(function_model, history_processors=[filter_responses])\n\n    message_history = [\n        ModelRequest(parts=[UserPromptPart(content='Question 1')]),\n        ModelResponse(parts=[TextPart(content='Answer 1')]),\n    ]\n\n    agent.run_sync('Question 2', message_history=message_history)\n    assert received_messages == [\n        ModelRequest(parts=[UserPromptPart(content='Question 1')]),\n        ModelRequest(parts=[UserPromptPart(content='Question 2')]),\n    ]\n```\n\n----------------------------------------\n\nTITLE: Pydantic AI Agent Run Result Message Access\nDESCRIPTION: This section details the methods available on `RunResult` and `StreamedRunResult` objects for accessing messages exchanged during an agent run.",
    "chunk_length": 3583
  },
  {
    "chunk_id": 193,
    "source": "pydantic_ai_llms_data",
    "content": "These methods allow retrieval of all messages (including prior runs) or only new messages from the current run, with options for JSON byte output. It also clarifies how message retrieval behaves with `StreamedRunResult` after streaming operations. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/message-history.md#_snippet_0\n\nLANGUAGE: APIDOC\nCODE:\n```\nRunResult / StreamedRunResult:\n  all_messages(): list[Message]\n    - Returns all messages from the current and prior runs. all_messages_json(): bytes\n    - Returns all messages as JSON bytes. new_messages(): list[Message]\n    - Returns only the messages from the current run. new_messages_json(): bytes\n    - Returns only the messages from the current run as JSON bytes. StreamedRunResult specific methods (affecting message completeness):\n  stream(): AsyncIterator[Any]\n    - Awaits the stream to complete, making final result message available. stream_text(): AsyncIterator[str]\n    - Awaits the stream to complete, making final result message available (unless delta=True). stream_structured(): AsyncIterator[Any]\n    - Awaits the stream to complete, making final result message available. get_output(): Any\n    - Awaits the output to be retrieved, making final result message available. ```\n\n----------------------------------------\n\nTITLE: Filtering Pydantic AI Message History with a Custom Processor\nDESCRIPTION: This snippet demonstrates how to use the `history_processors` parameter on a Pydantic AI `Agent` to modify message history before it's sent to the model. The `filter_responses` function serves as a custom processor, removing all `ModelResponse` messages and retaining only `ModelRequest` messages, which can be useful for specific privacy or context management scenarios. SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/message-history.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.messages import (\n    ModelMessage,\n    ModelRequest,\n    ModelResponse,\n    TextPart,\n    UserPromptPart,\n)\n\n\ndef filter_responses(messages: list[ModelMessage]) -> list[ModelMessage]:\n    \"\"\"Remove all ModelResponse messages, keeping only ModelRequest messages.\"\"\"\n    return [msg for msg in messages if isinstance(msg, ModelRequest)]\n\n# Create agent with history processor\nagent = Agent('openai:gpt-4o', history_processors=[filter_responses])\n\n# Example: Create some conversation history\nmessage_history = [\n    ModelRequest(parts=[UserPromptPart(content='What is 2+2?')]),\n    ModelResponse(parts=[TextPart(content='2+2 equals 4')]),  # This will be filtered out\n]\n\n# When you run the agent, the history processor will filter out ModelResponse messages\n# result = agent.run_sync('What about 3+3?', message_history=message_history)\n```",
    "chunk_length": 2766
  }
]